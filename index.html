<!DOCTYPE html>
<html lang="en">

<head>
    <title>Wenxun-Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                wenxun-private-arxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-16T00:00:00Z">2025-01-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid
  Prototyping in Virtual Reality Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Augusto Pinheiro de Sousa, Heiko Hamann, Oliver Deussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM is a foundational technique with broad applications in robotics and
AR/VR. SLAM simulations evaluate new concepts, but testing on
resource-constrained devices, such as VR HMDs, faces challenges: high
computational cost and restricted sensor data access. This work proposes a
sparse framework using mesh geometry projections as features, which improves
efficiency and circumvents direct sensor data access, advancing SLAM research
as we demonstrate in VR and through numerical evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skinned Motion Retargeting with Dense Geometric Interaction Perception <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing and maintaining geometric interactions among different body parts
is crucial for successful motion retargeting in skinned characters. Existing
approaches often overlook body geometries or add a geometry correction stage
after skeletal motion retargeting. This results in conflicts between skeleton
interaction and geometry correction, leading to issues such as jittery,
interpenetration, and contact mismatches. To address these challenges, we
introduce a new retargeting framework, MeshRet, which directly models the dense
geometric interactions in motion retargeting. Initially, we establish dense
mesh correspondences between characters using semantically consistent sensors
(SCS), effective across diverse mesh topologies. Subsequently, we develop a
novel spatio-temporal representation called the dense mesh interaction (DMI)
field. This field, a collection of interacting SCS feature vectors, skillfully
captures both contact and non-contact interactions between body geometries. By
aligning the DMI field during retargeting, MeshRet not only preserves motion
semantics but also prevents self-interpenetration and ensures contact
preservation. Extensive experiments on the public Mixamo <span class="highlight-title">dataset</span> and our
newly-collected ScanRet <span class="highlight-title">dataset</span> demonstrate that MeshRet achieves
state-of-the-art performance. Code available at
https://github.com/abcyzj/MeshRet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMesh: A Motion-aware <span class="highlight-title">Diffusion</span> Framework for Human Mesh Recovery
  from Videos <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware <span class="highlight-title">Diffusion</span>-like framework for video-based HMR. DiffMesh
establishes a bridge between <span class="highlight-title">diffusion</span> models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the <span class="highlight-title">diffusion</span> model.
Extensive experiments are conducted on the widely used <span class="highlight-title">dataset</span>s (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normal-<span class="highlight-title">NeRF</span>: Ambiguity-Robust Normal Estimation for Highly Reflective
  Scenes <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) often struggle with reconstructing and
<span class="highlight-title">rendering</span> highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance <span class="highlight-title">NeRF</span>'s capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity <span class="highlight-title">rendering</span> of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DehazeGS: Seeing Through Fog with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and <span class="highlight-title">rendering</span> quality. Although <span class="highlight-title">NeRF</span>-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, <span class="highlight-title">NeRF</span>'s implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward <span class="highlight-title">rendering</span> process. We introduce
DehazeGS, a method capable of decomposing and <span class="highlight-title">rendering</span> a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy <span class="highlight-title">dataset</span>s demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both <span class="highlight-title">rendering</span>
quality and computational efficiency. visualizations are available at
https://dehazegs.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Transmission and Deblurring: A Semantic Communication Approach
  Using Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based joint source-channel coding (JSCC) is emerging as a
promising technology for effective image transmission. However, most existing
approaches focus on transmitting clear images, overlooking real-world
challenges such as motion blur caused by camera shaking or fast-moving objects.
Motion blur often degrades image quality, making transmission and
reconstruction more challenging. Event cameras, which asynchronously record
pixel intensity changes with extremely low latency, have shown great potential
for motion deblurring tasks. However, the efficient transmission of the
abundant data generated by event cameras remains a significant challenge. In
this work, we propose a novel JSCC framework for the joint transmission of
blurry images and events, aimed at achieving high-quality reconstructions under
limited channel bandwidth. This approach is designed as a deblurring
task-oriented JSCC system. Since RGB cameras and event cameras capture the same
scene through different modalities, their outputs contain both shared and
domain-specific information. To avoid repeatedly transmitting the shared
information, we extract and transmit their shared information and
domain-specific information, respectively. At the receiver, the received
signals are processed by a deblurring decoder to generate clear images.
Additionally, we introduce a multi-stage training strategy to train the
proposed model. Simulation results demonstrate that our method significantly
outperforms existing JSCC-based image transmission schemes, addressing motion
blur effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention
  for Image Restoration Models Compression <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Transformer</span>-based encoder-decoder models have achieved remarkable success in
image-to-image transfer tasks, particularly in image restoration. However,
their high computational complexity-manifested in elevated FLOPs and parameter
counts-limits their application in real-world scenarios. Existing knowledge
distillation methods in image restoration typically employ lightweight student
models that directly mimic the intermediate features and reconstruction results
of the teacher, overlooking the implicit attention relationships between them.
To address this, we propose a Soft Knowledge Distillation (SKD) strategy that
incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for
compressing image restoration models. This mechanism facilitates interaction
between the student and teacher across both channel and spatial dimensions,
enabling the student to implicitly learn the attention matrices. Additionally,
we employ a Gaussian kernel function to measure the distance between student
and teacher features in kernel space, ensuring stable and efficient feature
learning. To further enhance the quality of reconstructed images, we replace
the commonly used L1 or KL divergence loss with a contrastive learning loss at
the image level. Experiments on three tasks-image deraining, deblurring, and
denoising-demonstrate that our SKD strategy significantly reduces computational
complexity while maintaining strong image restoration capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-15T00:00:00Z">2025-01-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and High-Quality Neural Implicit Representation for 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Yang, Bailin Deng, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various SDF-based neural implicit surface reconstruction methods have been
proposed recently, and have demonstrated remarkable modeling capabilities.
However, due to the global nature and limited representation ability of a
single network, existing methods still suffer from many drawbacks, such as
limited accuracy and scale of the reconstruction. In this paper, we propose a
versatile, scalable and high-quality neural implicit representation to address
these issues. We integrate a divide-and-conquer approach into the neural
SDF-based reconstruction. Specifically, we model the object or scene as a
fusion of multiple independent local neural SDFs with overlapping regions. The
construction of our representation involves three key steps: (1) constructing
the distribution and overlap relationship of the local radiance fields based on
object structure or data distribution, (2) relative pose registration for
adjacent local SDFs, and (3) SDF blending. Thanks to the independent
representation of each local region, our approach can not only achieve
high-fidelity surface reconstruction, but also enable scalable scene
reconstruction. Extensive experimental results demonstrate the effectiveness
and practicality of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A
  Benchmark of Geometric Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Nannini, Julian Suk, Patryk Rygiel, Simone Saitta, Luca Mariani, Riccardo Maranga, Andrea Baggiano, Gianluca Pontone, Alberto Redaelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery disease, caused by the narrowing of coronary vessels due to
atherosclerosis, is the leading cause of death worldwide. The diagnostic gold
standard, fractional flow reserve (FFR), measures the trans-stenotic pressure
ratio during maximal vasodilation but is invasive and costly. This has driven
the development of virtual FFR (vFFR) using computational fluid dynamics (CFD)
to simulate coronary flow. Geometric deep learning algorithms have shown
promise for learning features on meshes, including cardiovascular research
applications. This study empirically analyzes various backends for predicting
vFFR fields in coronary arteries as CFD surrogates, comparing six backends for
learning hemodynamics on meshes using CFD solutions as ground truth.
  The study has two parts: i) Using 1,500 synthetic left coronary artery
bifurcations, models were trained to predict pressure-related fields for vFFR
reconstruction, comparing different learning variables. ii) Using 427
patient-specific CFD simulations, experiments were repeated focusing on the
best-performing learning variable from the synthetic <span class="highlight-title">dataset</span>.
  Most backends performed well on the synthetic <span class="highlight-title">dataset</span>, especially when
predicting pressure drop over the manifold. <span class="highlight-title">Transformer</span>-based backends
outperformed others when predicting pressure and vFFR fields and were the only
models achieving strong performance on patient-specific data, excelling in both
average per-point error and vFFR accuracy in stenotic lesions.
  These results suggest geometric deep learning backends can effectively
replace CFD for simple geometries, while <span class="highlight-title">transformer</span>-based networks are
superior for complex, heterogeneous <span class="highlight-title">dataset</span>s. Pressure drop was identified as
the optimal network output for learning pressure-related fields.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View
  Synthesis <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that achieves state-of-the-art <span class="highlight-title">rendering</span> quality and
efficiency on monocular dynamic scene reconstruction using deformable 3D
Gaussians. Implicit deformable representations commonly model motion with a
canonical space and time-dependent backward-warping deformation field. Our
method, GauFRe, uses a forward-warping deformation to explicitly model
non-rigid transformations of scene geometry. Specifically, we propose a
template set of 3D Gaussians residing in a canonical space, and a
time-dependent forward-warping deformation field to model dynamic objects.
Additionally, we tailor a 3D Gaussian-specific static component supported by an
inductive bias-aware initialization approach which allows the deformation field
to focus on moving scene regions, improving the <span class="highlight-title">rendering</span> of complex real-world
motion. The differentiable pipeline is optimized end-to-end with a
<span class="highlight-title">self-supervised</span> <span class="highlight-title">rendering</span> loss. Experiments show our method achieves
competitive results and higher efficiency than both previous state-of-the-art
<span class="highlight-title">NeRF</span> and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20
mins and offer 96 FPS real-time <span class="highlight-title">rendering</span> on an RTX 3090 GPU. Project website:
https://lynl7130.github.io/gaufre/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025. 11 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When No-Reference Image Quality Models Meet MAP Estimation in <span class="highlight-title">Diffusion</span>
  Latents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixia Zhang, Dingquan Li, Guangtao Zhai, Xiaokang Yang, Kede Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary no-reference image quality assessment (NR-IQA) models can
effectively quantify perceived image quality, often achieving strong
correlations with human perceptual scores on standard IQA benchmarks. Yet,
limited efforts have been devoted to treating NR-IQA models as natural image
priors for real-world image enhancement, and consequently comparing them from a
perceptual optimization standpoint. In this work, we show -- for the first time
-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)
estimation framework for image enhancement. This is achieved by performing
gradient ascent in the <span class="highlight-title">diffusion</span> latent space rather than in the raw pixel
domain, leveraging a <span class="highlight-title">pretrain</span>ed differentiable and bijective <span class="highlight-title">diffusion</span> process.
Likely, different NR-IQA models lead to different enhanced outputs, which in
turn provides a new computational means of comparing them. Unlike conventional
correlation-based measures, our comparison method offers complementary insights
into the respective strengths and weaknesses of the competing NR-IQA models in
perceptual optimization scenarios. Additionally, we aim to improve the
best-performing NR-IQA model in <span class="highlight-title">diffusion</span> latent MAP estimation by
incorporating the advantages of other top-performing methods. The resulting
model delivers noticeably better results in enhancing real-world images
afflicted by unknown and complex distortions, all preserving a high degree of
image fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual
  Fidelity on Unseen Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13163v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13163v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deblurring networks have effectively restored clear images from the
blurred ones. However, they often struggle with generalization to unknown
domains. Moreover, these models typically focus on distortion metrics such as
PSNR and SSIM, neglecting the critical aspect of metrics aligned with human
perception. To address these limitations, we propose DeblurDiNAT, a deblurring
<span class="highlight-title">Transformer</span> based on Dilated Neighborhood Attention. First, DeblurDiNAT employs
an alternating dilation factor paradigm to capture both local and global
blurred patterns, enhancing generalization and perceptual clarity. Second, a
local cross-channel learner aids the <span class="highlight-title">Transformer</span> block to understand the
short-range relationships between adjacent channels. Additionally, we present a
linear feed-forward network with a simple while effective design. Finally, a
dual-stage feature fusion module is introduced as an alternative to the
existing approach, which efficiently process multi-scale visual information
across network levels. Compared to state-of-the-art models, our compact
DeblurDiNAT demonstrates superior generalization capabilities and achieves
remarkable performance in perceptual metrics, while maintaining a favorable
model size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchi Chen, Xiuyi Jia, Zhuoran Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, <span class="highlight-title">transformer</span>-based algorithms are making a splash in the domain of
image deblurring. Their achievement depends on the self-attention mechanism
with CNN stem to model long range dependencies between tokens. Unfortunately,
this ear-pleasing pipeline introduces high computational complexity and makes
it difficult to run an ultra-high-definition image on a single GPU in real
time. To trade-off accuracy and efficiency, the input degraded image is
computed cyclically over three dimensional ($C$, $W$, and $H$) signals without
a self-attention mechanism. We term this deep network as Multi-scale
Cubic-Mixer, which is acted on both the real and imaginary components after
fast Fourier transform to estimate the Fourier coefficients and thus obtain a
deblurred image. Furthermore, we combine the multi-scale cubic-mixer with a
slicing strategy to generate high-quality results at a much lower computational
cost. Experimental results demonstrate that the proposed algorithm performs
favorably against the state-of-the-art deblurring approaches on the several
benchmarks and a new ultra-high-definition <span class="highlight-title">dataset</span> in terms of accuracy and
speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-14T00:00:00Z">2025-01-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAViD: Modeling Dynamic Affordance of 3D Objects using <span class="highlight-title">Pre-train</span>ed Video
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Sangwon Beak, Hanbyul Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the ability of humans to use objects is crucial for AI to
improve daily life. Existing studies for learning such ability focus on
human-object patterns (e.g., contact, spatial relation, orientation) in static
situations, and learning Human-Object Interaction (HOI) patterns over time
(i.e., movement of human and object) is relatively less explored. In this
paper, we introduce a novel type of affordance named Dynamic Affordance. For a
given input 3D object mesh, we learn dynamic affordance which models the
distribution of both (1) human motion and (2) human-guided object pose during
interactions. As a core idea, we present a method to learn the 3D dynamic
affordance from synthetically generated 2D videos, leveraging a <span class="highlight-title">pre-train</span>ed
video <span class="highlight-title">diffusion</span> model. Specifically, we propose a pipeline that first generates
2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI
samples. Once we generate diverse 4D HOI samples on various target objects, we
train our DAViD, where we present a method based on the Low-Rank Adaptation
(LoRA) module for <span class="highlight-title">pre-train</span>ed human motion <span class="highlight-title">diffusion</span> model (MDM) and an object
pose <span class="highlight-title">diffusion</span> model with human pose guidance. Our motion <span class="highlight-title">diffusion</span> model is
extended for multi-object interactions, demonstrating the advantage of our
pipeline with LoRA for combining the concepts of object usage. Through
extensive experiments, we demonstrate our DAViD outperforms the baselines in
generating human motion with HOIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snuvclab.github.io/david/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Splatting with Normal Information for Mesh Extraction and
  Improved <span class="highlight-title">Rendering</span> <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meenakshi Krishnan, Liam Fowl, Ramani Duraiswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable 3D Gaussian splatting has emerged as an efficient and flexible
<span class="highlight-title">rendering</span> technique for representing complex scenes from a collection of 2D
views and enabling high-quality real-time novel-view synthesis. However, its
reliance on photometric losses can lead to imprecisely reconstructed geometry
and extracted meshes, especially in regions with high curvature or fine detail.
We propose a novel regularization method using the gradients of a signed
distance function estimated from the Gaussians, to improve the quality of
<span class="highlight-title">rendering</span> while also extracting a surface mesh. The regularizing normal
supervision facilitates better <span class="highlight-title">rendering</span> and mesh reconstruction, which is
crucial for downstream applications in video generation, animation, AR-VR and
gaming. We demonstrate the effectiveness of our approach on <span class="highlight-title">dataset</span>s such as
Mip-<span class="highlight-title">NeRF</span>360, Tanks and Temples, and Deep-Blending. Our method scores higher on
photorealism metrics compared to other mesh extracting <span class="highlight-title">rendering</span> methods
without compromising mesh quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025: Workshop on Generative Data Augmentation for Real-World
  Signal Processing Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric 2D Gaussian Splatting: Background Removal and
  Occlusion-Aware Pruning for Compact Object Models <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Rogge, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining imaging and shape features for prediction tasks of Alzheimer's
  disease classification and brain age regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nairouz Shehata, Carolina Piçarra, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate combining imaging and shape features extracted from MRI for
the clinically relevant tasks of brain age prediction and Alzheimer's disease
classification. Our proposed model fuses ResNet-extracted image embeddings with
shape embeddings from a bespoke graph neural network. The shape embeddings are
derived from surface meshes of 15 brain structures, capturing detailed
geometric information. Combined with the appearance features from T1-weighted
images, we observe improvements in the prediction performance on both tasks,
with substantial gains for classification. We evaluate the model using public
<span class="highlight-title">dataset</span>s, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of
fusing imaging and shape features for brain analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D human pose estimation from single-camera images and
videos have relied on parametric models, like SMPL. However, these models
oversimplify anatomical structures, limiting their accuracy in capturing true
joint locations and movements, which reduces their applicability in
biomechanics, healthcare, and robotics. Biomechanically accurate pose
estimation, on the other hand, typically requires costly marker-based motion
capture systems and optimization techniques in specialized labs. To bridge this
gap, we propose BioPose, a novel learning-based framework for predicting
biomechanically accurate 3D human pose directly from monocular videos. BioPose
includes three key components: a Multi-Query Human Mesh Recovery model
(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose
refinement technique. MQ-HMR leverages a multi-query deformable <span class="highlight-title">transformer</span> to
extract multi-scale fine-grained image features, enabling precise human mesh
recovery. NeurIK treats the mesh vertices as virtual markers, applying a
spatial-temporal network to regress biomechanically accurate 3D poses under
anatomical constraints. To further improve 3D pose estimations, a 2D-informed
refinement step optimizes the query tokens during inference by aligning the 3D
structure with 2D pose observations. Experiments on benchmark <span class="highlight-title">dataset</span>s
demonstrate that BioPose significantly outperforms state-of-the-art methods.
Project website:
\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Eigen Models for Human Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current personalized neural head avatars face a trade-off: lightweight models
lack detail and realism, while high-quality, animatable avatars require
significant computational resources, making them unsuitable for commodity
devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which
provide high-quality, lightweight, and easily controllable head avatars. GEM
utilizes 3D Gaussian primitives for representing the appearance combined with
Gaussian splatting for <span class="highlight-title">rendering</span>. Building on the success of mesh-based 3D
morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases
for representing the head appearance of a specific subject. In particular, we
construct linear bases to represent the position, scale, rotation, and opacity
of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives
of a specific head shape by a linear combination of the basis vectors, only
requiring a low-dimensional parameter vector that contains the respective
coefficients. We propose to construct these linear bases (GEM) by distilling
high-quality compute-intense CNN-based Gaussian avatar models that can generate
expression-dependent appearance changes like wrinkles. These high-quality
models are trained on multi-view videos of a subject and are distilled using a
series of principal component analyses. Once we have obtained the bases that
represent the animatable appearance space of a specific human, we learn a
regressor that takes a single RGB image as input and predicts the
low-dimensional parameter vector that corresponds to the shown facial
expression. In a series of experiments, we compare GEM's self-reenactment and
cross-person reenactment results to state-of-the-art 3D avatar methods,
demonstrating GEM's higher visual quality and better generalization to new
expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://zielon.github.io/gem/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework
designed for large scenes. The framework comprises four main components: VIO
Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO
Front End, RGB frames are processed through dense bundle adjustment and
uncertainty estimation to extract scene geometry and poses. Based on this
output, the mapping module incrementally constructs and maintains a 2D Gaussian
map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,
Score Manager, and Pose Refinement, which collectively improve mapping speed
and localization accuracy. This enables the SLAM system to handle large-scale
urban environments with up to 50 million Gaussian ellipsoids. To ensure global
consistency in large-scale scenes, we design a Loop Closure module, which
innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian
Splatting for loop closure detection and correction of the Gaussian map.
Additionally, we propose a Dynamic Eraser to address the inevitable presence of
dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor
and outdoor environments demonstrate that our approach achieves localization
performance on par with Visual-Inertial Odometry while surpassing recent
GS/<span class="highlight-title">NeRF</span> SLAM methods. It also significantly outperforms all existing methods in
terms of mapping and <span class="highlight-title">rendering</span> quality. Furthermore, we developed a mobile app
and verified that our framework can generate high-quality Gaussian maps in real
time using only a smartphone camera and a low-frequency IMU sensor. To the best
of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method
capable of operating in outdoor environments and supporting kilometer-scale
large scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanIR: Large-Scale Urban Scene Inverse <span class="highlight-title">Rendering</span> from a Single Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09349v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09349v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hao Lin, Bohan Liu, Yi-Ting Chen, Kuan-Sheng Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UrbanIR (Urban Scene Inverse <span class="highlight-title">Rendering</span>), a new inverse graphics
model that enables realistic, free-viewpoint <span class="highlight-title">rendering</span>s of scenes under various
lighting conditions with a single video. It accurately infers shape, albedo,
visibility, and sun and sky illumination from wide-baseline videos, such as
those from car-mounted cameras, differing from <span class="highlight-title">NeRF</span>'s dense view settings. In
this context, standard methods often yield subpar geometry and material
estimates, such as inaccurate roof representations and numerous 'floaters'.
UrbanIR addresses these issues with novel losses that reduce errors in inverse
graphics inference and <span class="highlight-title">rendering</span> artifacts. Its techniques allow for precise
shadow volume estimation in the original scene. The model's outputs support
controllable editing, enabling photorealistic free-viewpoint <span class="highlight-title">rendering</span>s of
night simulations, relit scenes, and inserted objects, marking a significant
improvement over existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://urbaninverserendering.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable <span class="highlight-title">rendering</span> techniques such as Neural Radiance
Fields (<span class="highlight-title">NeRF</span>) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD <span class="highlight-title">dataset</span>s demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed that modern image and video quality assessment
(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can
manipulate a video through preprocessing to artificially increase its quality
score according to a certain metric, despite no actual improvement in visual
quality. Most of the attacks studied in the literature are white-box attacks,
while black-box attacks in the context of VQA have received less attention.
Moreover, some research indicates a lack of transferability of adversarial
examples generated for one model to another when applied to VQA. In this paper,
we propose a cross-modal attack method, IC2VQA, aimed at exploring the
vulnerabilities of modern VQA models. This approach is motivated by the
observation that the low-level feature spaces of images and videos are similar.
We investigate the transferability of adversarial perturbations across
different modalities; specifically, we analyze how adversarial perturbations
generated on a white-box IQA model with an additional <span class="highlight-title">CLIP</span> module can
effectively target a VQA model. The addition of the <span class="highlight-title">CLIP</span> module serves as a
valuable aid in increasing transferability, as the <span class="highlight-title">CLIP</span> model is known for its
effective capture of low-level semantics. Extensive experiments demonstrate
that IC2VQA achieves a high success rate in attacking three black-box VQA
models. We compare our method with existing black-box attack strategies,
highlighting its superiority in terms of attack success within the same number
of iterations and levels of attack strength. We believe that the proposed
method will contribute to the deeper analysis of robust VQA metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-13T00:00:00Z">2025-01-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point
  Cloud or Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis A G Stuart, Michael P Pound
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D
reconstructions, but these scenes often require specialised renderers for
effective visualisation. In contrast, point clouds are a widely used 3D
representation and are compatible with most popular 3D processing software, yet
converting 3DGS scenes into point clouds is a complex challenge. In this work
we introduce 3DGS-to-PC, a flexible and highly customisable framework that is
capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We
sample points probabilistically from each Gaussian as a 3D density function. We
additionally threshold new points using the Mahalanobis distance to the
Gaussian centre, preventing extreme outliers. The result is a point cloud that
closely represents the shape encoded into the 3D Gaussian scene. Individual
Gaussians use spherical harmonics to adapt colours depending on view, and each
point may contribute only subtle colour hints to the resulting rendered scene.
To avoid spurious or incorrect colours that do not fit with the final point
cloud, we recalculate Gaussian colours via a customised image <span class="highlight-title">rendering</span>
approach, assigning each Gaussian the colour of the pixel to which it
contributes most across all views. 3DGS-to-PC also supports mesh generation
through Poisson Surface Reconstruction, applied to points sampled from
predicted surface Gaussians. This allows coloured meshes to be generated from
3DGS scenes without the need for re-training. This package is highly
customisable and capability of simple integration into existing 3DGS pipelines.
3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud
and surface-based formats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular
  Video Based on Rectified Mesh-embedded Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RMAvatar, a novel human avatar representation with Gaussian
splatting embedded on mesh to learn clothed avatar from a monocular video. We
utilize the explicit mesh geometry to represent motion and shape of a virtual
human and implicit appearance <span class="highlight-title">rendering</span> with Gaussian Splatting. Our method
consists of two main modules: Gaussian initialization module and Gaussian
rectification module. We embed Gaussians into triangular faces and control
their motion through the mesh, which ensures low-frequency motion and surface
deformation of the avatar. Due to the limitations of LBS formula, the human
skeleton is hard to control complex non-rigid transformations. We then design a
pose-related Gaussian rectification module to learn fine-detailed non-rigid
deformations, further improving the realism and expressiveness of the avatar.
We conduct extensive experiments on public <span class="highlight-title">dataset</span>s, RMAvatar shows
state-of-the-art performance on both <span class="highlight-title">rendering</span> quality and quantitative
evaluations. Please see our project page at https://rm-avatar.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in
reconstructing detailed 3D scenes within multi-view setups and the emergence of
large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based
method utilizing a human face foundation model as guidance with just a single
image as input. To achieve that, we extend such a model for diverse-view human
head generation by fine-tuning on synthetic data and modifying its
conditioning. Our avatars maintain a dense correspondence with a human face
mesh template, allowing blendshape-based expression generation. This is
achieved through a modified 3DGS approach, connectivity regularizers, and a
strategic initialization tailored for our task. Additionally, we propose an
optional efficient SDS-based correction step to refine the blendshape
expressions, enhancing realism and diversity. Experiments demonstrate that
Arc2Avatar achieves state-of-the-art realism and identity preservation,
effectively addressing color issues by allowing the use of very low guidance,
enabled by our strong identity prior and initialization strategy, without
compromising detail. Please visit https://arc2avatar.github.io for more
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page https://arc2avatar.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Human Perception of Novel View Synthesis: Subjective Quality
  Assessment of Gaussian Splatting and <span class="highlight-title">NeRF</span> in Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) and Neural Radiance Fields (<span class="highlight-title">NeRF</span>) are two
groundbreaking technologies that have revolutionized the field of Novel View
Synthesis (NVS), enabling immersive photorealistic <span class="highlight-title">rendering</span> and user
experiences by synthesizing multiple viewpoints from a set of images of sparse
views. The potential applications of NVS, such as high-quality virtual and
augmented reality, detailed 3D modeling, and realistic medical organ imaging,
underscore the importance of quality assessment of NVS methods from the
perspective of human perception. Although some previous studies have explored
subjective quality assessments for NVS technology, they still face several
challenges, especially in NVS methods selection, scenario coverage, and
evaluation methodology. To address these challenges, we conducted two
subjective experiments for the quality assessment of NVS technologies
containing both GS-based and <span class="highlight-title">NeRF</span>-based methods, focusing on dynamic and
real-world scenes. This study covers 360{\deg}, front-facing, and
single-viewpoint videos while providing a richer and greater number of real
scenes. Meanwhile, it's the first time to explore the impact of NVS methods in
dynamic scenes with moving objects. The two types of subjective experiments
help to fully comprehend the influences of different viewing paths from a human
perception perspective and pave the way for future development of
full-reference and no-reference quality metrics. In addition, we established a
comprehensive benchmark of various state-of-the-art objective metrics on the
proposed database, highlighting that existing methods still struggle to
accurately capture subjective quality. The results give us some insights into
the limitations of existing NVS methods and may promote the development of new
NVS methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Light Transport-aware <span class="highlight-title">Diffusion</span> Posterior Sampling for Single-View
  Reconstruction of 3D Volumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludwic Leonard, Nils Thuerey, Ruediger Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
<span class="highlight-title">diffusion</span> model trained on a novel benchmark <span class="highlight-title">dataset</span> comprising 1,000
synthetically simulated volumetric density fields. The neural <span class="highlight-title">diffusion</span> model
is trained on the latent codes of a novel, <span class="highlight-title">diffusion</span>-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric <span class="highlight-title">diffusion</span> posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic <span class="highlight-title">NeRF</span> approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-12T00:00:00Z">2025-01-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Prior for Few-Shot Drivable Head Avatar Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SynShot, a novel method for the few-shot inversion of a drivable
head avatar based on a synthetic prior. We tackle two major challenges. First,
training a controllable 3D generative network requires a large number of
diverse sequences, for which pairs of images and high-quality tracked meshes
are not always available. Second, state-of-the-art monocular avatar models
struggle to generalize to new views and expressions, lacking a strong prior and
often overfitting to a specific viewpoint distribution. Inspired by machine
learning models trained solely on synthetic data, we propose a method that
learns a prior model from a large <span class="highlight-title">dataset</span> of synthetic heads with diverse
identities, expressions, and viewpoints. With few input images, SynShot
fine-tunes the <span class="highlight-title">pretrain</span>ed synthetic prior to bridge the domain gap, modeling a
photorealistic head avatar that generalizes to novel expressions and
viewpoints. We model the head avatar using 3D Gaussian splatting and a
convolutional encoder-decoder that outputs Gaussian parameters in UV texture
space. To account for the different modeling complexities over parts of the
head (e.g., skin vs hair), we embed the prior with explicit control for
upsampling the number of per-part primitives. Compared to state-of-the-art
monocular methods that require thousands of real training images, SynShot
significantly improves novel view and expression synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website https://zielon.github.io/synshot/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double<span class="highlight-title">Diffusion</span>: Combining Heat <span class="highlight-title">Diffusion</span> with Denoising <span class="highlight-title">Diffusion</span> for
  Generative Learning on 3D Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Wang, Ziang Cheng, Zhenyu Li, Jiayu Yang, Haorui Ji, Pan Ji, Mehrtash Harandi, Richard Hartley, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Double<span class="highlight-title">Diffusion</span>, a novel framework that combines heat
dissipation <span class="highlight-title">diffusion</span> and denoising <span class="highlight-title">diffusion</span> for direct generative learning on
3D mesh surfaces. Our approach addresses the challenges of generating
continuous signal distributions residing on a curve manifold surface. Unlike
previous methods that rely on unrolling 3D meshes into 2D or adopting field
representations, Double<span class="highlight-title">Diffusion</span> leverages the Laplacian-Beltrami operator to
process features respecting the mesh structure. This combination enables
effective geometry-aware signal <span class="highlight-title">diffusion</span> across the underlying geometry. As
shown in Fig.1, we demonstrate that Double<span class="highlight-title">Diffusion</span> has the ability to generate
RGB signal distributions on complex 3D mesh surfaces and achieves per-category
shape-conditioned texture generation across different shape geometry. Our work
contributes a new direction in <span class="highlight-title">diffusion</span>-based generative modeling on 3D
surfaces, with potential applications in the field of 3D asset generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes: https://github.com/Wxyxixixi/DoubleDiffusion_3D_Mesh</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CULTURE3D: Cultural Landmarks and Terrain <span class="highlight-title">Dataset</span> for 3D Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zheng, Steve Zhang, Weizhe Lin, Aaron Zhang, Walterio W. Mayol-Cuevas, Junxiao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a large-scale fine-grained <span class="highlight-title">dataset</span> using
high-resolution images captured from locations worldwide. Compared to existing
<span class="highlight-title">dataset</span>s, our <span class="highlight-title">dataset</span> offers a significantly larger size and includes a higher
level of detail, making it uniquely suited for fine-grained 3D applications.
Notably, our <span class="highlight-title">dataset</span> is built using drone-captured aerial imagery, which
provides a more accurate perspective for capturing real-world site layouts and
architectural structures. By reconstructing environments with these detailed
images, our <span class="highlight-title">dataset</span> supports applications such as the <span class="highlight-title">COLMAP</span> format for
Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible
with widely-used techniques including SLAM, Multi-View Stereo, and Neural
Radiance Fields (<span class="highlight-title">NeRF</span>), enabling accurate 3D reconstructions and point clouds.
This makes it a benchmark for reconstruction and segmentation tasks. The
<span class="highlight-title">dataset</span> enables seamless integration with multi-modal data, supporting a range
of 3D applications, from architectural reconstruction to virtual tourism. Its
flexibility promotes innovation, facilitating breakthroughs in 3D modeling and
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActiveGAMER: Active GAussian Mapping through Efficient <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian
Splatting (3DGS) to achieve high-quality, real-time scene mapping and
exploration. Unlike traditional <span class="highlight-title">NeRF</span>-based methods, which are computationally
demanding and restrict active mapping performance, our approach leverages the
efficient <span class="highlight-title">rendering</span> capabilities of 3DGS, allowing effective and efficient
exploration in complex environments. The core of our system is a
<span class="highlight-title">rendering</span>-based information gain module that dynamically identifies the most
informative viewpoints for next-best-view planning, enhancing both geometric
and photometric reconstruction accuracy. ActiveGAMER also integrates a
carefully balanced framework, combining coarse-to-fine exploration,
post-refinement, and a global-local keyframe selection strategy to maximize
reconstruction completeness and fidelity. Our system autonomously explores and
reconstructs environments with state-of-the-art geometric and photometric
accuracy and completeness, significantly surpassing existing approaches in both
aspects. Extensive evaluations on benchmark <span class="highlight-title">dataset</span>s such as Replica and MP3D
highlight ActiveGAMER's effectiveness in active mapping tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super<span class="highlight-title">NeRF</span>-GAN: A Universal 3D-Consistent Super-Resolution Framework for
  Efficient and Enhanced 3D-Aware Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zheng, Linzhi Huang, Yizhou Yu, Yi Chang, Yilin Wang, Rui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural volume <span class="highlight-title">rendering</span> techniques, such as <span class="highlight-title">NeRF</span>, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
<span class="highlight-title">NeRF</span> presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose Super<span class="highlight-title">NeRF</span>-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of Super<span class="highlight-title">NeRF</span>-GAN is its
seamless integration with <span class="highlight-title">NeRF</span>-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
<span class="highlight-title">pre-train</span>ed generator capable of producing a <span class="highlight-title">NeRF</span> representation such as
tri-plane, we first perform volume <span class="highlight-title">rendering</span> to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a <span class="highlight-title">NeRF</span>
Super-Resolution module which learns a network to obtain a high-resolution
<span class="highlight-title">NeRF</span>. Next, we propose a novel Depth-Guided <span class="highlight-title">Rendering</span> process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided <span class="highlight-title">NeRF</span> <span class="highlight-title">rendering</span>. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Models Boost Low-Level Perceptual Similarity Metrics <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For full-reference image quality assessment (FR-IQA) using deep-learning
approaches, the perceptual similarity score between a distorted image and a
reference image is typically computed as a distance measure between features
extracted from a <span class="highlight-title">pretrain</span>ed CNN or more recently, a <span class="highlight-title">Transformer</span> network. Often,
these intermediate features require further fine-tuning or processing with
additional neural network layers to align the final similarity scores with
human judgments. So far, most IQA models based on foundation models have
primarily relied on the final layer or the embedding for the quality score
estimation. In contrast, this work explores the potential of utilizing the
intermediate features of these foundation models, which have largely been
unexplored so far in the design of low-level perceptual similarity metrics. We
demonstrate that the intermediate features are comparatively more effective.
Moreover, without requiring any training, these metrics can outperform both
traditional and state-of-the-art learned metrics by utilizing distance measures
between the features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025, Code: https://github.com/abhijay9/ZS-IQA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-11T00:00:00Z">2025-01-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVS-SQA: Exploring <span class="highlight-title">Self-Supervised</span> Quality Representation Learning for
  Neurally Synthesized Scenes without References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural View Synthesis (NVS), such as <span class="highlight-title">NeRF</span> and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled <span class="highlight-title">dataset</span>s, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional <span class="highlight-title">self-supervised</span>
learning predominantly relies on the "same instance, similar representation"
assumption and extensive <span class="highlight-title">dataset</span>s. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuroPump: Simultaneous Geometric and Color Rectification for Underwater
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Guo, Haoxiang Liao, Haibin Ling, Bingyao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image restoration aims to remove geometric and color distortions
due to water refraction, absorption and scattering. Previous studies focus on
restoring either color or the geometry, but to our best knowledge, not both.
However, in practice it may be cumbersome to address the two rectifications
one-by-one. In this paper, we propose NeuroPump, a <span class="highlight-title">self-supervised</span> method to
simultaneously optimize and rectify underwater geometry and color as if water
were pumped out. The key idea is to explicitly model refraction, absorption and
scattering in Neural Radiance Field (<span class="highlight-title">NeRF</span>) pipeline, such that it not only
performs simultaneous geometric and color rectification, but also enables to
synthesize novel views and optical effects by controlling the decoupled
parameters. In addition, to address issue of lack of real paired ground truth
images, we propose an underwater 360 benchmark <span class="highlight-title">dataset</span> that has real paired
(i.e., with and without water) images. Our method clearly outperforms other
baselines both quantitatively and qualitatively. Our project page is available
at: https://ygswu.github.io/NeuroPump.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-10T00:00:00Z">2025-01-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MinD-3D++: Advancing fMRI-Based 3D Reconstruction with High-Quality
  Textured Mesh Generation and a Comprehensive <span class="highlight-title">Dataset</span> <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxiong Gao, Yanwei Fu, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)
data, introduced as Recon3DMind, is of significant interest to both cognitive
neuroscience and computer vision. To advance this task, we present the fMRI-3D
<span class="highlight-title">dataset</span>, which includes data from 15 participants and showcases a total of
4,768 3D objects. The <span class="highlight-title">dataset</span> consists of two components: fMRI-Shape,
previously introduced and available at
https://huggingface.co/<span class="highlight-title">dataset</span>s/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,
proposed in this paper and available at
https://huggingface.co/<span class="highlight-title">dataset</span>s/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse
includes data from 5 subjects, 4 of whom are also part of the core set in
fMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all
accompanied by text captions. This significantly enhances the diversity and
potential applications of the <span class="highlight-title">dataset</span>. Moreover, we propose MinD-3D++, a novel
framework for decoding textured 3D visual information from fMRI signals. The
framework evaluates the feasibility of not only reconstructing 3D objects from
the human mind but also generating, for the first time, 3D textured meshes with
detailed textures from fMRI data. We establish new benchmarks by designing
metrics at the semantic, structural, and textured levels to evaluate model
performance. Furthermore, we assess the model's effectiveness in
out-of-distribution settings and analyze the attribution of the proposed 3D
pari fMRI <span class="highlight-title">dataset</span> in visual regions of interest (ROIs) in fMRI signals. Our
experiments demonstrate that MinD-3D++ not only reconstructs 3D objects with
high semantic and spatial accuracy but also provides deeper insights into how
the human brain processes 3D visual information. Project page:
https://jianxgao.github.io/MinD-3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of "MinD-3D: Reconstruct High-quality 3D objects in
  Human Brain", ECCV 2024 (arXiv: 2312.07485)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Masked Mesh Learning for Unsupervised Anomaly Detection
  on 3D Cortical Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chun Yang, Sicheng Dai, Saige Rutherford, Christian Gaser, Andre F Marquand, Christian F Beckmann, Thomas Wolfers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection in brain imaging is challenging. In this
paper, we propose a <span class="highlight-title">self-supervised</span> masked mesh learning for unsupervised
anomaly detection in 3D cortical surfaces. Our framework leverages the
intrinsic geometry of the cortical surface to learn a <span class="highlight-title">self-supervised</span>
representation that captures the underlying structure of the brain. We
introduce a masked mesh convolutional neural network (MMN) that learns to
predict masked regions of the cortical surface. By training the MMN on a large
<span class="highlight-title">dataset</span> of healthy subjects, we learn a representation that captures the normal
variation in the cortical surface. We then use this representation to detect
anomalies in unseen individuals by calculating anomaly scores based on the
reconstruction error of the MMN. We evaluate our framework by training on
population-scale <span class="highlight-title">dataset</span> UKB and HCP-Aging and testing on two <span class="highlight-title">dataset</span>s of
Alzheimer's disease patients ADNI and OASIS3. Our results show that our
framework can detect anomalies in cortical thickness, cortical volume, and
cortical sulcus features, which are known to be sensitive biomarkers for
Alzheimer's disease. Our proposed framework provides a promising approach for
unsupervised anomaly detection based on normative variation of cortical
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity
  Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due
to its high-quality <span class="highlight-title">rendering</span>, and ultra-fast training and <span class="highlight-title">rendering</span> speed.
However, due to the unstructured and irregular nature of Gaussian point clouds,
it is difficult to guarantee geometric reconstruction accuracy and multi-view
consistency simply by relying on image reconstruction loss. Although many
studies on surface reconstruction based on 3DGS have emerged recently, the
quality of their meshes is generally unsatisfactory. To address this problem,
we propose a fast planar-based Gaussian splatting reconstruction representation
(PGSR) to achieve high-fidelity surface reconstruction while ensuring
high-quality <span class="highlight-title">rendering</span>. Specifically, we first introduce an unbiased depth
<span class="highlight-title">rendering</span> method, which directly renders the distance from the camera origin to
the Gaussian plane and the corresponding normal map based on the Gaussian
distribution of the point cloud, and divides the two to obtain the unbiased
depth. We then introduce single-view geometric, multi-view photometric, and
geometric regularization to preserve global geometric accuracy. We also propose
a camera exposure compensation model to cope with scenes with large
illumination variations. Experiments on indoor and outdoor scenes show that our
method achieves fast training and <span class="highlight-title">rendering</span> while maintaining high-fidelity
<span class="highlight-title">rendering</span> and geometric reconstruction, outperforming 3DGS-based and <span class="highlight-title">NeRF</span>-based
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://zju3dv.github.io/pgsr/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UV-Attack: Physical-World Adversarial Attacks for Person Detection via
  Dynamic-<span class="highlight-title">NeRF</span>-based UV Mapping <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Wenxuan Zhang, Kaisheng Liang, Bin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (<span class="highlight-title">NeRF</span>) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-<span class="highlight-title">NeRF</span>-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic <span class="highlight-title">NeRF</span> models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.75%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic <span class="highlight-title">NeRF</span>-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 22 figures, submitted to ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity
  Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due
to its high-quality <span class="highlight-title">rendering</span>, and ultra-fast training and <span class="highlight-title">rendering</span> speed.
However, due to the unstructured and irregular nature of Gaussian point clouds,
it is difficult to guarantee geometric reconstruction accuracy and multi-view
consistency simply by relying on image reconstruction loss. Although many
studies on surface reconstruction based on 3DGS have emerged recently, the
quality of their meshes is generally unsatisfactory. To address this problem,
we propose a fast planar-based Gaussian splatting reconstruction representation
(PGSR) to achieve high-fidelity surface reconstruction while ensuring
high-quality <span class="highlight-title">rendering</span>. Specifically, we first introduce an unbiased depth
<span class="highlight-title">rendering</span> method, which directly renders the distance from the camera origin to
the Gaussian plane and the corresponding normal map based on the Gaussian
distribution of the point cloud, and divides the two to obtain the unbiased
depth. We then introduce single-view geometric, multi-view photometric, and
geometric regularization to preserve global geometric accuracy. We also propose
a camera exposure compensation model to cope with scenes with large
illumination variations. Experiments on indoor and outdoor scenes show that our
method achieves fast training and <span class="highlight-title">rendering</span> while maintaining high-fidelity
<span class="highlight-title">rendering</span> and geometric reconstruction, outperforming 3DGS-based and <span class="highlight-title">NeRF</span>-based
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://zju3dv.github.io/pgsr/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backdoor Attacks against No-Reference Image Quality Assessment Models
  via a Scalable Trigger <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the
quality of a single input image without using any reference, plays a critical
role in evaluating and optimizing computer vision systems, e.g., low-light
enhancement. Recent research indicates that NR-IQA models are susceptible to
adversarial attacks, which can significantly alter predicted scores with
visually imperceptible perturbations. Despite revealing vulnerabilities, these
attack methods have limitations, including high computational demands,
untargeted manipulation, limited practical utility in white-box scenarios, and
reduced effectiveness in black-box scenarios. To address these challenges, we
shift our focus to another significant threat and present a novel
poisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker
to manipulate the IQA model's output to any desired target value by simply
adjusting a scaling coefficient $\alpha$ for the trigger. We propose to inject
the trigger in the discrete cosine transform (DCT) domain to improve the local
invariance of the trigger for countering trigger diminishment in NR-IQA models
due to widely adopted data augmentations. Furthermore, the universal
adversarial perturbations (UAP) in the DCT space are designed as the trigger,
to increase IQA model susceptibility to manipulation and improve attack
effectiveness. In addition to the heuristic method for poison-label BAIQA
(P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on
$\alpha$ sampling and image data refinement, driven by theoretical insights we
reveal. Extensive experiments on diverse <span class="highlight-title">dataset</span>s and various NR-IQA models
demonstrate the effectiveness of our attacks. Code can be found at
https://github.com/yuyi-sd/BAIQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Sample Generation of <span class="highlight-title">Diffusion</span> Models using Noise Level
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abulikemu Abuduweili, Chenyang Yuan, Changliu Liu, Frank Permenter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The denoising process of <span class="highlight-title">diffusion</span> models can be interpreted as an
approximate projection of noisy samples onto the data manifold. Moreover, the
noise level in these samples approximates their distance to the underlying
manifold. Building on this insight, we propose a novel method to enhance sample
generation by aligning the estimated noise level with the true distance of
noisy samples to the manifold. Specifically, we introduce a noise level
correction network, leveraging a <span class="highlight-title">pre-train</span>ed denoising network, to refine noise
level estimates during the denoising process. Additionally, we extend this
approach to various image restoration tasks by integrating task-specific
constraints, including inpainting, deblurring, super-resolution, colorization,
and compressed sensing. Experimental results demonstrate that our method
significantly improves sample quality in both unconstrained and constrained
generation scenarios. Notably, the proposed noise level correction framework is
compatible with existing denoising schedulers (e.g., DDIM), offering additional
performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-09T00:00:00Z">2025-01-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daxuan Ren, Hezi Shi, Jianmin Zheng, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iso-surface extraction from an implicit field is a fundamental process in
various applications of computer vision and graphics. When dealing with
geometric shapes with complicated geometric details, many existing algorithms
suffer from high computational costs and memory usage. This paper proposes
McGrids, a novel approach to improve the efficiency of iso-surface extraction.
The key idea is to construct adaptive grids for iso-surface extraction rather
than using a simple uniform grid as prior art does. Specifically, we formulate
the problem of constructing adaptive grids as a probability sampling problem,
which is then solved by Monte Carlo process. We demonstrate McGrids' capability
with extensive experiments from both analytical SDFs computed from surface
meshes and learned implicit fields from real multiview images. The experiment
results show that our McGrids can significantly reduce the number of implicit
field queries, resulting in significant memory reduction, while producing
high-quality meshes with rich geometric details.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Zhang, Jing Lin, Ailing Zeng, Guanlin Wu, Shunlin Lu, Yurong Fu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Motion-X++, a large-scale multimodal 3D
expressive whole-body human motion <span class="highlight-title">dataset</span>. Existing motion <span class="highlight-title">dataset</span>s
predominantly capture body-only poses, lacking facial expressions, hand
gestures, and fine-grained pose descriptions, and are typically limited to lab
settings with manually labeled text descriptions, thereby restricting their
scalability. To address this issue, we develop a scalable annotation pipeline
that can automatically capture 3D whole-body human motion and comprehensive
textural labels from RGB videos and build the Motion-X <span class="highlight-title">dataset</span> comprising 81.1K
text-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving
the annotation pipeline, introducing more data modalities, and scaling up the
data quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations
covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K
audios, 19.5M frame-level whole-body pose descriptions, and 120.5K
sequence-level semantic labels. Comprehensive experiments validate the accuracy
of our annotation pipeline and highlight Motion-X++'s significant benefits for
generating expressive, precise, and natural motion with paired multimodal
labels supporting several downstream tasks, including text-driven whole-body
motion generation,audio-driven motion generation, 3D whole-body human mesh
recovery, and 2D whole-body keypoints estimation, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures, This work extends and enhances the research
  published in the NeurIPS 2023 paper, "Motion-X: A Large-scale 3D Expressive
  Whole-body Human Motion Dataset". arXiv admin note: substantial text overlap
  with arXiv:2307.00818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Mesh Completion to AI Designed Crown 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Golriz Hosseinimanesh, Farnoosh Ghadiri, Francois Guibault, Farida Cheriet, Julia Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a dental crown is a time-consuming and labor intensive process. Our
goal is to simplify crown design and minimize the tediousness of making manual
adjustments while still ensuring the highest level of accuracy and consistency.
To this end, we present a new end- to-end deep learning approach, coined Dental
Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud
context. The dental context includes the tooth prepared to receive a crown and
its surroundings, namely the two adjacent teeth and the three closest teeth in
the opposing jaw. We formulate crown generation in terms of completing this
point cloud context. A feature extractor first converts the input point cloud
into a set of feature vectors that represent local regions in the point cloud.
The set of feature vectors is then fed into a <span class="highlight-title">transformer</span> to predict a new set
of feature vectors for the missing region (crown). Subsequently, a point
reconstruction head, followed by a multi-layer perceptron, is used to predict a
dense set of points with normals. Finally, a differentiable point-to-mesh layer
serves to reconstruct the crown surface mesh. We compare our DMC method to a
graph-based convolutional neural network which learns to deform a crown mesh
from a generic crown shape to the target geometry. Extensive experiments on our
<span class="highlight-title">dataset</span> demonstrate the effectiveness of our method, which attains an average
of 0.062 Chamfer Distance.The code is available
at:https://github.com/Golriz-code/DMC.gi
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daxuan Ren, Hezi Shi, Jianmin Zheng, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iso-surface extraction from an implicit field is a fundamental process in
various applications of computer vision and graphics. When dealing with
geometric shapes with complicated geometric details, many existing algorithms
suffer from high computational costs and memory usage. This paper proposes
McGrids, a novel approach to improve the efficiency of iso-surface extraction.
The key idea is to construct adaptive grids for iso-surface extraction rather
than using a simple uniform grid as prior art does. Specifically, we formulate
the problem of constructing adaptive grids as a probability sampling problem,
which is then solved by Monte Carlo process. We demonstrate McGrids' capability
with extensive experiments from both analytical SDFs computed from surface
meshes and learned implicit fields from real multiview images. The experiment
results show that our McGrids can significantly reduce the number of implicit
field queries, resulting in significant memory reduction, while producing
high-quality meshes with rich geometric details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> as Shader: 3D-aware Video <span class="highlight-title">Diffusion</span> for Versatile Video
  Generation Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Diffusion</span> models have demonstrated impressive performance in generating
high-quality videos from text <span class="highlight-title">prompt</span>s or images. However, precise control over
the video generation process, such as camera manipulation or content editing,
remains a significant challenge. Existing methods for controlled video
generation are typically limited to a single control type, lacking the
flexibility to handle diverse control demands. In this paper, we introduce
<span class="highlight-title">Diffusion</span> as Shader (DaS), a novel approach that supports multiple video
control tasks within a unified architecture. Our key insight is that achieving
versatile video control necessitates leveraging 3D control signals, as videos
are fundamentally 2D <span class="highlight-title">rendering</span>s of dynamic 3D content. Unlike prior methods
limited to 2D control signals, DaS leverages 3D tracking videos as control
inputs, making the video <span class="highlight-title">diffusion</span> process inherently 3D-aware. This innovation
allows DaS to achieve a wide range of video controls by simply manipulating the
3D tracking videos. A further advantage of using 3D tracking videos is their
ability to effectively link frames, significantly enhancing the temporal
consistency of the generated videos. With just 3 days of fine-tuning on 8 H800
GPUs using less than 10k videos, DaS demonstrates strong control capabilities
across diverse tasks, including mesh-to-video generation, camera control,
motion transfer, and object manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://igl-hkust.github.io/das/ Codes:
  https://github.com/IGL-HKUST/DiffusionAsShader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EndoPerfect: A Hybrid <span class="highlight-title">NeRF</span>-Stereo Vision Approach Pioneering Monocular
  Depth Estimation and 3D Reconstruction in Endoscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04041v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04041v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Chen, Wenhao Li, Nicole Gunderson, Jeremy Ruthberg, Randall Bly, Zhenglong Sun, Waleed M. Abuzeid, Eric J. Seibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional
accuracy, with the mean error and standard deviation necessitating within the
range of a single CT slice (0.625 mm), as the critical structures in the nasal
cavity are situated within submillimeter distances from surgical instruments.
This poses a formidable challenge when using conventional monocular endoscopes.
Depth estimation is crucial for 3D reconstruction, yet existing depth
estimation methodologies either suffer from inherent accuracy limitations or,
in the case of learning-based approaches, perform poorly when applied to ESS
despite succeeding on their original <span class="highlight-title">dataset</span>s. In this study, we present a
novel, highly generalizable method that combines Neural Radiance Fields (<span class="highlight-title">NeRF</span>)
and stereo depth estimation for 3D reconstruction that can derive metric
monocular depth. Our approach begins with an initial <span class="highlight-title">NeRF</span> reconstruction
yielding a coarse 3D scene, the subsequent creation of binocular pairs within
coarse 3D scene, and generation of depth maps through stereo vision, These
depth maps are used to supervise subsequent <span class="highlight-title">NeRF</span> iteration, progressively
refining <span class="highlight-title">NeRF</span> and binocular depth, the refinement process continues until the
depth maps converged. This recursive process generates high-accuracy depth maps
from monocular endoscopic video. Evaluation in synthetic endoscopy shows a
depth accuracy of 0.125 $\pm$ 0.443 mm, well within the 0.625 mm threshold.
Further clinical experiments with real endoscopic data demonstrate a mean
distance to CT mesh of 0.269 mm, representing the highest accuracy among
monocular 3D reconstruction methods in ESS.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EndoPerfect: A Hybrid <span class="highlight-title">NeRF</span>-Stereo Vision Approach Pioneering Monocular
  Depth Estimation and 3D Reconstruction in Endoscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04041v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04041v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Chen, Wenhao Li, Nicole Gunderson, Jeremy Ruthberg, Randall Bly, Zhenglong Sun, Waleed M. Abuzeid, Eric J. Seibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional
accuracy, with the mean error and standard deviation necessitating within the
range of a single CT slice (0.625 mm), as the critical structures in the nasal
cavity are situated within submillimeter distances from surgical instruments.
This poses a formidable challenge when using conventional monocular endoscopes.
Depth estimation is crucial for 3D reconstruction, yet existing depth
estimation methodologies either suffer from inherent accuracy limitations or,
in the case of learning-based approaches, perform poorly when applied to ESS
despite succeeding on their original <span class="highlight-title">dataset</span>s. In this study, we present a
novel, highly generalizable method that combines Neural Radiance Fields (<span class="highlight-title">NeRF</span>)
and stereo depth estimation for 3D reconstruction that can derive metric
monocular depth. Our approach begins with an initial <span class="highlight-title">NeRF</span> reconstruction
yielding a coarse 3D scene, the subsequent creation of binocular pairs within
coarse 3D scene, and generation of depth maps through stereo vision, These
depth maps are used to supervise subsequent <span class="highlight-title">NeRF</span> iteration, progressively
refining <span class="highlight-title">NeRF</span> and binocular depth, the refinement process continues until the
depth maps converged. This recursive process generates high-accuracy depth maps
from monocular endoscopic video. Evaluation in synthetic endoscopy shows a
depth accuracy of 0.125 $\pm$ 0.443 mm, well within the 0.625 mm threshold.
Further clinical experiments with real endoscopic data demonstrate a mean
distance to CT mesh of 0.269 mm, representing the highest accuracy among
monocular 3D reconstruction methods in ESS.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-08T00:00:00Z">2025-01-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of single-image 3D object reconstruction. Recent works
have diverged into two directions: regression-based modeling and generative
modeling. Regression methods efficiently infer visible surfaces, but struggle
with occluded regions. Generative methods handle uncertain regions better by
modeling distributions, but are computationally expensive and the generation is
often misaligned with visible surfaces. In this paper, we present SPAR3D, a
novel two-stage approach aiming to take the best of both directions. The first
stage of SPAR3D generates sparse 3D point clouds using a lightweight point
<span class="highlight-title">diffusion</span> model, which has a fast sampling speed. The second stage uses both
the sampled point cloud and the input image to create highly detailed meshes.
Our two-stage design enables probabilistic modeling of the ill-posed
single-image 3D task while maintaining high computational efficiency and great
output fidelity. Using point clouds as an intermediate representation further
allows for interactive user edits. Evaluated on diverse <span class="highlight-title">dataset</span>s, SPAR3D
demonstrates superior performance over previous state-of-the-art methods, at an
inference speed of 0.7 seconds. Project page with code and model:
https://spar3d.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using
  Gaussian Splatting with Depth-Feature Consistency <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Gaussian Splatting has sparked a new trend in the field of computer
vision. Apart from novel view synthesis, it has also been extended to the area
of multi-view reconstruction. The latest methods facilitate complete, detailed
surface reconstruction while ensuring fast training speed. However, these
methods still require dense input views, and their output quality significantly
degrades with sparse views. We observed that the Gaussian primitives tend to
overfit the few training views, leading to noisy floaters and incomplete
reconstruction surfaces. In this paper, we present an innovative sparse-view
reconstruction framework that leverages intra-view depth and multi-view feature
consistency to achieve remarkably accurate surface reconstruction.
Specifically, we utilize monocular depth ranking information to supervise the
consistency of depth distribution within patches and employ a smoothness loss
to enhance the continuity of the distribution. To achieve finer surface
reconstruction, we optimize the absolute position of depth through multi-view
projection features. Extensive experiments on DTU and BlendedMVS demonstrate
that our method outperforms state-of-the-art methods with a speedup of 60x to
200x, achieving swift and fine-grained mesh reconstruction without the need for
costly <span class="highlight-title">pre-train</span>ing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025. Project page:
  https://alvin528.github.io/FatesGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instructive3D: Editing Large Reconstruction Models with Text
  Instructions <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Kathare, Ankit Dhiman, K Vikas Gowda, Siddharth Aravindan, Shubham Monga, Basavaraja Shanthappa Vandrotti, Lokesh R Boregowda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Transformer</span> based methods have enabled users to create, modify, and
comprehend text and image data. Recently proposed Large Reconstruction Models
(LRMs) further extend this by providing the ability to generate high-quality 3D
models with the help of a single object image. These models, however, lack the
ability to manipulate or edit the finer details, such as adding standard design
patterns or changing the color and reflectance of the generated objects, thus
lacking fine-grained control that may be very helpful in domains such as
augmented reality, animation and gaming. Naively training LRMs for this purpose
would require generating precisely edited images and 3D object pairs, which is
computationally expensive. In this paper, we propose Instructive3D, a novel LRM
based model that integrates generation and fine-grained editing, through user
text <span class="highlight-title">prompt</span>s, of 3D objects into a single model. We accomplish this by adding
an adapter that performs a <span class="highlight-title">diffusion</span> process conditioned on a text <span class="highlight-title">prompt</span>
specifying edits in the triplane latent space representation of 3D object
models. Our method does not require the generation of edited 3D objects.
Additionally, Instructive3D allows us to perform geometrically consistent
modifications, as the edits done through user-defined text <span class="highlight-title">prompt</span>s are applied
to the triplane latent representation thus enhancing the versatility and
precision of 3D objects generated. We compare the objects generated by
Instructive3D and a baseline that first generates the 3D object meshes using a
standard LRM model and then edits these 3D objects using text <span class="highlight-title">prompt</span>s when
images are provided from the Objaverse LVIS <span class="highlight-title">dataset</span>. We find that Instructive3D
produces qualitatively superior 3D objects with the properties specified by the
edit <span class="highlight-title">prompt</span>s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored
  Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Yu, Xianzhi Li, Yuan Tang, Xu Han, Jinfeng Xu, Long Hu, Min Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing textured meshes from colored point clouds is an important but
challenging task. Most existing methods yield blurry-looking textures or rely
on 3D training data that are hard to acquire. Regarding this, we propose
PointDreamer, a novel framework for textured mesh reconstruction from colored
point cloud via <span class="highlight-title">diffusion</span>-based 2D inpainting. Specifically, we first
reconstruct an untextured mesh. Next, we project the input point cloud into 2D
space to generate sparse multi-view images, and then inpaint empty pixels
utilizing a <span class="highlight-title">pre-train</span>ed 2D <span class="highlight-title">diffusion</span> model. After that, we unproject the colors
of the inpainted dense images onto the untextured mesh, thus obtaining the
final textured mesh. This project-inpaint-unproject pipeline bridges the gap
between 3D point clouds and 2D <span class="highlight-title">diffusion</span> models for the first time. Thanks to
the powerful 2D <span class="highlight-title">diffusion</span> model <span class="highlight-title">pre-train</span>ed on extensive 2D data, PointDreamer
reconstructs clear, high-quality textures with high robustness to sparse or
noisy input. Also, it's zero-shot requiring no extra training. In addition, we
design Non-Border-First unprojection strategy to address the border-area
inconsistency issue, which is less explored but commonly-occurred in methods
that generate 3D textures from multiview images. Extensive qualitative and
quantitative experiments on various synthetic and real-scanned <span class="highlight-title">dataset</span>s show
the SoTA performance of PointDreamer, by significantly outperforming baseline
methods with 30% improvement in LPIPS score (from 0.118 to 0.068). Code at:
https://github.com/YuQiao0303/PointDreamer.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MB-TaylorFormer V2: Improved Multi-branch Linear <span class="highlight-title">Transformer</span> Expanded by
  Taylor Formula for Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Jin, Yuwei Qiu, Kaihao Zhang, Hongdong Li, Wenhan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, <span class="highlight-title">Transformer</span> networks have demonstrated outstanding performance in
the field of image restoration due to the global receptive field and
adaptability to input. However, the quadratic computational complexity of
Softmax-attention poses a significant limitation on its extensive application
in image restoration tasks, particularly for high-resolution images. To tackle
this challenge, we propose a novel variant of the <span class="highlight-title">Transformer</span>. This variant
leverages the Taylor expansion to approximate the Softmax-attention and
utilizes the concept of norm-preserving mapping to approximate the remainder of
the first-order Taylor expansion, resulting in a linear computational
complexity. Moreover, we introduce a multi-branch architecture featuring
multi-scale patch embedding into the proposed <span class="highlight-title">Transformer</span>, which has four
distinct advantages: 1) various sizes of the receptive field; 2) multi-level
semantic information; 3) flexible shapes of the receptive field; 4) accelerated
training and inference speed. Hence, the proposed model, named the second
version of Taylor formula expansion-based <span class="highlight-title">Transformer</span> (for short
MB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine
features, capture long-distance pixel interactions with limited computational
cost, and improve the approximation of the Taylor expansion remainder.
Experimental results across diverse image restoration benchmarks demonstrate
that MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image
restoration tasks, such as image dehazing, deraining, desnowing, motion
deblurring, and denoising, with very little computational overhead. The source
code is available at https://github.com/FVL2020/MB-TaylorFormerV2.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-07T00:00:00Z">2025-01-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeshConv3D: Efficient convolution and pooling operators for triangular
  3D meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germain Bregeon, Marius Preda, Radu Ispas, Titus Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have been pivotal in various 2D image
analysis tasks, including computer vision, image indexing and retrieval or
semantic classification. Extending CNNs to 3D data such as point clouds and 3D
meshes raises significant challenges since the very basic convolution and
pooling operators need to be completely re-visited and re-defined in an
appropriate manner to tackle irregular connectivity issues. In this paper, we
introduce MeshConv3D, a 3D mesh-dedicated methodology integrating specialized
convolution and face collapse-based pooling operators. MeshConv3D operates
directly on meshes of arbitrary topology, without any need of prior
re-meshing/conversion techniques. In order to validate our approach, we have
considered a semantic classification task. The experimental results obtained on
three distinct benchmark <span class="highlight-title">dataset</span>s show that the proposed approach makes it
possible to achieve equivalent or superior classification results, while
minimizing the related memory footprint and computational load.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google
  Earth and Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Gao, Liangzhi Li, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently released open-source <span class="highlight-title">pre-train</span>ed foundational image segmentation and
object detection models (SAM2+GroundingDINO) allow for geometrically consistent
segmentation of objects of interest in multi-view 2D images. Users can use
text-based or click-based <span class="highlight-title">prompt</span>s to segment objects of interest without
requiring labeled training <span class="highlight-title">dataset</span>s. Gaussian Splatting allows for the learning
of the 3D representation of a scene's geometry and radiance based on 2D images.
Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and
our improvements in mask refinement based on morphological operations and
contour simplification, we created a pipeline to extract the 3D mesh of any
building based on its name, address, or geographic coordinates.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">NeRF</span>s are Mirror Detectors: Using Structural Similarity for Multi-View
  Mirror Scene Reconstruction with 3D Surface Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leif Van Holland, Michael Weinmann, Jan U. Müller, Patrick Stotko, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While neural radiance fields (<span class="highlight-title">NeRF</span>) led to a breakthrough in photorealistic
novel view synthesis, handling mirroring surfaces still denotes a particular
challenge as they introduce severe inconsistencies in the scene representation.
Previous attempts either focus on reconstructing single reflective objects or
rely on strong supervision guidance in terms of additional user-provided
annotations of visible image regions of the mirrors, thereby limiting the
practical usability. In contrast, in this paper, we present <span class="highlight-title">NeRF</span>-MD, a method
which shows that <span class="highlight-title">NeRF</span>s can be considered as mirror detectors and which is
capable of reconstructing neural radiance fields of scenes containing mirroring
surfaces without the need for prior annotations. To this end, we first compute
an initial estimate of the scene geometry by training a standard <span class="highlight-title">NeRF</span> using a
depth reprojection loss. Our key insight lies in the fact that parts of the
scene corresponding to a mirroring surface will still exhibit a significant
photometric inconsistency, whereas the remaining parts are already
reconstructed in a plausible manner. This allows us to detect mirror surfaces
by fitting geometric primitives to such inconsistent regions in this initial
stage of the training. Using this information, we then jointly optimize the
radiance field and mirror geometry in a second training stage to refine their
quality. We demonstrate the capability of our method to allow the faithful
detection of mirrors in the scene as well as the reconstruction of a single
consistent scene representation, and demonstrate its potential in comparison to
baseline and mirror-aware approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralSVG: An Implicit Representation for Text-to-Vector Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector graphics are essential in design, providing artists with a versatile
medium for creating resolution-independent and highly editable visual content.
Recent advancements in vision-language and <span class="highlight-title">diffusion</span> models have fueled
interest in text-to-vector graphics generation. However, existing approaches
often suffer from over-parameterized outputs or treat the layered structure - a
core feature of vector graphics - as a secondary goal, diminishing their
practical use. Recognizing the importance of layered SVG representations, we
propose NeuralSVG, an implicit neural representation for generating vector
graphics from text <span class="highlight-title">prompt</span>s. Inspired by Neural Radiance Fields (<span class="highlight-title">NeRF</span>s),
NeuralSVG encodes the entire scene into the weights of a small <span class="highlight-title">MLP</span> network,
optimized using Score Distillation Sampling (SDS). To encourage a layered
structure in the generated SVG, we introduce a dropout-based regularization
technique that strengthens the standalone meaning of each shape. We
additionally demonstrate that utilizing a neural representation provides an
added benefit of inference-time control, enabling users to dynamically adapt
the generated SVG based on user-provided inputs, all with a single learned
representation. Through extensive qualitative and quantitative evaluations, we
demonstrate that NeuralSVG outperforms existing methods in generating
structured and flexible SVG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sagipolaczek.github.io/NeuralSVG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Yang, Hengyu Liu, Chenxin Li, Yining Sun, Wuyang Li, Yifan Liu, Yiyang Lin, Yixuan Yuan, Nanyang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of 3D reconstruction technology, the widespread
distribution of 3D data has become a future trend. While traditional visual
data (such as images and videos) and <span class="highlight-title">NeRF</span>-based formats already have mature
techniques for copyright protection, steganographic techniques for the emerging
3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address
this, we propose ConcealGS, an innovative method for embedding implicit
information into 3D-GS. By introducing the knowledge distillation and gradient
optimization strategy based on 3D-GS, ConcealGS overcomes the limitations of
<span class="highlight-title">NeRF</span>-based models and enhances the robustness of implicit information and the
quality of 3D reconstruction. We evaluate ConcealGS in various potential
application scenarios, and experimental results have demonstrated that
ConcealGS not only successfully recovers implicit information but also has
almost no impact on <span class="highlight-title">rendering</span> quality, providing a new approach for embedding
invisible and recoverable information into 3D models in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AE-<span class="highlight-title">NeRF</span>: Augmenting Event-Based Neural Radiance Fields for Non-ideal
  Conditions and Larger Scene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing <span class="highlight-title">NeRF</span> from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-<span class="highlight-title">NeRF</span> to address the challenges of learning event-based
<span class="highlight-title">NeRF</span> from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based <span class="highlight-title">NeRF</span>
(e-<span class="highlight-title">NeRF</span>) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-<span class="highlight-title">NeRF</span> network and a vanilla e-<span class="highlight-title">NeRF</span> network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event <span class="highlight-title">dataset</span>s. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-06T00:00:00Z">2025-01-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian
  Splatting Based Data Augmentation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentian Qu, Jiahe Li, Jian Cheng, Jian Shi, Chenyu Meng, Cuixia Ma, Hongan Wang, Xiaoming Deng, Yinda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding of bimanual hand-object interaction plays an important role in
robotics and virtual reality. However, due to significant occlusions between
hands and object as well as the high degree-of-freedom motions, it is
challenging to collect and annotate a high-quality, large-scale <span class="highlight-title">dataset</span>, which
prevents further improvement of bimanual hand-object interaction-related
baselines. In this work, we propose a new 3D Gaussian Splatting based data
augmentation framework for bimanual hand-object interaction, which is capable
of augmenting existing <span class="highlight-title">dataset</span> to large-scale photorealistic data with various
hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects
and hands, and to deal with the <span class="highlight-title">rendering</span> blur problem due to multi-resolution
input images used, we design a super-resolution module. Second, we extend the
single hand grasping pose optimization module for the bimanual hand object to
generate various poses of bimanual hand-object interaction, which can
significantly expand the pose distribution of the <span class="highlight-title">dataset</span>. Third, we conduct an
analysis for the impact of different aspects of the proposed data augmentation
on the understanding of the bimanual hand-object interaction. We perform our
data augmentation on two benchmarks, H2O and Arctic, and verify that our method
can improve the performance of the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCRREAM : SCan, Register, REnder And Map:A Framework for Annotating
  Accurate and Dense 3D Indoor Scenes with a Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyunJun Jung, Weihang Li, Shun-Cheng Wu, William Bittner, Nikolas Brasch, Jifei Song, Eduardo Pérez-Pellitero, Zhensong Zhang, Arthur Moreau, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, 3d indoor <span class="highlight-title">dataset</span>s have generally prioritized scale over
ground-truth accuracy in order to obtain improved generalization. However,
using these <span class="highlight-title">dataset</span>s to evaluate dense geometry tasks, such as depth <span class="highlight-title">rendering</span>,
can be problematic as the meshes of the <span class="highlight-title">dataset</span> are often incomplete and may
produce wrong ground truth to evaluate the details. In this paper, we propose
SCRREAM, a <span class="highlight-title">dataset</span> annotation framework that allows annotation of fully dense
meshes of objects in the scene and registers camera poses on the real image
sequence, which can produce accurate ground truth for both sparse 3D as well as
dense 3D tasks. We show the details of the <span class="highlight-title">dataset</span> annotation pipeline and
showcase four possible variants of <span class="highlight-title">dataset</span>s that can be obtained from our
framework with example scenes, such as indoor reconstruction and SLAM, scene
editing & object removal, human reconstruction and 6d pose estimation. Recent
pipelines for indoor reconstruction and SLAM serve as new benchmarks. In
contrast to previous indoor <span class="highlight-title">dataset</span>, our design allows to evaluate dense
geometry tasks on eleven sample scenes against accurately rendered ground truth
depth maps.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEDiff: Latent Exposure <span class="highlight-title">Diffusion</span> for HDR Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Zhihao Xia, Thomas Leimkuehler, Karol Myszkowski, Xuaner Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While consumer displays increasingly support more than 10 stops of dynamic
range, most image assets such as internet photographs and generative AI content
remain limited to 8-bit low dynamic range (LDR), constraining their utility
across high dynamic range (HDR) applications. Currently, no generative model
can produce high-bit, high-dynamic range content in a generalizable way.
Existing LDR-to-HDR conversion methods often struggle to produce photorealistic
details and physically-plausible dynamic range in the <span class="highlight-title">clip</span>ped areas. We
introduce LEDiff, a method that enables a generative model with HDR content
generation through latent space fusion inspired by image-space exposure fusion
techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic
range of existing low-dynamic range images. Our approach uses a small HDR
<span class="highlight-title">dataset</span> to enable a <span class="highlight-title">pretrain</span>ed <span class="highlight-title">diffusion</span> model to recover detail and dynamic
range in <span class="highlight-title">clip</span>ped highlights and shadows. LEDiff brings HDR capabilities to
existing generative models and converts any LDR image to HDR, creating
photorealistic HDR outputs for image generation, image-based lighting (HDR
environment map generation), and photographic effects such as depth of field
simulation, where linear HDR data is essential for realistic quality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-05T00:00:00Z">2025-01-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BeSplat: Gaussian Splatting from a Single Blurry Image and Event Stream <span class="chip">WACV-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gopi Raju Matta, Reddypalli Trisha, Kaushik Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis has been greatly enhanced by the development of radiance
field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively
addressed key challenges, such as long training times and slow <span class="highlight-title">rendering</span>
speeds, typically associated with Neural Radiance Fields (<span class="highlight-title">NeRF</span>), while
maintaining high-quality reconstructions. In this work (BeSplat), we
demonstrate the recovery of sharp radiance field (Gaussian splats) from a
single motion-blurred image and its corresponding event stream. Our method
jointly learns the scene representation via Gaussian Splatting and recovers the
camera motion through Bezier SE(3) formulation effectively, minimizing
discrepancies between synthesized and real-world measurements of both blurry
image and corresponding event stream. We evaluate our approach on both
synthetic and real <span class="highlight-title">dataset</span>s, showcasing its ability to render view-consistent,
sharp images from the learned radiance field and the estimated camera
trajectory. To the best of our knowledge, ours is the first work to address
this highly challenging ill-posed problem in a Gaussian Splatting framework
with the effective incorporation of temporal information captured using the
event stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at EVGEN2025, WACV-25 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Super-Resolution in Neural Radiance Fields via Variational
  <span class="highlight-title">Diffusion</span> Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrey Vishen, Jatin Sarabu, Saurav Kumar, Chinmay Bharathulwar, Rithwick Lakshmanan, Vishnu Srinivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for <span class="highlight-title">diffusion</span>-guided frameworks for view-consistent
super-resolution (SR) in neural <span class="highlight-title">rendering</span>. Our approach leverages existing 2D
SR models in conjunction with advanced techniques such as Variational Score
Distilling (VSD) and a LoRA fine-tuning helper, with spatial training to
significantly boost the quality and consistency of upscaled 2D images compared
to the previous methods in the literature, such as Renoised Score Distillation
(RSD) proposed in DiSR-<span class="highlight-title">NeRF</span> (1), or SDS proposed in DreamFusion. The VSD score
facilitates precise fine-tuning of SR models, resulting in high-quality,
view-consistent images. To address the common challenge of inconsistencies
among independent SR 2D images, we integrate Iterative 3D Synchronization
(I3DS) from the DiSR-<span class="highlight-title">NeRF</span> framework. Our quantitative benchmarks and
qualitative results on the LLFF <span class="highlight-title">dataset</span> demonstrate the superior performance of
our system compared to existing methods such as DiSR-<span class="highlight-title">NeRF</span>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All our code is available at
  https://github.com/shreyvish5678/Advancing-Super-Resolution-in-Neural-Radiance-Fields-via-Variational-Diffusion-Strategies</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-04T00:00:00Z">2025-01-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Optimization for 4D Human-Scene Reconstruction in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing human motion and its surrounding environment is crucial for
understanding human-scene interaction and predicting human movements in the
scene. While much progress has been made in capturing human-scene interaction
in constrained environments, those prior methods can hardly reconstruct the
natural and diverse human motion and scene context from web videos. In this
work, we propose JOSH, a novel optimization-based method for 4D human-scene
reconstruction in the wild from monocular videos. JOSH uses techniques in both
dense scene reconstruction and human mesh recovery as initialization, and then
it leverages the human-scene contact constraints to jointly optimize the scene,
the camera poses, and the human motion. Experiment results show JOSH achieves
better results on both global human motion estimation and dense scene
reconstruction by joint optimization of scene geometry and human motion. We
further design a more efficient model, JOSH3R, and directly train it with
pseudo-labels from web videos. JOSH3R outperforms other optimization-free
methods by only training with labels predicted from JOSH, further demonstrating
its accuracy and generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://genforce.github.io/JOSH/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gen-<span class="highlight-title">NeRF</span>: Efficient and Generalizable Neural Radiance Fields via
  Algorithm-Hardware Co-Design <span class="chip">ISCA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11842v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11842v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonggan Fu, Zhifan Ye, Jiayi Yuan, Shunyao Zhang, Sixu Li, Haoran You, Yingyan Celine Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable <span class="highlight-title">NeRF</span>s is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-<span class="highlight-title">NeRF</span>, an algorithm-hardware
co-design framework dedicated to generalizable <span class="highlight-title">NeRF</span> acceleration, which for the
first time enables real-time generalizable <span class="highlight-title">NeRF</span>s. On the algorithm side,
Gen-<span class="highlight-title">NeRF</span> integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-<span class="highlight-title">NeRF</span>
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-<span class="highlight-title">NeRF</span> accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-<span class="highlight-title">NeRF</span>
framework in enabling real-time and generalizable novel view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISCA 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-03T00:00:00Z">2025-01-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^3$-Human: Dynamic Disentangled Digital Human from Monocular Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghu Chen, Bo Peng, Yunfan Tao, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce D$^3$-Human, a method for reconstructing Dynamic Disentangled
Digital Human geometry from monocular videos. Past monocular video human
reconstruction primarily focuses on reconstructing undecoupled clothed human
bodies or only reconstructing clothing, making it difficult to apply directly
in applications such as animation production. The challenge in reconstructing
decoupled clothing and body lies in the occlusion caused by clothing over the
body. To this end, the details of the visible area and the plausibility of the
invisible area must be ensured during the reconstruction process. Our proposed
method combines explicit and implicit representations to model the decoupled
clothed human body, leveraging the robustness of explicit representations and
the flexibility of implicit representations. Specifically, we reconstruct the
visible region as SDF and propose a novel human manifold signed distance field
(hmSDF) to segment the visible clothing and visible body, and then merge the
visible and invisible body. Extensive experimental results demonstrate that,
compared with existing reconstruction schemes, D$^3$-Human can achieve
high-quality decoupled reconstruction of the human body wearing different
clothing, and can be directly applied to clothing transfer and animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://ustc3dv.github.io/D3Human/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generic Objects as Pose Probes for Few-shot View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance fields including <span class="highlight-title">NeRF</span>s and 3D Gaussians demonstrate great potential
in high-fidelity <span class="highlight-title">rendering</span> and scene reconstruction, while they require a
substantial number of posed images as inputs. <span class="highlight-title">COLMAP</span> is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view <span class="highlight-title">NeRF</span> reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as "pose
probes". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume <span class="highlight-title">rendering</span> optimization
(object <span class="highlight-title">NeRF</span> and scene <span class="highlight-title">NeRF</span>) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple <span class="highlight-title">dataset</span>s. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where <span class="highlight-title">COLMAP</span> struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANTHROPOS-V: benchmarking the novel task of Crowd Volume Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Collorone, Stefano D'Arrigo, Massimiliano Pappa, Guido Maria D'Amely di Melendugno, Giovanni Ficarra, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the novel task of Crowd Volume Estimation (CVE), defined as the
process of estimating the collective body volume of crowds using only RGB
images. Besides event management and public safety, CVE can be instrumental in
approximating body weight, unlocking weight sensitive applications such as
infrastructure stress assessment, and assuring even weight balance. We propose
the first benchmark for CVE, comprising ANTHROPOS-V, a synthetic photorealistic
video <span class="highlight-title">dataset</span> featuring crowds in diverse urban environments. Its annotations
include each person's volume, SMPL shape parameters, and keypoints. Also, we
explore metrics pertinent to CVE, define baseline models adapted from Human
Mesh Recovery and Crowd Counting domains, and propose a CVE specific
methodology that surpasses baselines. Although synthetic, the weights and
heights of individuals are aligned with the real-world population distribution
across genders, and they transfer to the downstream task of CVE from real
images. Benchmark and code are available at
github.com/colloroneluca/Crowd-Volume-Estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KeyNode-Driven Geometry Coding for Real-World Scanned Human Dynamic Mesh
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huong Hoang, Truong Nguyen, Pamela Cosman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The compression of real-world scanned 3D human dynamic meshes is an emerging
research area, driven by applications such as telepresence, virtual reality,
and 3D digital streaming. Unlike synthesized dynamic meshes with fixed
topology, scanned dynamic meshes often not only have varying topology across
frames but also scan defects such as holes and outliers, increasing the
complexity of prediction and compression. Additionally, human meshes often
combine rigid and non-rigid motions, making accurate prediction and encoding
significantly more difficult compared to objects that exhibit purely rigid
motion. To address these challenges, we propose a compression method designed
for real-world scanned human dynamic meshes, leveraging embedded key nodes. The
temporal motion of each vertex is formulated as a distance-weighted combination
of transformations from neighboring key nodes, requiring the transmission of
solely the key nodes' transformations. To enhance the quality of the
KeyNode-driven prediction, we introduce an octree-based residual coding scheme
and a Dual-direction prediction mode, which uses I-frames from both directions.
Extensive experiments demonstrate that our method achieves significant
improvements over the state-of-the-art, with an average bitrate saving of
24.51% across the evaluated sequences, particularly excelling at low bitrates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberta Longhini, Marcel Büsching, Bardienus P. Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Cloth-Splatting, a method for estimating 3D states of cloth from
RGB images through a prediction-update framework. Cloth-Splatting leverages an
action-conditioned dynamics model for predicting future states and uses 3D
Gaussian Splatting to update the predicted states. Our key insight is that
coupling a 3D mesh-based representation with Gaussian Splatting allows us to
define a differentiable map between the cloth state space and the image space.
This enables the use of gradient-based optimization techniques to refine
inaccurate state estimates using only RGB supervision. Our experiments
demonstrate that Cloth-Splatting not only improves state estimation accuracy
over current baselines but also reduces convergence time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 8th Conference on Robot Learning (CoRL 2024). Code
  and videos available at: kth-rpl.github.io/cloth-splatting</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generic Objects as Pose Probes for Few-shot View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance fields including <span class="highlight-title">NeRF</span>s and 3D Gaussians demonstrate great potential
in high-fidelity <span class="highlight-title">rendering</span> and scene reconstruction, while they require a
substantial number of posed images as inputs. <span class="highlight-title">COLMAP</span> is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view <span class="highlight-title">NeRF</span> reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as "pose
probes". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume <span class="highlight-title">rendering</span> optimization
(object <span class="highlight-title">NeRF</span> and scene <span class="highlight-title">NeRF</span>) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple <span class="highlight-title">dataset</span>s. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where <span class="highlight-title">COLMAP</span> struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow Priors for Linear Inverse Problems via Iterative Corrupted
  Trajectory Matching <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18816v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18816v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasi Zhang, Peiyu Yu, Yaxuan Zhu, Yingshan Chang, Feng Gao, Ying Nian Wu, Oscar Leong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models based on flow matching have attracted significant attention
for their simplicity and superior performance in high-resolution image
synthesis. By leveraging the instantaneous change-of-variables formula, one can
directly compute image likelihoods from a learned flow, making them enticing
candidates as priors for downstream tasks such as inverse problems. In
particular, a natural approach would be to incorporate such image probabilities
in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however,
lies in the slow computation of the log-likelihood, as it requires
backpropagating through an ODE solver, which can be prohibitively slow for
high-dimensional problems. In this work, we propose an iterative algorithm to
approximate the MAP estimator efficiently to solve a variety of linear inverse
problems. Our algorithm is mathematically justified by the observation that the
MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives,
where $N$ is the number of function evaluations. By leveraging Tweedie's
formula, we show that we can perform gradient steps to sequentially optimize
these objectives. We validate our approach for various linear inverse problems,
such as super-resolution, deblurring, inpainting, and compressed sensing, and
demonstrate that we can outperform other methods based on flow matching. Code
is available at https://github.com/YasminZhang/ICTM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-02T00:00:00Z">2025-01-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmonyIQA: Pioneering Benchmark and Model for Image Harmonization
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Xu, Huiyu Duan, Guangji Ma, Liu Yang, Jiarui Wang, Qingbo Wu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image composition involves extracting a foreground object from one image and
pasting it into another image through Image harmonization algorithms (IHAs),
which aim to adjust the appearance of the foreground object to better match the
background. Existing image quality assessment (IQA) methods may fail to align
with human visual preference on image harmonization due to the insensitivity to
minor color or light inconsistency. To address the issue and facilitate the
advancement of IHAs, we introduce the first Image Quality Assessment Database
for image Harmony evaluation (HarmonyIQAD), which consists of 1,350 harmonized
images generated by 9 different IHAs, and the corresponding human visual
preference scores. Based on this database, we propose a Harmony Image Quality
Assessment (HarmonyIQA), to predict human visual preference for harmonized
images. Extensive experiments show that HarmonyIQA achieves state-of-the-art
performance on human visual preference evaluation for harmonized images, and
also achieves competing results on traditional IQA tasks. Furthermore,
cross-<span class="highlight-title">dataset</span> evaluation also shows that HarmonyIQA exhibits better
generalization ability than <span class="highlight-title">self-supervised</span> learning-based IQA methods. Both
HarmonyIQAD and HarmonyIQA will be made publicly available upon paper
publication.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-01T00:00:00Z">2025-01-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian <span class="highlight-title">NeRF</span>: Quantifying Uncertainty with Volume Density for Neural
  Implicit Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibeak Lee, Kyeongsu Kang, Seongbo Ha, Hyeonwoo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Bayesian Neural Radiance Field (<span class="highlight-title">NeRF</span>), which explicitly
quantifies uncertainty in the volume density by modeling uncertainty in the
occupancy, without the need for additional networks, making it particularly
suited for challenging observations and uncontrolled image environments. <span class="highlight-title">NeRF</span>
diverges from traditional geometric methods by providing an enriched scene
representation, <span class="highlight-title">rendering</span> color and density in 3D space from various
viewpoints. However, <span class="highlight-title">NeRF</span> encounters limitations in addressing uncertainties
solely through geometric structure information, leading to inaccuracies when
interpreting scenes with insufficient real-world observations. While previous
efforts have relied on auxiliary networks, we propose a series of formulation
extensions to <span class="highlight-title">NeRF</span> that manage uncertainties in density, both color and
density, and occupancy, all without the need for additional networks. In
experiments, we show that our method significantly enhances performance on RGB
and depth images in the comprehensive <span class="highlight-title">dataset</span>. Given that uncertainty modeling
aligns well with the inherently uncertain environments of Simultaneous
Localization and Mapping (SLAM), we applied our approach to SLAM systems and
observed notable improvements in mapping and tracking performance. These
results confirm the effectiveness of our Bayesian <span class="highlight-title">NeRF</span> approach in quantifying
uncertainty based on geometric structure, making it a robust solution for
challenging real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRIS: Inverse <span class="highlight-title">Rendering</span> of Indoor Scenes from Low Dynamic Range Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hao Lin, Jia-Bin Huang, Zhengqin Li, Zhao Dong, Christian Richardt, Tuotuo Li, Michael Zollhöfer, Johannes Kopf, Shenlong Wang, Changil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse <span class="highlight-title">rendering</span> seeks to recover 3D geometry, surface material, and
lighting from captured images, enabling advanced applications such as
novel-view synthesis, relighting, and virtual object insertion. However, most
existing techniques rely on high dynamic range (HDR) images as input, limiting
accessibility for general users. In response, we introduce IRIS, an inverse
<span class="highlight-title">rendering</span> framework that recovers the physically based material,
spatially-varying HDR lighting, and camera response functions from multi-view,
low-dynamic-range (LDR) images. By eliminating the dependence on HDR input, we
make inverse <span class="highlight-title">rendering</span> technology more accessible. We evaluate our approach on
real-world and synthetic scenes and compare it with state-of-the-art methods.
Our results show that IRIS effectively recovers HDR lighting, accurate
material, and plausible camera response functions, supporting photorealistic
relighting and object insertion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://irisldr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-30T00:00:00Z">2024-12-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">NeRF</span>-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion
  Strategies for <span class="highlight-title">NeRF</span>-based Indoor Multi-view 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Huang, Xinyang Li, Yansong Qu, Changli Wu, Xiaofan Li, Shengchuan Zhang, Liujuan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In indoor scenes, the diverse distribution of object locations and scales
makes the visual 3D perception task a big challenge.
  Previous works (e.g, <span class="highlight-title">NeRF</span>-Det) have demonstrated that implicit representation
has the capacity to benefit the visual 3D perception task in indoor scenes with
high amount of overlap between input images.
  However, previous works cannot fully utilize the advancement of implicit
representation because of fixed sampling and simple multi-view feature fusion.
  In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a
simple yet effective method, <span class="highlight-title">NeRF</span>-DetS, to address above issues. <span class="highlight-title">NeRF</span>-DetS
includes two modules: Progressive Adaptive Sampling Strategy (PASS) and
Depth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).
  Specifically,
  (1)PASS can automatically sample features of each layer within a dense 3D
detector, using offsets predicted by the previous layer.
  (2)DS-MHA can not only efficiently fuse multi-view features with strong
occlusion awareness but also reduce computational cost.
  Extensive experiments on ScanNetV2 <span class="highlight-title">dataset</span> demonstrate our <span class="highlight-title">NeRF</span>-DetS
outperforms <span class="highlight-title">NeRF</span>-Det, by achieving +5.02% and +5.92% improvement in mAP under
IoU25 and IoU50, respectively. Also, <span class="highlight-title">NeRF</span>-DetS shows consistent improvements on
ARKITScenes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Image Quality Assessment for Robotic Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbo Zhang, Chunyi Li, Liang Yuan, Guoquan Zheng, Jie Hao, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) of user-generated content (UGC) is a critical
technique for human quality of experience (QoE). However, for robot-generated
content (RGC), will its image quality be consistent with the Moravec paradox
and counter to human common sense? Human subjective scoring is more based on
the attractiveness of the image. Embodied agent are required to interact and
perceive in the environment, and finally perform specific tasks. Visual images
as inputs directly influence downstream tasks. In this paper, we first propose
an embodied image quality assessment (EIQA) frameworks. We establish assessment
metrics for input images based on the downstream tasks of robot. In addition,
we construct an Embodied Preference Database (EPD) containing 5,000 reference
and distorted image annotations. The performance of mainstream IQA algorithms
on EPD <span class="highlight-title">dataset</span> is finally verified. The experiments demonstrate that quality
assessment of embodied images is different from that of humans. We sincerely
hope that the EPD can contribute to the development of embodied AI by focusing
on image quality assessment. The benchmark is available at
https://github.com/Jianbo-maker/EPD_benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-29T00:00:00Z">2024-12-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Objects to Life: 4D generation from 3D objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ohad Rahamim, Ori Malca, Dvir Samuel, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative modeling now enable the creation of 4D
content (moving 3D objects) controlled with text <span class="highlight-title">prompt</span>s. 4D generation has
large potential in applications like virtual worlds, media, and gaming, but
existing methods provide limited control over the appearance and geometry of
generated content. In this work, we introduce a method for animating
user-provided 3D objects by conditioning on textual <span class="highlight-title">prompt</span>s to guide 4D
generation, enabling custom animations while maintaining the identity of the
original object. We first convert a 3D mesh into a ``static" 4D Neural Radiance
Field (<span class="highlight-title">NeRF</span>) that preserves the visual attributes of the input object. Then, we
animate the object using an Image-to-Video <span class="highlight-title">diffusion</span> model driven by text. To
improve motion realism, we introduce an incremental viewpoint selection
protocol for sampling perspectives to promote lifelike movement and a masked
Score Distillation Sampling (SDS) loss, which leverages attention maps to focus
optimization on relevant regions. We evaluate our model in terms of temporal
coherence, <span class="highlight-title">prompt</span> adherence, and visual fidelity and find that our method
outperforms baselines that are based on other approaches, achieving up to
threefold improvements in identity preservation measured using LPIPS scores,
and effectively balancing visual quality with dynamic content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PartGen: Part-level 3D Generation and Reconstruction with Multi-View
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text- or image-to-3D generators and 3D scanners can now produce 3D assets
with high-quality shapes and textures. These assets typically consist of a
single, fused representation, like an implicit neural field, a Gaussian
mixture, or a mesh, without any useful structure. However, most applications
and creative workflows require assets to be made of several meaningful parts
that can be manipulated independently. To address this gap, we introduce
PartGen, a novel approach that generates 3D objects composed of meaningful
parts starting from text, an image, or an unstructured 3D object. First, given
multiple views of a 3D object, generated or rendered, a multi-view <span class="highlight-title">diffusion</span>
model extracts a set of plausible and view-consistent part segmentations,
dividing the object into parts. Then, a second multi-view <span class="highlight-title">diffusion</span> model takes
each part separately, fills in the occlusions, and uses those completed views
for 3D reconstruction by feeding them to a 3D reconstruction network. This
completion process considers the context of the entire object to ensure that
the parts integrate cohesively. The generative completion model can make up for
the information missing due to occlusions; in extreme cases, it can hallucinate
entirely invisible parts based on the input 3D asset. We evaluate our method on
generated and real 3D assets and show that it outperforms segmentation and
part-extraction baselines by a large margin. We also showcase downstream
applications such as 3D part editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://silent-chen.github.io/PartGen/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Objects to Life: 4D generation from 3D objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ohad Rahamim, Ori Malca, Dvir Samuel, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative modeling now enable the creation of 4D
content (moving 3D objects) controlled with text <span class="highlight-title">prompt</span>s. 4D generation has
large potential in applications like virtual worlds, media, and gaming, but
existing methods provide limited control over the appearance and geometry of
generated content. In this work, we introduce a method for animating
user-provided 3D objects by conditioning on textual <span class="highlight-title">prompt</span>s to guide 4D
generation, enabling custom animations while maintaining the identity of the
original object. We first convert a 3D mesh into a ``static" 4D Neural Radiance
Field (<span class="highlight-title">NeRF</span>) that preserves the visual attributes of the input object. Then, we
animate the object using an Image-to-Video <span class="highlight-title">diffusion</span> model driven by text. To
improve motion realism, we introduce an incremental viewpoint selection
protocol for sampling perspectives to promote lifelike movement and a masked
Score Distillation Sampling (SDS) loss, which leverages attention maps to focus
optimization on relevant regions. We evaluate our model in terms of temporal
coherence, <span class="highlight-title">prompt</span> adherence, and visual fidelity and find that our method
outperforms baselines that are based on other approaches, achieving up to
threefold improvements in identity preservation measured using LPIPS scores,
and effectively balancing visual quality with dynamic content.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Image Restoration Using Few-Step Guidance of Consistency
  Models (and Beyond) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Garber, Tom Tirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, it has become popular to tackle image restoration tasks with
a single <span class="highlight-title">pretrain</span>ed <span class="highlight-title">diffusion</span> model (DM) and data-fidelity guidance, instead of
training a dedicated deep neural network per task. However, such "zero-shot"
restoration schemes currently require many Neural Function Evaluations (NFEs)
for performing well, which may be attributed to the many NFEs needed in the
original generative functionality of the DMs. Recently, faster variants of DMs
have been explored for image generation. These include Consistency Models
(CMs), which can generate samples via a couple of NFEs. However, existing works
that use guided CMs for restoration still require tens of NFEs or fine-tuning
of the model per task that leads to performance drop if the assumptions during
the fine-tuning are not accurate. In this paper, we propose a zero-shot
restoration scheme that uses CMs and operates well with as little as 4 NFEs. It
is based on a wise combination of several ingredients: better initialization,
back-projection guidance, and above all a novel noise injection mechanism. We
demonstrate the advantages of our approach for image super-resolution,
deblurring and inpainting. Interestingly, we show that the usefulness of our
noise injection technique goes beyond CMs: it can also mitigate the performance
degradation of existing guided DM methods when reducing their NFE count.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code can be found at: https://github.com/tirer-lab/CM4IR</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-28T00:00:00Z">2024-12-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyun Li, Haiyu Zhao, Wenxin Wang, Peng Hu, Yuanbiao Gou, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Mamba have shown promising results in image
restoration. These methods typically flatten 2D images into multiple distinct
1D sequences along rows and columns, process each sequence independently using
selective scan operation, and recombine them to form the outputs. However, such
a paradigm overlooks two vital aspects: i) the local relationships and spatial
continuity inherent in natural images, and ii) the discrepancies among
sequences unfolded through totally different ways. To overcome the drawbacks,
we explore two problems in Mamba-based restoration methods: i) how to design a
scanning strategy preserving both locality and continuity while facilitating
restoration, and ii) how to aggregate the distinct sequences unfolded in
totally different ways. To address these problems, we propose a novel
Mamba-based Image Restoration model (MaIR), which consists of Nested S-shaped
Scanning strategy (NSS) and Sequence Shuffle Attention block (SSA).
Specifically, NSS preserves locality and continuity of the input images through
the stripe-based scanning region and the S-shaped scanning path, respectively.
SSA aggregates sequences through calculating attention weights within the
corresponding channels of different sequences. Thanks to NSS and SSA, MaIR
surpasses 40 baselines across 14 challenging <span class="highlight-title">dataset</span>s, achieving
state-of-the-art performance on the tasks of image super-resolution, denoising,
deblurring and dehazing. Our codes will be available after acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Diffusion</span> Models for Inverse Problems with Covariance-Aware
  Posterior Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Mohajer Hamidi, En-Hui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse problems exist in many disciplines of science and engineering. In
computer vision, for example, tasks such as inpainting, deblurring, and super
resolution can be effectively modeled as inverse problems. Recently, denoising
<span class="highlight-title">diffusion</span> probabilistic models (DDPMs) are shown to provide a promising
solution to noisy linear inverse problems without the need for additional task
specific training. Specifically, with the prior provided by DDPMs, one can
sample from the posterior by approximating the likelihood. In the literature,
approximations of the likelihood are often based on the mean of conditional
densities of the reverse process, which can be obtained using Tweedie formula.
To obtain a better approximation to the likelihood, in this paper we first
derive a closed form formula for the covariance of the reverse process. Then,
we propose a method based on finite difference method to approximate this
covariance such that it can be readily obtained from the existing <span class="highlight-title">pretrain</span>ed
DDPMs, thereby not increasing the complexity compared to existing approaches.
Finally, based on the mean and approximated covariance of the reverse process,
we present a new approximation to the likelihood. We refer to this method as
covariance-aware <span class="highlight-title">diffusion</span> posterior sampling (CA-DPS). Experimental results
show that CA-DPS significantly improves reconstruction performance without
requiring hyperparameter tuning. The code for the paper is put in the
supplementary materials.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-27T00:00:00Z">2024-12-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharpening Neural Implicit Functions with Frequency Consolidation Priors <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Chen, Yu-Shen Liu, Zhizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
<span class="highlight-title">pre-train</span>ed models are available at \url{https://github.com/chenchao15/FCP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Transformer</span>-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
<span class="highlight-title">Transformer</span>. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP <span class="highlight-title">dataset</span>s demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Radiance Fields from a Single Snapshot Compressive Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (<span class="highlight-title">NeRF</span>). Specifically, we propose SCI<span class="highlight-title">NeRF</span>, in which we
formulate the physical imaging process of SCI as part of the training of <span class="highlight-title">NeRF</span>,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/<span class="highlight-title">rendering</span> speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the <span class="highlight-title">rendering</span> capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural Similarity in Deep Features: Image Quality Assessment Robust
  to Geometrically Disparate Reference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Quality Assessment (IQA) with references plays an important role in
optimizing and evaluating computer vision tasks. Traditional methods assume
that all pixels of the reference and test images are fully aligned. Such
Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world
problems with various geometric deformations between the two images. Although
significant effort has been made to attack Geometrically-Disparate-Reference
IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for
example, by dedicated designs for image super-resolution and retargeting, or by
assuming the geometric distortions to be small that can be countered by
translation-robust filters or by explicit image registrations. Here we rethink
this problem and propose a unified, non-training-based Deep Structural
Similarity (DeepSSIM) approach to address the above problems in a single
framework, which assesses structural similarity of deep features in a simple
but efficient way and uses an attention calibration strategy to alleviate
attention deviation. The proposed method, without application-specific design,
achieves state-of-the-art performance on AR-IQA <span class="highlight-title">dataset</span>s and meanwhile shows
strong robustness to various GDR-IQA test cases. Interestingly, our test also
shows the effectiveness of DeepSSIM as an optimization tool for training image
super-resolution, enhancement and restoration, implying an even wider
generalizability. \footnote{Source code will be made public after the <span class="highlight-title">review</span> is
completed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Adversarial Network on Motion-Blur Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro <span class="highlight-title">dataset</span> which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-26T00:00:00Z">2024-12-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Editable Head Avatars with 3D Gaussian GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating animatable and editable 3D head avatars is essential for various
applications in computer vision and graphics. Traditional 3D-aware generative
adversarial networks (GANs), often using implicit fields like Neural Radiance
Fields (<span class="highlight-title">NeRF</span>), achieve photorealistic and view-consistent 3D head synthesis.
However, these methods face limitations in deformation flexibility and
editability, hindering the creation of lifelike and easily modifiable 3D heads.
We propose a novel approach that enhances the editability and animation control
of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit
3D representation. This method enables easier illumination control and improved
editability. Central to our approach is the Editable Gaussian Head (EG-Head)
model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing
precise expression control and flexible texture editing for accurate animation
while preserving identity. To capture complex non-facial geometries like hair,
we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments
demonstrate that our approach delivers high-quality 3D-aware synthesis with
state-of-the-art controllability. Our code and models are available at
https://github.com/liguohao96/EGG3D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View
  Stereo <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonggwon Lee, Junkyu Park, Khang Truong Giang, Sungho Jo, Soohwan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of online 3D model generation for neural
<span class="highlight-title">rendering</span> using an RGB image stream. Previous research has tackled this issue
by incorporating Neural Radiance Fields (<span class="highlight-title">NeRF</span>) or 3D Gaussian Splatting (3DGS)
as scene representations within dense SLAM methods. However, most studies focus
primarily on estimating coarse 3D scenes rather than achieving detailed
reconstructions. Moreover, depth estimation based solely on images is often
ambiguous, resulting in low-quality 3D models that lead to inaccurate
<span class="highlight-title">rendering</span>s. To overcome these limitations, we propose a novel framework for
high-quality 3DGS modeling that leverages an online multi-view stereo (MVS)
approach. Our method estimates MVS depth using sequential frames from a local
time window and applies comprehensive depth refinement techniques to filter out
outliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore,
we introduce a parallelized backend module that optimizes the 3DGS model
efficiently, ensuring timely updates with each new keyframe. Experimental
results demonstrate that our method outperforms state-of-the-art dense SLAM
methods, particularly excelling in challenging outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, submitted to IEEE ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from
  Unsynchronized and Uncalibrated Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on dynamic neural field reconstruction assume input from
synchronized multi-view videos with known poses. These input constraints are
often unmet in real-world setups, making the approach impractical. We
demonstrate that unsynchronized videos with unknown poses can generate dynamic
<span class="highlight-title">neural fields</span> if the videos capture human motion. Humans are one of the most
common dynamic subjects whose poses can be estimated using state-of-the-art
methods. While noisy, the estimated human shape and pose parameters provide a
decent initialization for the highly non-convex and under-constrained problem
of training a consistent dynamic neural representation. Given the sequences of
pose and shape of humans, we estimate the time offsets between videos, followed
by camera pose estimations by analyzing 3D joint locations. Then, we train
dynamic <span class="highlight-title">NeRF</span> employing multiresolution rids while simultaneously refining both
time offsets and camera poses. The setup still involves optimizing many
parameters, therefore, we introduce a robust progressive learning strategy to
stabilize the process. Experiments show that our approach achieves accurate
spatiotemporal calibration and high-quality scene reconstruction in challenging
conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pou-Chun Kung, Xianling Zhang, Katherine A. Skinner, Nikita Jaipuria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic 3D scene reconstruction plays an important role in autonomous
driving, enabling the generation of novel data from existing <span class="highlight-title">dataset</span>s to
simulate safety-critical scenarios and expand training data without additional
acquisition costs. Gaussian Splatting (GS) facilitates real-time,
photorealistic <span class="highlight-title">rendering</span> with an explicit 3D Gaussian representation of the
scene, providing faster processing and more intuitive scene editing than the
implicit Neural Radiance Fields (<span class="highlight-title">NeRF</span>s). While extensive GS research has
yielded promising advancements in autonomous driving applications, they
overlook two critical aspects: First, existing methods mainly focus on
low-speed and feature-rich urban scenes and ignore the fact that highway
scenarios play a significant role in autonomous driving. Second, while LiDARs
are commonplace in autonomous driving platforms, existing methods learn
primarily from images and use LiDAR only for initial estimates or without
precise sensor modeling, thus missing out on leveraging the rich depth
information LiDAR offers and limiting the ability to synthesize LiDAR data. In
this paper, we propose a novel GS method for dynamic <span class="highlight-title">scene synthesis</span> and
editing with improved scene reconstruction through LiDAR supervision and
support for LiDAR <span class="highlight-title">rendering</span>. Unlike prior works that are tested mostly on urban
<span class="highlight-title">dataset</span>s, to the best of our knowledge, we are the first to focus on the more
challenging and highly relevant highway scenes for autonomous driving, with
sparse sensor views and monotone backgrounds. Visit our project page at:
https://umautobots.github.io/lihi_gs
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Model Based Visual Compensation Guidance and Visual Difference
  Analysis for No-Reference Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA)
methods still suffer from finding a balance between learning feature
information at the pixel level of the image and capturing high-level feature
information and the efficient utilization of the obtained high-level feature
information remains a challenge. As a novel class of state-of-the-art (SOTA)
generative model, the <span class="highlight-title">diffusion</span> model exhibits the capability to model
intricate relationships, enabling a comprehensive understanding of images and
possessing a better learning of both high-level and low-level visual features.
In view of these, we pioneer the exploration of the <span class="highlight-title">diffusion</span> model into the
domain of NR-IQA. Firstly, we devise a new <span class="highlight-title">diffusion</span> restoration network that
leverages the produced enhanced image and noise-containing images,
incorporating nonlinear features obtained during the denoising process of the
<span class="highlight-title">diffusion</span> model, as high-level visual information. Secondly, two visual
evaluation branches are designed to comprehensively analyze the obtained
high-level feature information. These include the visual compensation guidance
branch, grounded in the <span class="highlight-title">transformer</span> architecture and noise embedding strategy,
and the visual difference analysis branch, built on the ResNet architecture and
the residual transposed attention block. Extensive experiments are conducted on
seven public NR-IQA <span class="highlight-title">dataset</span>s, and the results demonstrate that the proposed
model outperforms SOTA methods for NR-IQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TIP</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-25T00:00:00Z">2024-12-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneously Recovering Multi-Person Meshes and Multi-View Cameras
  with Human Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buzhen Huang, Jingyi Ju, Yuan Shu, Yangang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic multi-person mesh recovery has broad applications in sports
broadcasting, virtual reality, and video games. However, current multi-view
frameworks rely on a time-consuming camera calibration procedure. In this work,
we focus on multi-person motion capture with uncalibrated cameras, which mainly
faces two challenges: one is that inter-person interactions and occlusions
introduce inherent ambiguities for both camera calibration and motion capture;
the other is that a lack of dense correspondences can be used to constrain
sparse camera geometries in a dynamic multi-person scene. Our key idea is to
incorporate motion prior knowledge to simultaneously estimate camera parameters
and human meshes from noisy human semantics. We first utilize human information
from 2D images to initialize intrinsic and extrinsic parameters. Thus, the
approach does not rely on any other calibration tools or background features.
Then, a pose-geometry consistency is introduced to associate the detected
humans from different views. Finally, a latent motion prior is proposed to
refine the camera parameters and human motions. Experimental results show that
accurate camera parameters and human motions can be obtained through a one-step
reconstruction. The code are publicly available
at~\url{https://github.com/boycehbz/DMMR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TCSVT. arXiv admin note: text overlap with arXiv:2110.10355</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAND: Hierarchical Attention Network for Multi-Scale Handwritten
  Document Recognition and Layout Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwritten document recognition (HDR) is one of the most challenging tasks
in the field of computer vision, due to the various writing styles and complex
layouts inherent in handwritten texts. Traditionally, this problem has been
approached as two separate tasks, handwritten text recognition and layout
analysis, and struggled to integrate the two processes effectively. This paper
introduces HAND (Hierarchical Attention Network for Multi-Scale Document), a
novel end-to-end and segmentation-free architecture for simultaneous text
recognition and layout analysis tasks. Our model's key components include an
advanced convolutional encoder integrating Gated Depth-wise Separable and
Octave Convolutions for robust feature extraction, a Multi-Scale Adaptive
Processing (MSAP) framework that dynamically adjusts to document complexity and
a hierarchical attention decoder with memory-augmented and sparse attention
mechanisms. These components enable our model to scale effectively from
single-line to triple-column pages while maintaining computational efficiency.
Additionally, HAND adopts curriculum learning across five complexity levels. To
improve the recognition accuracy of complex ancient manuscripts, we fine-tune
and integrate a Domain-Adaptive <span class="highlight-title">Pre-train</span>ed mT5 model for post-processing
refinement. Extensive evaluations on the READ 2016 <span class="highlight-title">dataset</span> demonstrate the
superior performance of HAND, achieving up to 59.8% reduction in CER for
line-level recognition and 31.2% for page-level recognition compared to
state-of-the-art methods. The model also maintains a compact size of 5.60M
parameters while establishing new benchmarks in both text recognition and
layout analysis. Source code and <span class="highlight-title">pre-train</span>ed models are available at :
https://github.com/MHHamdan/HAND.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-24T00:00:00Z">2024-12-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics
  Manipulation with Long-Horizon Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purposed embodied agents are designed to understand the users'
natural instructions or intentions and act precisely to complete universal
tasks. Recently, methods based on foundation models especially
Vision-Language-Action models (VLAs) have shown a substantial potential to
solve language-conditioned manipulation (LCM) tasks well. However, existing
benchmarks do not adequately meet the needs of VLAs and relative algorithms. To
better define such general-purpose tasks in the context of LLMs and advance the
research in VLAs, we present VLABench, an open-source benchmark for evaluating
universal LCM task learning. VLABench provides 100 carefully designed
categories of tasks, with strong randomization in each category of task and a
total of 2000+ objects. VLABench stands out from previous benchmarks in four
key aspects: 1) tasks requiring world knowledge and common sense transfer, 2)
natural language instructions with implicit human intentions rather than
templates, 3) long-horizon tasks demanding multi-step reasoning, and 4)
evaluation of both action policies and language model capabilities. The
benchmark assesses multiple competencies including understanding of
mesh\&texture, spatial relationship, semantic instruction, physical laws,
knowledge transfer and reasoning, etc. To support the downstream finetuning, we
provide high-quality training data collected via an automated framework
incorporating heuristic skills and prior information. The experimental results
indicate that both the current state-of-the-art <span class="highlight-title">pretrain</span>ed VLAs and the
workflow based on VLMs face challenges in our tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Lesion Tracking in 3D Total Body Photography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Lun Huang, Minghao Xue, Zhiyou Liu, Davood Tashayyod, Jun Kang, Amir Gandjbakhche, Misha Kazhdan, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melanoma is the most deadly form of skin cancer. Tracking the evolution of
nevi and detecting new lesions across the body is essential for the early
detection of melanoma. Despite prior work on longitudinal tracking of skin
lesions in 3D total body photography, there are still several challenges,
including 1) low accuracy for finding correct lesion pairs across scans, 2)
sensitivity to noisy lesion detection, and 3) lack of large-scale <span class="highlight-title">dataset</span>s with
numerous annotated lesion pairs. We propose a framework that takes in a pair of
3D textured meshes, matches lesions in the context of total body photography,
and identifies unmatchable lesions. We start by computing correspondence maps
bringing the source and target meshes to a template mesh. Using these maps to
define source/target signals over the template domain, we construct a flow
field aligning the mapped signals. The initial correspondence maps are then
refined by advecting forward/backward along the vector field. Finally, lesion
assignment is performed using the refined correspondence maps. We propose the
first large-scale <span class="highlight-title">dataset</span> for skin lesion tracking with 25K lesion pairs across
198 subjects. The proposed method achieves a success rate of 89.9% (at 10 mm
criterion) for all pairs of annotated lesions and a matching accuracy of 98.2%
for subjects with more than 200 lesions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03771v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03771v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Guo, Liwen Hu, Yuanxi Bai, Jiawei Yao, Lei Ma, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance
in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.
Fulfilling this requirement can be challenging in real-world scenarios
especially when the camera moves fast, which severely limits the application of
3DGS. To address these challenges, we proposed Spike Gausian Splatting
(SpikeGS), the first framework that integrates the spike streams into 3DGS
pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With
accumulation rasterization, interval supervision, and a specially designed
pipeline, SpikeGS extracts detailed geometry and texture from high temporal
resolution but texture lacking spike stream, reconstructs 3D scenes captured in
1 second. Extensive experiments on multiple synthetic and real-world <span class="highlight-title">dataset</span>s
demonstrate the superiority of SpikeGS compared with existing spike-based and
deblur 3D scene reconstruction methods. Codes and data will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-23T00:00:00Z">2024-12-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstructing People, Places, and Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Müller, Hongsuk Choi, Anthony Zhang, Brent Yi, Jitendra Malik, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present "Humans and Structure from Motion" (HSfM), a method for jointly
reconstructing multiple human meshes, scene point clouds, and camera parameters
in a metric world coordinate system from a sparse set of uncalibrated
multi-view images featuring people. Our approach combines data-driven scene
reconstruction with the traditional Structure-from-Motion (SfM) framework to
achieve more accurate scene reconstruction and camera estimation, while
simultaneously recovering human meshes. In contrast to existing scene
reconstruction and SfM methods that lack metric scale information, our method
estimates approximate metric scale by leveraging a human statistical model.
Furthermore, it reconstructs multiple human meshes within the same world
coordinate system alongside the scene point cloud, effectively capturing
spatial relationships among individuals and their positions in the environment.
We initialize the reconstruction of humans, scenes, and cameras using robust
foundational models and jointly optimize these elements. This joint
optimization synergistically improves the accuracy of each component. We
compare our method to existing approaches on two challenging benchmarks,
EgoHumans and EgoExo4D, demonstrating significant improvements in human
localization accuracy within the world coordinate frame (reducing error from
3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our
results show that incorporating human data into the SfM pipeline improves
camera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).
Additionally, qualitative results show that our approach improves overall scene
reconstruction quality. Our code is available at: muelea.github.io/hsfm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: muelea.github.io/hsfm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Geometry Processing via Spherical Neural Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romy Williamson, Niloy J. Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surfaces (e.g., neural map encoding, deep implicits and neural
radiance fields) have recently gained popularity because of their generic
structure (e.g., multi-layer perceptron) and easy integration with modern
learning-based setups. Traditionally, we have a rich toolbox of geometry
processing algorithms designed for polygonal meshes to analyze and operate on
surface geometry. In the absence of an analogous toolbox, neural
representations are typically discretized and converted into a mesh, before
applying any geometry processing algorithm. This is unsatisfactory and, as we
demonstrate, unnecessary. In this work, we propose a spherical neural surface
representation for genus-0 surfaces and demonstrate how to compute core
geometric operators directly on this representation. Namely, we estimate
surface normals and first and second fundamental forms of the surface, as well
as compute surface gradient, surface divergence and Laplace-Beltrami operator
on scalar/vector fields defined on the surface. Our representation is fully
seamless, overcoming a key limitation of similar explicit representations such
as Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable
geometry processing directly on the neural representations without any
unnecessary meshing. We demonstrate illustrative applications in (neural)
spectral analysis, heat flow and mean curvature flow, and evaluate robustness
to isometric shape variations. We propose theoretical formulations and validate
their numerical estimates, against analytical estimates, mesh-based baselines,
and neural alternatives, where available. By systematically linking neural
surface representations with classical geometry processing algorithms, we
believe that this work can become a key ingredient in enabling neural geometry
processing. Code will be released upon acceptance, accessible from the project
webpage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAMPER: A Dual-Stage Medical Report Generation Framework with
  Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical report generation is crucial for clinical diagnosis and patient
management, summarizing diagnoses and recommendations based on medical imaging.
However, existing work often overlook the clinical pipeline involved in report
writing, where physicians typically conduct an initial quick <span class="highlight-title">review</span> followed by
a detailed examination. Moreover, current alignment methods may lead to
misaligned relationships. To address these issues, we propose DAMPER, a
dual-stage framework for medical report generation that mimics the clinical
pipeline of report writing in two stages. In the first stage, a MeSH-Guided
Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image
features with medical subject headings (MeSH) features to generate a rough
keyphrase representation of the overall impression. In the second stage, a
Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs
hypergraphs for image patches and report annotations, modeling high-order
relationships within each modality and performing hypergraph matching to
capture semantic correlations between image regions and textual phrases.
Finally,the coarse-grained visual features, generated MeSH representations, and
visual hypergraph features are fed into a report decoder to produce the final
medical report. Extensive experiments on public <span class="highlight-title">dataset</span>s demonstrate the
effectiveness of DAMPER in generating comprehensive and accurate medical
reports, outperforming state-of-the-art methods across various evaluation
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Implicit and Explicit Representations of Radiance Fields: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Hubert, Gamal Elghazaly, Raphael Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) revolutionized novel view synthesis in recent
years by offering a new volumetric representation, which is compact and
provides high-quality image <span class="highlight-title">rendering</span>. However, the methods to edit those
radiance fields developed slower than the many improvements to other aspects of
<span class="highlight-title">NeRF</span>. With the recent development of alternative radiance field-based
representations inspired by <span class="highlight-title">NeRF</span> as well as the worldwide rise in popularity of
text-to-image models, many new opportunities and strategies have emerged to
provide radiance field editing. In this paper, we deliver a comprehensive
<span class="highlight-title">survey</span> of the different editing methods present in the literature for <span class="highlight-title">NeRF</span> and
other similar radiance field representations. We propose a new taxonomy for
classifying existing works based on their editing methodologies, <span class="highlight-title">review</span>
pioneering models, reflect on current and potential new applications of
radiance field editing, and compare state-of-the-art approaches in terms of
editing options and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Dynamic Novel View Synthesis Technologies for Cinematography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis (NVS) has shown significant promise for applications in
cinematographic production, particularly through the exploitation of Neural
Radiance Fields (<span class="highlight-title">NeRF</span>) and Gaussian Splatting (GS). These methods model real 3D
scenes, enabling the creation of new shots that are challenging to capture in
the real world due to set topology or expensive equipment requirement. This
innovation also offers cinematographic advantages such as smooth camera
movements, virtual re-shoots, slow-motion effects, etc. This paper explores
dynamic NVS with the aim of facilitating the model selection process. We
showcase its potential through a short montage filmed using various NVS models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LokiTalk: Learning Fine-Grained and Generalizable Correspondences to
  Enhance <span class="highlight-title">NeRF</span>-based Talking Head Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Li, Ruobing Zheng, Bonan Li, Zicheng Zhang, Meng Wang, Jingdong Chen, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in talking head synthesis since the introduction
of Neural Radiance Fields (<span class="highlight-title">NeRF</span>), visual artifacts and high training costs
persist as major obstacles to large-scale commercial adoption. We propose that
identifying and establishing fine-grained and generalizable correspondences
between driving signals and generated results can simultaneously resolve both
problems. Here we present LokiTalk, a novel framework designed to enhance
<span class="highlight-title">NeRF</span>-based talking heads with lifelike facial dynamics and improved training
efficiency. To achieve fine-grained correspondences, we introduce
Region-Specific Deformation Fields, which decompose the overall portrait motion
into lip movements, eye blinking, head pose, and torso movements. By
hierarchically modeling the driving signals and their associated regions
through two cascaded deformation fields, we significantly improve dynamic
accuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware
Knowledge Transfer, a plug-and-play module that learns generalizable dynamic
and static correspondences from multi-identity videos, while simultaneously
extracting ID-specific dynamic and static features to refine the depiction of
individual characters. Comprehensive evaluations demonstrate that LokiTalk
delivers superior high-fidelity results and training efficiency compared to
previous methods. The code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://digital-avatar.github.io/ai/LokiTalk/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-22T00:00:00Z">2024-12-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Di Huang, Weicai Ye, Guofeng Zhang, Wanli Ouyang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Signed Distance Function (SDF)-based volume <span class="highlight-title">rendering</span> has demonstrated
significant capabilities in surface reconstruction. Although promising,
SDF-based methods often fail to capture detailed geometric structures,
resulting in visible defects. By comparing SDF-based volume <span class="highlight-title">rendering</span> to
density-based volume <span class="highlight-title">rendering</span>, we identify two main factors within the
SDF-based approach that degrade surface quality: SDF-to-density representation
and geometric regularization. These factors introduce challenges that hinder
the optimization of the SDF field. To address these issues, we introduce
NeuRodin, a novel two-stage neural surface reconstruction framework that not
only achieves high-fidelity surface reconstruction but also retains the
flexible optimization characteristics of density-based methods. NeuRodin
incorporates innovative strategies that facilitate transformation of arbitrary
topologies and reduce artifacts associated with density bias. Extensive
evaluations on the Tanks and Temples and ScanNet++ <span class="highlight-title">dataset</span>s demonstrate the
superiority of NeuRodin, showing strong reconstruction capabilities for both
indoor and outdoor environments using solely posed RGB captures. Project
website: https://open3dvlab.github.io/NeuRodin/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Optimal Sampling for Learning SDF Using <span class="highlight-title">MLP</span>s Equipped with Positional
  Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guying Lin, Lei Yang, Yuan Liu, Congyi Zhang, Junhui Hou, Xiaogang Jin, Taku Komura, John Keyser, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit fields, such as the neural signed distance field (SDF) of a
shape, have emerged as a powerful representation for many applications, e.g.,
encoding a 3D shape and performing collision detection. Typically, implicit
fields are encoded by Multi-layer Perceptrons (<span class="highlight-title">MLP</span>) with positional encoding
(PE) to capture high-frequency geometric details. However, a notable side
effect of such PE-equipped <span class="highlight-title">MLP</span>s is the noisy artifacts present in the learned
implicit fields. While increasing the sampling rate could in general mitigate
these artifacts, in this paper we aim to explain this adverse phenomenon
through the lens of Fourier analysis. We devise a tool to determine the
appropriate sampling rate for learning an accurate neural implicit field
without undesirable side effects. Specifically, we propose a simple yet
effective method to estimate the intrinsic frequency of a given network with
randomized weights based on the Fourier analysis of the network's responses. It
is observed that a PE-equipped <span class="highlight-title">MLP</span> has an intrinsic frequency much higher than
the highest frequency component in the PE layer. Sampling against this
intrinsic frequency following the Nyquist-Sannon sampling theorem allows us to
determine an appropriate training sampling rate. We empirically show in the
setting of SDF fitting that this recommended sampling rate is sufficient to
secure accurate fitting results, while further increasing the sampling rate
would not further noticeably reduce the fitting error. Training PE-equipped
<span class="highlight-title">MLP</span>s simply with our sampling strategy leads to performances superior to the
existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Quality Assessment: Investigating Causal Perceptual Effects with
  Abductive Counterfactual Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Shen, Mingliang Zhou, Yu Chen, Xuekai Wei, Jun Luo, Huayan Pu, Weijia Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing full-reference image quality assessment (FR-IQA) methods often fail
to capture the complex causal mechanisms that underlie human perceptual
responses to image distortions, limiting their ability to generalize across
diverse scenarios. In this paper, we propose an FR-IQA method based on
abductive counterfactual inference to investigate the causal relationships
between deep network features and perceptual distortions. First, we explore the
causal effects of deep features on perception and integrate causal reasoning
with feature comparison, constructing a model that effectively handles complex
distortion types across different IQA scenarios. Second, the analysis of the
perceptual causal correlations of our proposed method is independent of the
backbone architecture and thus can be applied to a variety of deep networks.
Through abductive counterfactual experiments, we validate the proposed causal
relationships, confirming the model's superior perceptual relevance and
interpretability of quality scores. The experimental results demonstrate the
robustness and effectiveness of the method, providing competitive quality
predictions across multiple benchmarks. The source code is available at
https://anonymous.4open.science/r/DeepCausalQuality-25BC.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-21T00:00:00Z">2024-12-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUCES-MV: A Multi-View <span class="highlight-title">Dataset</span> for Near-Field Point Light Source
  Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fotios Logothetis, Ignas Budvytis, Stephan Liwicki, Roberto Cipolla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The biggest improvements in Photometric Stereo (PS) field has recently come
from adoption of differentiable volumetric <span class="highlight-title">rendering</span> techniques such as <span class="highlight-title">NeRF</span> or
Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV
benchmark. However, while there are sizeable <span class="highlight-title">dataset</span>s for environment lit
objects such as Digital Twin Catalogue (DTS), there are only several small
Photometric Stereo <span class="highlight-title">dataset</span>s which often lack challenging objects (simple,
smooth, untextured) and practical, small form factor (near-field) light setup.
  To address this, we propose LUCES-MV, the first real-world, multi-view
<span class="highlight-title">dataset</span> designed for near-field point light source photometric stereo. Our
<span class="highlight-title">dataset</span> includes 15 objects with diverse materials, each imaged under varying
light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from
the camera center. To facilitate transparent end-to-end evaluation, our <span class="highlight-title">dataset</span>
provides not only ground truth normals and ground truth object meshes and poses
but also light and camera calibration images.
  We evaluate state-of-the-art near-field photometric stereo algorithms,
highlighting their strengths and limitations across different material and
shape complexities. LUCES-MV <span class="highlight-title">dataset</span> offers an important benchmark for
developing more robust, accurate and scalable real-world Photometric Stereo
based 3D reconstruction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensing Surface Patches in Volume <span class="highlight-title">Rendering</span> for Inferring Signed
  Distance Functions <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Jiang, Tong Wu, Jing Hua, Zhizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is vital to recover 3D geometry from multi-view RGB images in many 3D
computer vision tasks. The latest methods infer the geometry represented as a
signed distance field by minimizing the <span class="highlight-title">rendering</span> error on the field through
volume <span class="highlight-title">rendering</span>. However, it is still challenging to explicitly impose
constraints on surfaces for inferring more geometry details due to the limited
ability of sensing surfaces in volume <span class="highlight-title">rendering</span>. To resolve this problem, we
introduce a method to infer signed distance functions (SDFs) with a better
sense of surfaces through volume <span class="highlight-title">rendering</span>. Using the gradients and signed
distances, we establish a small surface patch centered at the estimated
intersection along a ray by pulling points randomly sampled nearby. Hence, we
are able to explicitly impose surface constraints on the sensed surface patch,
such as multi-view photo consistency and supervision from depth or normal
priors, through volume <span class="highlight-title">rendering</span>. We evaluate our method by numerical and
visual comparisons on scene benchmarks. Our superiority over the latest methods
justifies our effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared at AAAI25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DMesh++: An Efficient Differentiable Mesh for Complex Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyun Son, Matheus Gadelha, Yang Zhou, Matthew Fisher, Zexiang Xu, Yi-Ling Qiao, Ming C. Lin, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent probabilistic methods for 3D triangular meshes capture diverse shapes
by differentiable mesh connectivity, but face high computational costs with
increased shape details. We introduce a new differentiable mesh processing
method in 2D and 3D that addresses this challenge and efficiently handles
meshes with intricate structures. Additionally, we present an algorithm that
adapts the mesh resolution to local geometry in 2D for efficient
representation. We demonstrate the effectiveness of our approach on 2D point
cloud and 3D multi-view reconstruction tasks. Visit our project page
(https://sonsang.github.io/dmesh2-project) for source code and supplementary
material.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 27 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUCES-MV: A Multi-View <span class="highlight-title">Dataset</span> for Near-Field Point Light Source
  Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fotios Logothetis, Ignas Budvytis, Stephan Liwicki, Roberto Cipolla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The biggest improvements in Photometric Stereo (PS) field has recently come
from adoption of differentiable volumetric <span class="highlight-title">rendering</span> techniques such as <span class="highlight-title">NeRF</span> or
Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV
benchmark. However, while there are sizeable <span class="highlight-title">dataset</span>s for environment lit
objects such as Digital Twin Catalogue (DTS), there are only several small
Photometric Stereo <span class="highlight-title">dataset</span>s which often lack challenging objects (simple,
smooth, untextured) and practical, small form factor (near-field) light setup.
  To address this, we propose LUCES-MV, the first real-world, multi-view
<span class="highlight-title">dataset</span> designed for near-field point light source photometric stereo. Our
<span class="highlight-title">dataset</span> includes 15 objects with diverse materials, each imaged under varying
light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from
the camera center. To facilitate transparent end-to-end evaluation, our <span class="highlight-title">dataset</span>
provides not only ground truth normals and ground truth object meshes and poses
but also light and camera calibration images.
  We evaluate state-of-the-art near-field photometric stereo algorithms,
highlighting their strengths and limitations across different material and
shape complexities. LUCES-MV <span class="highlight-title">dataset</span> offers an important benchmark for
developing more robust, accurate and scalable real-world Photometric Stereo
based 3D reconstruction methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUCES-MV: A Multi-View <span class="highlight-title">Dataset</span> for Near-Field Point Light Source
  Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fotios Logothetis, Ignas Budvytis, Stephan Liwicki, Roberto Cipolla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The biggest improvements in Photometric Stereo (PS) field has recently come
from adoption of differentiable volumetric <span class="highlight-title">rendering</span> techniques such as <span class="highlight-title">NeRF</span> or
Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV
benchmark. However, while there are sizeable <span class="highlight-title">dataset</span>s for environment lit
objects such as Digital Twin Catalogue (DTS), there are only several small
Photometric Stereo <span class="highlight-title">dataset</span>s which often lack challenging objects (simple,
smooth, untextured) and practical, small form factor (near-field) light setup.
  To address this, we propose LUCES-MV, the first real-world, multi-view
<span class="highlight-title">dataset</span> designed for near-field point light source photometric stereo. Our
<span class="highlight-title">dataset</span> includes 15 objects with diverse materials, each imaged under varying
light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from
the camera center. To facilitate transparent end-to-end evaluation, our <span class="highlight-title">dataset</span>
provides not only ground truth normals and ground truth object meshes and poses
but also light and camera calibration images.
  We evaluate state-of-the-art near-field photometric stereo algorithms,
highlighting their strengths and limitations across different material and
shape complexities. LUCES-MV <span class="highlight-title">dataset</span> offers an important benchmark for
developing more robust, accurate and scalable real-world Photometric Stereo
based 3D reconstruction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence Matters: Harnessing Video Models in 3D Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyun-kyu Ko, Dongheok Park, Youngin Park, Byeonghyeon Lee, Juhee Han, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D super-resolution aims to reconstruct high-fidelity 3D models from
low-resolution (LR) multi-view images. Early studies primarily focused on
single-image super-resolution (SISR) models to upsample LR images into
high-resolution images. However, these methods often lack view consistency
because they operate independently on each image. Although various
post-processing techniques have been extensively explored to mitigate these
inconsistencies, they have yet to fully resolve the issues. In this paper, we
perform a comprehensive study of 3D super-resolution by leveraging video
super-resolution (VSR) models. By utilizing VSR models, we ensure a higher
degree of spatial consistency and can reference surrounding spatial
information, leading to more accurate and detailed reconstructions. Our
findings reveal that VSR models can perform remarkably well even on sequences
that lack precise spatial alignment. Given this observation, we propose a
simple yet practical approach to align LR images without involving fine-tuning
or generating 'smooth' trajectory from the trained 3D models over LR images.
The experimental results show that the surprisingly simple algorithms can
achieve the state-of-the-art results of 3D super-resolution tasks on standard
benchmark <span class="highlight-title">dataset</span>s, such as the <span class="highlight-title">NeRF</span>-synthetic and Mip<span class="highlight-title">NeRF</span>-360 <span class="highlight-title">dataset</span>s.
Project page: https://ko-lani.github.io/Sequence-Matters
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ko-lani.github.io/Sequence-Matters</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-20T00:00:00Z">2024-12-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Robust Neural Reconstruction from Sparse Point Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amine Ouasfi, Shubhendu Jena, Eric Marchand, Adnane Boukhayma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the challenging problem of learning Signed Distance Functions
(SDF) from sparse and noisy 3D point clouds. In contrast to recent methods that
depend on smoothness priors, our method, rooted in a distributionally robust
optimization (DRO) framework, incorporates a regularization term that leverages
samples from the uncertainty regions of the model to improve the learned SDFs.
Thanks to tractable dual formulations, we show that this framework enables a
stable and efficient optimization of SDFs in the absence of ground truth
supervision. Using a variety of synthetic and real data evaluations from
different modalities, we show that our DRO based learning framework can improve
SDF learning with respect to baselines and the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page : https://ouasfi.github.io/sdro/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15723v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15723v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixin Xu, Jiangbei Hu, Jiaze Li, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface reconstruction from multi-view images is a core challenge in 3D
vision. Recent studies have explored signed distance fields (SDF) within Neural
Radiance Fields (<span class="highlight-title">NeRF</span>) to achieve high-fidelity surface reconstructions.
However, these approaches often suffer from slow training and <span class="highlight-title">rendering</span> speeds
compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques
attempt to fuse depth information to extract geometry from 3DGS, but frequently
result in incomplete reconstructions and fragmented surfaces. In this paper, we
introduce GSurf, a novel end-to-end method for learning a signed distance field
directly from Gaussian primitives. The continuous and smooth nature of SDF
addresses common issues in the 3DGS family, such as holes resulting from noisy
or missing depth data. By using Gaussian splatting for <span class="highlight-title">rendering</span>, GSurf avoids
the redundant volume <span class="highlight-title">rendering</span> typically required in other GS and SDF
integrations. Consequently, GSurf achieves faster training and <span class="highlight-title">rendering</span> speeds
while delivering 3D reconstruction quality comparable to neural implicit
surface methods, such as VolSDF and NeuS. Experimental results across various
benchmark <span class="highlight-title">dataset</span>s demonstrate the effectiveness of our method in producing
high-fidelity 3D reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>see https://github.com/xubaixinxbx/Gsurf</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface
  Reconstruction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surface representation has demonstrated remarkable success in the
areas of novel view synthesis and 3D reconstruction. However, assessing the
geometric quality of 3D reconstructions in the absence of ground truth mesh
remains a significant challenge, due to its <span class="highlight-title">rendering</span>-based optimization
process and entangled learning of appearance and geometry with photometric
losses. In this paper, we present a novel framework, i.e, GURecon, which
establishes a geometric uncertainty field for the neural surface based on
geometric consistency. Different from existing methods that rely on
<span class="highlight-title">rendering</span>-based measurement, GURecon models a continuous 3D uncertainty field
for the reconstructed surface, and is learned by an online distillation
approach without introducing real geometric information for supervision.
Moreover, in order to mitigate the interference of illumination on geometric
consistency, a decoupled field is learned and exploited to finetune the
uncertainty field. Experiments on various <span class="highlight-title">dataset</span>s demonstrate the superiority
of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play
extension to various neural surface representations and improvement on
downstream tasks such as incremental reconstruction. The code and supplementary
material are available on the project website:
https://zju3dv.github.io/GURecon/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025. Project page:
  https://zju3dv.github.io/GURecon/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15723v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15723v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixin Xu, Jiangbei Hu, Jiaze Li, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface reconstruction from multi-view images is a core challenge in 3D
vision. Recent studies have explored signed distance fields (SDF) within Neural
Radiance Fields (<span class="highlight-title">NeRF</span>) to achieve high-fidelity surface reconstructions.
However, these approaches often suffer from slow training and <span class="highlight-title">rendering</span> speeds
compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques
attempt to fuse depth information to extract geometry from 3DGS, but frequently
result in incomplete reconstructions and fragmented surfaces. In this paper, we
introduce GSurf, a novel end-to-end method for learning a signed distance field
directly from Gaussian primitives. The continuous and smooth nature of SDF
addresses common issues in the 3DGS family, such as holes resulting from noisy
or missing depth data. By using Gaussian splatting for <span class="highlight-title">rendering</span>, GSurf avoids
the redundant volume <span class="highlight-title">rendering</span> typically required in other GS and SDF
integrations. Consequently, GSurf achieves faster training and <span class="highlight-title">rendering</span> speeds
while delivering 3D reconstruction quality comparable to neural implicit
surface methods, such as VolSDF and NeuS. Experimental results across various
benchmark <span class="highlight-title">dataset</span>s demonstrate the effectiveness of our method in producing
high-fidelity 3D reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>see https://github.com/xubaixinxbx/Gsurf</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Quality Assessment: Enhancing Perceptual Exploration and
  Interpretation with Collaborative Feature Refinement and Hausdorff distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuekai Wei, Junyu Zhang, Qinlin Hu, Mingliang Zhou\\Yong Feng, Weizhi Xian, Huayan Pu, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current full-reference image quality assessment (FR-IQA) methods often fuse
features from reference and distorted images, overlooking that color and
luminance distortions occur mainly at low frequencies, whereas edge and texture
distortions occur at high frequencies. This work introduces a pioneering
training-free FR-IQA method that accurately predicts image quality in alignment
with the human visual system (HVS) by leveraging a novel perceptual degradation
modelling approach to address this limitation. First, a collaborative feature
refinement module employs a carefully designed wavelet transform to extract
perceptually relevant features, capturing multiscale perceptual information and
mimicking how the HVS analyses visual information at various scales and
orientations in the spatial and frequency domains. Second, a Hausdorff
distance-based distribution similarity measurement module robustly assesses the
discrepancy between the feature distributions of the reference and distorted
images, effectively handling outliers and variations while mimicking the
ability of HVS to perceive and tolerate certain levels of distortion. The
proposed method accurately captures perceptual quality differences without
requiring training data or subjective quality scores. Extensive experiments on
multiple benchmark <span class="highlight-title">dataset</span>s demonstrate superior performance compared with
existing state-of-the-art approaches, highlighting its ability to correlate
strongly with the HVS.\footnote{The code is available at
\url{https://anonymous.4open.science/r/CVPR2025-F339}.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-generated Image Quality Assessment in Visual Communication <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Yixuan Li, Baoliang Chen, Hanwei Zhu, Shiqi Wang, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the quality of artificial intelligence-generated images (AIGIs)
plays a crucial role in their application in real-world scenarios. However,
traditional image quality assessment (IQA) algorithms primarily focus on
low-level visual perception, while existing IQA works on AIGIs overemphasize
the generated content itself, neglecting its effectiveness in real-world
applications. To bridge this gap, we propose AIGI-VC, a quality assessment
database for AI-Generated Images in Visual Communication, which studies the
communicability of AIGIs in the advertising field from the perspectives of
information clarity and emotional interaction. The <span class="highlight-title">dataset</span> consists of 2,500
images spanning 14 advertisement topics and 8 emotion types. It provides
coarse-grained human preference annotations and fine-grained preference
descriptions, benchmarking the abilities of IQA methods in preference
prediction, interpretation, and reasoning. We conduct an empirical study of
existing representative IQA methods and large multi-modal models on the AIGI-VC
<span class="highlight-title">dataset</span>, uncovering their strengths and weaknesses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-2025; Project page: https://github.com/ytian73/AIGI-VC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A study on the adequacy of common IQA measures for medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19224v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19224v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Breger, Clemens Karner, Ian Selby, Janek Gröhl, Sören Dittmer, Edward Lilley, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) is standard practice in the development stage
of novel machine learning algorithms that operate on images. The most commonly
used IQA measures have been developed and tested for natural images, but not in
the medical setting. Reported inconsistencies arising in medical images are not
surprising, as they have different properties than natural images. In this
study, we test the applicability of common IQA measures for medical image data
by comparing their assessment to manually rated chest X-ray (5 experts) and
photoacoustic image data (2 experts). Moreover, we include supplementary
studies on grayscale natural images and accelerated brain MRI data. The results
of all experiments show a similar outcome in line with previous findings for
medical images: PSNR and SSIM in the default setting are in the lower range of
the result list and HaarPSI outperforms the other tested measures in the
overall performance. Also among the top performers in our experiments are the
full reference measures FSIM, LPIPS and MS-SSIM. Generally, the results on
natural images yield considerably higher correlations, suggesting that
additional employment of tailored IQA measures for medical imaging algorithms
is needed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-01-20T01:04:30.036299269Z">
            2025-01-20 01:04:30 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
