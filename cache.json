{"2025-01-15T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2501.08577v1","updated":"2025-01-15T04:56:26Z","published":"2025-01-15T04:56:26Z","title":"Scalable and High-Quality Neural Implicit Representation for 3D\n  Reconstruction","summary":"  Various SDF-based neural implicit surface reconstruction methods have been\nproposed recently, and have demonstrated remarkable modeling capabilities.\nHowever, due to the global nature and limited representation ability of a\nsingle network, existing methods still suffer from many drawbacks, such as\nlimited accuracy and scale of the reconstruction. In this paper, we propose a\nversatile, scalable and high-quality neural implicit representation to address\nthese issues. We integrate a divide-and-conquer approach into the neural\nSDF-based reconstruction. Specifically, we model the object or scene as a\nfusion of multiple independent local neural SDFs with overlapping regions. The\nconstruction of our representation involves three key steps: (1) constructing\nthe distribution and overlap relationship of the local radiance fields based on\nobject structure or data distribution, (2) relative pose registration for\nadjacent local SDFs, and (3) SDF blending. Thanks to the independent\nrepresentation of each local region, our approach can not only achieve\nhigh-fidelity surface reconstruction, but also enable scalable scene\nreconstruction. Extensive experimental results demonstrate the effectiveness\nand practicality of our proposed method.\n","authors":["Leyuan Yang","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08577v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2501.09046v1","updated":"2025-01-15T09:52:40Z","published":"2025-01-15T09:52:40Z","title":"Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A\n  Benchmark of Geometric Deep Learning Models","summary":"  Coronary artery disease, caused by the narrowing of coronary vessels due to\natherosclerosis, is the leading cause of death worldwide. The diagnostic gold\nstandard, fractional flow reserve (FFR), measures the trans-stenotic pressure\nratio during maximal vasodilation but is invasive and costly. This has driven\nthe development of virtual FFR (vFFR) using computational fluid dynamics (CFD)\nto simulate coronary flow. Geometric deep learning algorithms have shown\npromise for learning features on meshes, including cardiovascular research\napplications. This study empirically analyzes various backends for predicting\nvFFR fields in coronary arteries as CFD surrogates, comparing six backends for\nlearning hemodynamics on meshes using CFD solutions as ground truth.\n  The study has two parts: i) Using 1,500 synthetic left coronary artery\nbifurcations, models were trained to predict pressure-related fields for vFFR\nreconstruction, comparing different learning variables. ii) Using 427\npatient-specific CFD simulations, experiments were repeated focusing on the\nbest-performing learning variable from the synthetic dataset.\n  Most backends performed well on the synthetic dataset, especially when\npredicting pressure drop over the manifold. Transformer-based backends\noutperformed others when predicting pressure and vFFR fields and were the only\nmodels achieving strong performance on patient-specific data, excelling in both\naverage per-point error and vFFR accuracy in stenotic lesions.\n  These results suggest geometric deep learning backends can effectively\nreplace CFD for simple geometries, while transformer-based networks are\nsuperior for complex, heterogeneous datasets. Pressure drop was identified as\nthe optimal network output for learning pressure-related fields.\n","authors":["Guido Nannini","Julian Suk","Patryk Rygiel","Simone Saitta","Luca Mariani","Riccardo Maranga","Andrea Baggiano","Gianluca Pontone","Alberto Redaelli"],"pdf_url":"https://arxiv.org/pdf/2501.09046v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2312.11458v3","updated":"2025-01-15T22:17:24Z","published":"2023-12-18T18:59:03Z","title":"GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View\n  Synthesis","summary":"  We propose a method that achieves state-of-the-art rendering quality and\nefficiency on monocular dynamic scene reconstruction using deformable 3D\nGaussians. Implicit deformable representations commonly model motion with a\ncanonical space and time-dependent backward-warping deformation field. Our\nmethod, GauFRe, uses a forward-warping deformation to explicitly model\nnon-rigid transformations of scene geometry. Specifically, we propose a\ntemplate set of 3D Gaussians residing in a canonical space, and a\ntime-dependent forward-warping deformation field to model dynamic objects.\nAdditionally, we tailor a 3D Gaussian-specific static component supported by an\ninductive bias-aware initialization approach which allows the deformation field\nto focus on moving scene regions, improving the rendering of complex real-world\nmotion. The differentiable pipeline is optimized end-to-end with a\nself-supervised rendering loss. Experiments show our method achieves\ncompetitive results and higher efficiency than both previous state-of-the-art\nNeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20\nmins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website:\nhttps://lynl7130.github.io/gaufre/index.html\n","authors":["Yiqing Liang","Numair Khan","Zhengqin Li","Thu Nguyen-Phuoc","Douglas Lanman","James Tompkin","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.11458v3.pdf","comment":"WACV 2025. 11 pages, 8 figures, 5 tables"}],"IQA":[{"id":"http://arxiv.org/abs/2403.06406v2","updated":"2025-01-15T12:36:24Z","published":"2024-03-11T03:35:41Z","title":"When No-Reference Image Quality Models Meet MAP Estimation in Diffusion\n  Latents","summary":"  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify perceived image quality, often achieving strong\ncorrelations with human perceptual scores on standard IQA benchmarks. Yet,\nlimited efforts have been devoted to treating NR-IQA models as natural image\npriors for real-world image enhancement, and consequently comparing them from a\nperceptual optimization standpoint. In this work, we show -- for the first time\n-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by performing\ngradient ascent in the diffusion latent space rather than in the raw pixel\ndomain, leveraging a pretrained differentiable and bijective diffusion process.\nLikely, different NR-IQA models lead to different enhanced outputs, which in\nturn provides a new computational means of comparing them. Unlike conventional\ncorrelation-based measures, our comparison method offers complementary insights\ninto the respective strengths and weaknesses of the competing NR-IQA models in\nperceptual optimization scenarios. Additionally, we aim to improve the\nbest-performing NR-IQA model in diffusion latent MAP estimation by\nincorporating the advantages of other top-performing methods. The resulting\nmodel delivers noticeably better results in enhancing real-world images\nafflicted by unknown and complex distortions, all preserving a high degree of\nimage fidelity.\n","authors":["Weixia Zhang","Dingquan Li","Guangtao Zhai","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2403.06406v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.13163v5","updated":"2025-01-15T18:45:15Z","published":"2024-03-19T21:31:31Z","title":"DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual\n  Fidelity on Unseen Domains","summary":"  Recent deblurring networks have effectively restored clear images from the\nblurred ones. However, they often struggle with generalization to unknown\ndomains. Moreover, these models typically focus on distortion metrics such as\nPSNR and SSIM, neglecting the critical aspect of metrics aligned with human\nperception. To address these limitations, we propose DeblurDiNAT, a deblurring\nTransformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs\nan alternating dilation factor paradigm to capture both local and global\nblurred patterns, enhancing generalization and perceptual clarity. Second, a\nlocal cross-channel learner aids the Transformer block to understand the\nshort-range relationships between adjacent channels. Additionally, we present a\nlinear feed-forward network with a simple while effective design. Finally, a\ndual-stage feature fusion module is introduced as an alternative to the\nexisting approach, which efficiently process multi-scale visual information\nacross network levels. Compared to state-of-the-art models, our compact\nDeblurDiNAT demonstrates superior generalization capabilities and achieves\nremarkable performance in perceptual metrics, while maintaining a favorable\nmodel size.\n","authors":["Hanzhou Liu","Binghan Li","Chengkai Liu","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13163v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.03678v2","updated":"2025-01-15T06:32:05Z","published":"2022-06-08T05:04:43Z","title":"Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer","summary":"  Currently, transformer-based algorithms are making a splash in the domain of\nimage deblurring. Their achievement depends on the self-attention mechanism\nwith CNN stem to model long range dependencies between tokens. Unfortunately,\nthis ear-pleasing pipeline introduces high computational complexity and makes\nit difficult to run an ultra-high-definition image on a single GPU in real\ntime. To trade-off accuracy and efficiency, the input degraded image is\ncomputed cyclically over three dimensional ($C$, $W$, and $H$) signals without\na self-attention mechanism. We term this deep network as Multi-scale\nCubic-Mixer, which is acted on both the real and imaginary components after\nfast Fourier transform to estimate the Fourier coefficients and thus obtain a\ndeblurred image. Furthermore, we combine the multi-scale cubic-mixer with a\nslicing strategy to generate high-quality results at a much lower computational\ncost. Experimental results demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art deblurring approaches on the several\nbenchmarks and a new ultra-high-definition dataset in terms of accuracy and\nspeed.\n","authors":["Xingchi Chen","Xiuyi Jia","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2206.03678v2.pdf","comment":"9 pages"}]},"2025-01-09T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2409.06710v2","updated":"2025-01-09T07:24:09Z","published":"2024-08-25T07:55:06Z","title":"McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction","summary":"  Iso-surface extraction from an implicit field is a fundamental process in\nvarious applications of computer vision and graphics. When dealing with\ngeometric shapes with complicated geometric details, many existing algorithms\nsuffer from high computational costs and memory usage. This paper proposes\nMcGrids, a novel approach to improve the efficiency of iso-surface extraction.\nThe key idea is to construct adaptive grids for iso-surface extraction rather\nthan using a simple uniform grid as prior art does. Specifically, we formulate\nthe problem of constructing adaptive grids as a probability sampling problem,\nwhich is then solved by Monte Carlo process. We demonstrate McGrids' capability\nwith extensive experiments from both analytical SDFs computed from surface\nmeshes and learned implicit fields from real multiview images. The experiment\nresults show that our McGrids can significantly reduce the number of implicit\nfield queries, resulting in significant memory reduction, while producing\nhigh-quality meshes with rich geometric details.\n","authors":["Daxuan Ren","Hezi Shi","Jianmin Zheng","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2409.06710v2.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2501.05098v1","updated":"2025-01-09T09:37:27Z","published":"2025-01-09T09:37:27Z","title":"Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset","summary":"  In this paper, we introduce Motion-X++, a large-scale multimodal 3D\nexpressive whole-body human motion dataset. Existing motion datasets\npredominantly capture body-only poses, lacking facial expressions, hand\ngestures, and fine-grained pose descriptions, and are typically limited to lab\nsettings with manually labeled text descriptions, thereby restricting their\nscalability. To address this issue, we develop a scalable annotation pipeline\nthat can automatically capture 3D whole-body human motion and comprehensive\ntextural labels from RGB videos and build the Motion-X dataset comprising 81.1K\ntext-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving\nthe annotation pipeline, introducing more data modalities, and scaling up the\ndata quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations\ncovering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K\naudios, 19.5M frame-level whole-body pose descriptions, and 120.5K\nsequence-level semantic labels. Comprehensive experiments validate the accuracy\nof our annotation pipeline and highlight Motion-X++'s significant benefits for\ngenerating expressive, precise, and natural motion with paired multimodal\nlabels supporting several downstream tasks, including text-driven whole-body\nmotion generation,audio-driven motion generation, 3D whole-body human mesh\nrecovery, and 2D whole-body keypoints estimation, etc.\n","authors":["Yuhong Zhang","Jing Lin","Ailing Zeng","Guanlin Wu","Shunlin Lu","Yurong Fu","Yuanhao Cai","Ruimao Zhang","Haoqian Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.05098v1.pdf","comment":"17 pages, 14 figures, This work extends and enhances the research\n  published in the NeurIPS 2023 paper, \"Motion-X: A Large-scale 3D Expressive\n  Whole-body Human Motion Dataset\". arXiv admin note: substantial text overlap\n  with arXiv:2307.00818"},{"id":"http://arxiv.org/abs/2409.06710v2","updated":"2025-01-09T07:24:09Z","published":"2024-08-25T07:55:06Z","title":"McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction","summary":"  Iso-surface extraction from an implicit field is a fundamental process in\nvarious applications of computer vision and graphics. When dealing with\ngeometric shapes with complicated geometric details, many existing algorithms\nsuffer from high computational costs and memory usage. This paper proposes\nMcGrids, a novel approach to improve the efficiency of iso-surface extraction.\nThe key idea is to construct adaptive grids for iso-surface extraction rather\nthan using a simple uniform grid as prior art does. Specifically, we formulate\nthe problem of constructing adaptive grids as a probability sampling problem,\nwhich is then solved by Monte Carlo process. We demonstrate McGrids' capability\nwith extensive experiments from both analytical SDFs computed from surface\nmeshes and learned implicit fields from real multiview images. The experiment\nresults show that our McGrids can significantly reduce the number of implicit\nfield queries, resulting in significant memory reduction, while producing\nhigh-quality meshes with rich geometric details.\n","authors":["Daxuan Ren","Hezi Shi","Jianmin Zheng","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2409.06710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03847v2","updated":"2025-01-09T04:25:42Z","published":"2025-01-07T15:01:58Z","title":"Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video\n  Generation Control","summary":"  Diffusion models have demonstrated impressive performance in generating\nhigh-quality videos from text prompts or images. However, precise control over\nthe video generation process, such as camera manipulation or content editing,\nremains a significant challenge. Existing methods for controlled video\ngeneration are typically limited to a single control type, lacking the\nflexibility to handle diverse control demands. In this paper, we introduce\nDiffusion as Shader (DaS), a novel approach that supports multiple video\ncontrol tasks within a unified architecture. Our key insight is that achieving\nversatile video control necessitates leveraging 3D control signals, as videos\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\ninputs, making the video diffusion process inherently 3D-aware. This innovation\nallows DaS to achieve a wide range of video controls by simply manipulating the\n3D tracking videos. A further advantage of using 3D tracking videos is their\nability to effectively link frames, significantly enhancing the temporal\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\nacross diverse tasks, including mesh-to-video generation, camera control,\nmotion transfer, and object manipulation.\n","authors":["Zekai Gu","Rui Yan","Jiahao Lu","Peng Li","Zhiyang Dou","Chenyang Si","Zhen Dong","Qifeng Liu","Cheng Lin","Ziwei Liu","Wenping Wang","Yuan Liu"],"pdf_url":"https://arxiv.org/pdf/2501.03847v2.pdf","comment":"Project page: https://igl-hkust.github.io/das/ Codes:\n  https://github.com/IGL-HKUST/DiffusionAsShader"},{"id":"http://arxiv.org/abs/2501.04914v1","updated":"2025-01-09T02:10:15Z","published":"2025-01-09T02:10:15Z","title":"From Mesh Completion to AI Designed Crown","summary":"  Designing a dental crown is a time-consuming and labor intensive process. Our\ngoal is to simplify crown design and minimize the tediousness of making manual\nadjustments while still ensuring the highest level of accuracy and consistency.\nTo this end, we present a new end- to-end deep learning approach, coined Dental\nMesh Completion (DMC), to generate a crown mesh conditioned on a point cloud\ncontext. The dental context includes the tooth prepared to receive a crown and\nits surroundings, namely the two adjacent teeth and the three closest teeth in\nthe opposing jaw. We formulate crown generation in terms of completing this\npoint cloud context. A feature extractor first converts the input point cloud\ninto a set of feature vectors that represent local regions in the point cloud.\nThe set of feature vectors is then fed into a transformer to predict a new set\nof feature vectors for the missing region (crown). Subsequently, a point\nreconstruction head, followed by a multi-layer perceptron, is used to predict a\ndense set of points with normals. Finally, a differentiable point-to-mesh layer\nserves to reconstruct the crown surface mesh. We compare our DMC method to a\ngraph-based convolutional neural network which learns to deform a crown mesh\nfrom a generic crown shape to the target geometry. Extensive experiments on our\ndataset demonstrate the effectiveness of our method, which attains an average\nof 0.062 Chamfer Distance.The code is available\nat:https://github.com/Golriz-code/DMC.gi\n","authors":["Golriz Hosseinimanesh","Farnoosh Ghadiri","Francois Guibault","Farida Cheriet","Julia Keren"],"pdf_url":"https://arxiv.org/pdf/2501.04914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04041v4","updated":"2025-01-09T00:39:56Z","published":"2024-10-05T05:26:21Z","title":"EndoPerfect: A Hybrid NeRF-Stereo Vision Approach Pioneering Monocular\n  Depth Estimation and 3D Reconstruction in Endoscopy","summary":"  3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional\naccuracy, with the mean error and standard deviation necessitating within the\nrange of a single CT slice (0.625 mm), as the critical structures in the nasal\ncavity are situated within submillimeter distances from surgical instruments.\nThis poses a formidable challenge when using conventional monocular endoscopes.\nDepth estimation is crucial for 3D reconstruction, yet existing depth\nestimation methodologies either suffer from inherent accuracy limitations or,\nin the case of learning-based approaches, perform poorly when applied to ESS\ndespite succeeding on their original datasets. In this study, we present a\nnovel, highly generalizable method that combines Neural Radiance Fields (NeRF)\nand stereo depth estimation for 3D reconstruction that can derive metric\nmonocular depth. Our approach begins with an initial NeRF reconstruction\nyielding a coarse 3D scene, the subsequent creation of binocular pairs within\ncoarse 3D scene, and generation of depth maps through stereo vision, These\ndepth maps are used to supervise subsequent NeRF iteration, progressively\nrefining NeRF and binocular depth, the refinement process continues until the\ndepth maps converged. This recursive process generates high-accuracy depth maps\nfrom monocular endoscopic video. Evaluation in synthetic endoscopy shows a\ndepth accuracy of 0.125 $\\pm$ 0.443 mm, well within the 0.625 mm threshold.\nFurther clinical experiments with real endoscopic data demonstrate a mean\ndistance to CT mesh of 0.269 mm, representing the highest accuracy among\nmonocular 3D reconstruction methods in ESS.\n","authors":["Pengcheng Chen","Wenhao Li","Nicole Gunderson","Jeremy Ruthberg","Randall Bly","Zhenglong Sun","Waleed M. Abuzeid","Eric J. Seibel"],"pdf_url":"https://arxiv.org/pdf/2410.04041v4.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2410.04041v4","updated":"2025-01-09T00:39:56Z","published":"2024-10-05T05:26:21Z","title":"EndoPerfect: A Hybrid NeRF-Stereo Vision Approach Pioneering Monocular\n  Depth Estimation and 3D Reconstruction in Endoscopy","summary":"  3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional\naccuracy, with the mean error and standard deviation necessitating within the\nrange of a single CT slice (0.625 mm), as the critical structures in the nasal\ncavity are situated within submillimeter distances from surgical instruments.\nThis poses a formidable challenge when using conventional monocular endoscopes.\nDepth estimation is crucial for 3D reconstruction, yet existing depth\nestimation methodologies either suffer from inherent accuracy limitations or,\nin the case of learning-based approaches, perform poorly when applied to ESS\ndespite succeeding on their original datasets. In this study, we present a\nnovel, highly generalizable method that combines Neural Radiance Fields (NeRF)\nand stereo depth estimation for 3D reconstruction that can derive metric\nmonocular depth. Our approach begins with an initial NeRF reconstruction\nyielding a coarse 3D scene, the subsequent creation of binocular pairs within\ncoarse 3D scene, and generation of depth maps through stereo vision, These\ndepth maps are used to supervise subsequent NeRF iteration, progressively\nrefining NeRF and binocular depth, the refinement process continues until the\ndepth maps converged. This recursive process generates high-accuracy depth maps\nfrom monocular endoscopic video. Evaluation in synthetic endoscopy shows a\ndepth accuracy of 0.125 $\\pm$ 0.443 mm, well within the 0.625 mm threshold.\nFurther clinical experiments with real endoscopic data demonstrate a mean\ndistance to CT mesh of 0.269 mm, representing the highest accuracy among\nmonocular 3D reconstruction methods in ESS.\n","authors":["Pengcheng Chen","Wenhao Li","Nicole Gunderson","Jeremy Ruthberg","Randall Bly","Zhenglong Sun","Waleed M. Abuzeid","Eric J. Seibel"],"pdf_url":"https://arxiv.org/pdf/2410.04041v4.pdf","comment":null}]},"2025-01-03T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2408.16690v3","updated":"2025-01-03T15:05:06Z","published":"2024-08-29T16:37:58Z","title":"Generic Objects as Pose Probes for Few-shot View Synthesis","summary":"  Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance. Our project page is\navailable at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this\nhttps URL}\n","authors":["Zhirui Gao","Renjiao Yi","Chenyang Zhu","Ke Zhuang","Wei Chen","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01589v1","updated":"2025-01-03T00:58:35Z","published":"2025-01-03T00:58:35Z","title":"D$^3$-Human: Dynamic Disentangled Digital Human from Monocular Video","summary":"  We introduce D$^3$-Human, a method for reconstructing Dynamic Disentangled\nDigital Human geometry from monocular videos. Past monocular video human\nreconstruction primarily focuses on reconstructing undecoupled clothed human\nbodies or only reconstructing clothing, making it difficult to apply directly\nin applications such as animation production. The challenge in reconstructing\ndecoupled clothing and body lies in the occlusion caused by clothing over the\nbody. To this end, the details of the visible area and the plausibility of the\ninvisible area must be ensured during the reconstruction process. Our proposed\nmethod combines explicit and implicit representations to model the decoupled\nclothed human body, leveraging the robustness of explicit representations and\nthe flexibility of implicit representations. Specifically, we reconstruct the\nvisible region as SDF and propose a novel human manifold signed distance field\n(hmSDF) to segment the visible clothing and visible body, and then merge the\nvisible and invisible body. Extensive experimental results demonstrate that,\ncompared with existing reconstruction schemes, D$^3$-Human can achieve\nhigh-quality decoupled reconstruction of the human body wearing different\nclothing, and can be directly applied to clothing transfer and animation.\n","authors":["Honghu Chen","Bo Peng","Yunfan Tao","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.01589v1.pdf","comment":"Project Page: https://ustc3dv.github.io/D3Human/"}],"Mesh":[{"id":"http://arxiv.org/abs/2501.01877v1","updated":"2025-01-03T15:57:28Z","published":"2025-01-03T15:57:28Z","title":"ANTHROPOS-V: benchmarking the novel task of Crowd Volume Estimation","summary":"  We introduce the novel task of Crowd Volume Estimation (CVE), defined as the\nprocess of estimating the collective body volume of crowds using only RGB\nimages. Besides event management and public safety, CVE can be instrumental in\napproximating body weight, unlocking weight sensitive applications such as\ninfrastructure stress assessment, and assuring even weight balance. We propose\nthe first benchmark for CVE, comprising ANTHROPOS-V, a synthetic photorealistic\nvideo dataset featuring crowds in diverse urban environments. Its annotations\ninclude each person's volume, SMPL shape parameters, and keypoints. Also, we\nexplore metrics pertinent to CVE, define baseline models adapted from Human\nMesh Recovery and Crowd Counting domains, and propose a CVE specific\nmethodology that surpasses baselines. Although synthetic, the weights and\nheights of individuals are aligned with the real-world population distribution\nacross genders, and they transfer to the downstream task of CVE from real\nimages. Benchmark and code are available at\ngithub.com/colloroneluca/Crowd-Volume-Estimation.\n","authors":["Luca Collorone","Stefano D'Arrigo","Massimiliano Pappa","Guido Maria D'Amely di Melendugno","Giovanni Ficarra","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2501.01877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01717v1","updated":"2025-01-03T09:22:16Z","published":"2025-01-03T09:22:16Z","title":"KeyNode-Driven Geometry Coding for Real-World Scanned Human Dynamic Mesh\n  Compression","summary":"  The compression of real-world scanned 3D human dynamic meshes is an emerging\nresearch area, driven by applications such as telepresence, virtual reality,\nand 3D digital streaming. Unlike synthesized dynamic meshes with fixed\ntopology, scanned dynamic meshes often not only have varying topology across\nframes but also scan defects such as holes and outliers, increasing the\ncomplexity of prediction and compression. Additionally, human meshes often\ncombine rigid and non-rigid motions, making accurate prediction and encoding\nsignificantly more difficult compared to objects that exhibit purely rigid\nmotion. To address these challenges, we propose a compression method designed\nfor real-world scanned human dynamic meshes, leveraging embedded key nodes. The\ntemporal motion of each vertex is formulated as a distance-weighted combination\nof transformations from neighboring key nodes, requiring the transmission of\nsolely the key nodes' transformations. To enhance the quality of the\nKeyNode-driven prediction, we introduce an octree-based residual coding scheme\nand a Dual-direction prediction mode, which uses I-frames from both directions.\nExtensive experiments demonstrate that our method achieves significant\nimprovements over the state-of-the-art, with an average bitrate saving of\n24.51% across the evaluated sequences, particularly excelling at low bitrates.\n","authors":["Huong Hoang","Truong Nguyen","Pamela Cosman"],"pdf_url":"https://arxiv.org/pdf/2501.01717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01715v1","updated":"2025-01-03T09:17:30Z","published":"2025-01-03T09:17:30Z","title":"Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision","summary":"  We introduce Cloth-Splatting, a method for estimating 3D states of cloth from\nRGB images through a prediction-update framework. Cloth-Splatting leverages an\naction-conditioned dynamics model for predicting future states and uses 3D\nGaussian Splatting to update the predicted states. Our key insight is that\ncoupling a 3D mesh-based representation with Gaussian Splatting allows us to\ndefine a differentiable map between the cloth state space and the image space.\nThis enables the use of gradient-based optimization techniques to refine\ninaccurate state estimates using only RGB supervision. Our experiments\ndemonstrate that Cloth-Splatting not only improves state estimation accuracy\nover current baselines but also reduces convergence time.\n","authors":["Alberta Longhini","Marcel Büsching","Bardienus P. Duisterhof","Jens Lundell","Jeffrey Ichnowski","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2501.01715v1.pdf","comment":"Accepted at the 8th Conference on Robot Learning (CoRL 2024). Code\n  and videos available at: kth-rpl.github.io/cloth-splatting"}],"NeRF":[{"id":"http://arxiv.org/abs/2408.16690v3","updated":"2025-01-03T15:05:06Z","published":"2024-08-29T16:37:58Z","title":"Generic Objects as Pose Probes for Few-shot View Synthesis","summary":"  Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance. Our project page is\navailable at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this\nhttps URL}\n","authors":["Zhirui Gao","Renjiao Yi","Chenyang Zhu","Ke Zhuang","Wei Chen","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2408.16690v3.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2405.18816v4","updated":"2025-01-03T07:24:36Z","published":"2024-05-29T06:56:12Z","title":"Flow Priors for Linear Inverse Problems via Iterative Corrupted\n  Trajectory Matching","summary":"  Generative models based on flow matching have attracted significant attention\nfor their simplicity and superior performance in high-resolution image\nsynthesis. By leveraging the instantaneous change-of-variables formula, one can\ndirectly compute image likelihoods from a learned flow, making them enticing\ncandidates as priors for downstream tasks such as inverse problems. In\nparticular, a natural approach would be to incorporate such image probabilities\nin a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however,\nlies in the slow computation of the log-likelihood, as it requires\nbackpropagating through an ODE solver, which can be prohibitively slow for\nhigh-dimensional problems. In this work, we propose an iterative algorithm to\napproximate the MAP estimator efficiently to solve a variety of linear inverse\nproblems. Our algorithm is mathematically justified by the observation that the\nMAP objective can be approximated by a sum of $N$ ``local MAP'' objectives,\nwhere $N$ is the number of function evaluations. By leveraging Tweedie's\nformula, we show that we can perform gradient steps to sequentially optimize\nthese objectives. We validate our approach for various linear inverse problems,\nsuch as super-resolution, deblurring, inpainting, and compressed sensing, and\ndemonstrate that we can outperform other methods based on flow matching. Code\nis available at https://github.com/YasminZhang/ICTM.\n","authors":["Yasi Zhang","Peiyu Yu","Yaxuan Zhu","Yingshan Chang","Feng Gao","Ying Nian Wu","Oscar Leong"],"pdf_url":"https://arxiv.org/pdf/2405.18816v4.pdf","comment":"Accepted to NeurIPS 2024"}]},"2024-12-27T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2412.19720v1","updated":"2024-12-27T16:18:46Z","published":"2024-12-27T16:18:46Z","title":"Sharpening Neural Implicit Functions with Frequency Consolidation Priors","summary":"  Signed Distance Functions (SDFs) are vital implicit representations to\nrepresent high fidelity 3D surfaces. Current methods mainly leverage a neural\nnetwork to learn an SDF from various supervisions including signed distances,\n3D point clouds, or multi-view images. However, due to various reasons\nincluding the bias of neural network on low frequency content, 3D unaware\nsampling, sparsity in point clouds, or low resolutions of images, neural\nimplicit representations still struggle to represent geometries with high\nfrequency components like sharp structures, especially for the ones learned\nfrom images or point clouds. To overcome this challenge, we introduce a method\nto sharpen a low frequency SDF observation by recovering its high frequency\ncomponents, pursuing a sharper and more complete surface. Our key idea is to\nlearn a mapping from a low frequency observation to a full frequency coverage\nin a data-driven manner, leading to a prior knowledge of shape consolidation in\nthe frequency domain, dubbed frequency consolidation priors. To better\ngeneralize a learned prior to unseen shapes, we introduce to represent\nfrequency components as embeddings and disentangle the embedding of the low\nfrequency component from the embedding of the full frequency component. This\ndisentanglement allows the prior to generalize on an unseen low frequency\nobservation by simply recovering its full frequency embedding through a\ntest-time self-reconstruction. Our evaluations under widely used benchmarks or\nreal scenes show that our method can recover high frequency component and\nproduce more accurate surfaces than the latest methods. The code, data, and\npre-trained models are available at \\url{https://github.com/chenchao15/FCP}.\n","authors":["Chao Chen","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2412.19720v1.pdf","comment":"Accepted by AAAI 2025"}],"Mesh":[{"id":"http://arxiv.org/abs/2412.19676v1","updated":"2024-12-27T14:54:12Z","published":"2024-12-27T14:54:12Z","title":"Optimizing Local-Global Dependencies for Accurate 3D Human Pose\n  Estimation","summary":"  Transformer-based methods have recently achieved significant success in 3D\nhuman pose estimation, owing to their strong ability to model long-range\ndependencies. However, relying solely on the global attention mechanism is\ninsufficient for capturing the fine-grained local details, which are crucial\nfor accurate pose estimation. To address this, we propose SSR-STF, a\ndual-stream model that effectively integrates local features with global\ndependencies to enhance 3D human pose estimation. Specifically, we introduce\nSSRFormer, a simple yet effective module that employs the skeleton selective\nrefine attention (SSRA) mechanism to capture fine-grained local dependencies in\nhuman pose sequences, complementing the global dependencies modeled by the\nTransformer. By adaptively fusing these two feature streams, SSR-STF can better\nlearn the underlying structure of human poses, overcoming the limitations of\ntraditional methods in local feature extraction. Extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves\nstate-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm\nrespectively, outperforming existing methods in both accuracy and\ngeneralization. Furthermore, the motion representations learned by our model\nprove effective in downstream tasks such as human mesh recovery. Codes are\navailable at https://github.com/poker-xu/SSR-STF.\n","authors":["Guangsheng Xu","Guoyi Zhang","Lejia Ye","Shuwei Gan","Xiaohu Zhang","Xia Yang"],"pdf_url":"https://arxiv.org/pdf/2412.19676v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2412.19483v1","updated":"2024-12-27T06:40:44Z","published":"2024-12-27T06:40:44Z","title":"Learning Radiance Fields from a Single Snapshot Compressive Image","summary":"  In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)\ntechnique for recovering the underlying 3D scene structure from a single\ntemporal compressed image. SCI is a cost-effective method that enables the\nrecording of high-dimensional data, such as hyperspectral or temporal\ninformation, into a single image using low-cost 2D imaging sensors. To achieve\nthis, a series of specially designed 2D masks are usually employed, reducing\nstorage and transmission requirements and offering potential privacy\nprotection. Inspired by this, we take one step further to recover the encoded\n3D scene information leveraging powerful 3D scene representation capabilities\nof neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we\nformulate the physical imaging process of SCI as part of the training of NeRF,\nallowing us to exploit its impressive performance in capturing complex scene\nstructures. In addition, we further integrate the popular 3D Gaussian Splatting\n(3DGS) framework and propose SCISplat to improve 3D scene reconstruction\nquality and training/rendering speed by explicitly optimizing point clouds into\n3D Gaussian representations. To assess the effectiveness of our method, we\nconduct extensive evaluations using both synthetic data and real data captured\nby our SCI system. Experimental results demonstrate that our proposed approach\nsurpasses the state-of-the-art methods in terms of image reconstruction and\nnovel view synthesis. Moreover, our method also exhibits the ability to render\nhigh frame-rate multi-view consistent images in real time by leveraging SCI and\nthe rendering capabilities of 3DGS. Codes will be available at:\nhttps://github.com/WU- CVGL/SCISplat.\n","authors":["Yunhao Li","Xiang Liu","Xiaodong Wang","Xin Yuan","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.19483v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2412.19553v1","updated":"2024-12-27T09:51:23Z","published":"2024-12-27T09:51:23Z","title":"Structural Similarity in Deep Features: Image Quality Assessment Robust\n  to Geometrically Disparate Reference","summary":"  Image Quality Assessment (IQA) with references plays an important role in\noptimizing and evaluating computer vision tasks. Traditional methods assume\nthat all pixels of the reference and test images are fully aligned. Such\nAligned-Reference IQA (AR-IQA) approaches fail to address many real-world\nproblems with various geometric deformations between the two images. Although\nsignificant effort has been made to attack Geometrically-Disparate-Reference\nIQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for\nexample, by dedicated designs for image super-resolution and retargeting, or by\nassuming the geometric distortions to be small that can be countered by\ntranslation-robust filters or by explicit image registrations. Here we rethink\nthis problem and propose a unified, non-training-based Deep Structural\nSimilarity (DeepSSIM) approach to address the above problems in a single\nframework, which assesses structural similarity of deep features in a simple\nbut efficient way and uses an attention calibration strategy to alleviate\nattention deviation. The proposed method, without application-specific design,\nachieves state-of-the-art performance on AR-IQA datasets and meanwhile shows\nstrong robustness to various GDR-IQA test cases. Interestingly, our test also\nshows the effectiveness of DeepSSIM as an optimization tool for training image\nsuper-resolution, enhancement and restoration, implying an even wider\ngeneralizability. \\footnote{Source code will be made public after the review is\ncompleted.\n","authors":["Keke Zhang","Weiling Chen","Tiesong Zhao","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.19553v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2412.19479v1","updated":"2024-12-27T06:12:50Z","published":"2024-12-27T06:12:50Z","title":"Generative Adversarial Network on Motion-Blur Image Restoration","summary":"  In everyday life, photographs taken with a camera often suffer from motion\nblur due to hand vibrations or sudden movements. This phenomenon can\nsignificantly detract from the quality of the images captured, making it an\ninteresting challenge to develop a deep learning model that utilizes the\nprinciples of adversarial networks to restore clarity to these blurred pixels.\nIn this project, we will focus on leveraging Generative Adversarial Networks\n(GANs) to effectively deblur images affected by motion blur. A GAN-based\nTensorflow model is defined, training and evaluating by GoPro dataset which\ncomprises paired street view images featuring both clear and blurred versions.\nThis adversarial training process between Discriminator and Generator helps to\nproduce increasingly realistic images over time. Peak Signal-to-Noise Ratio\n(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation\nmetrics used to provide quantitative measures of image quality, allowing us to\nevaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and\nmean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in\nthis project. The blurry pixels are sharper in the output of GAN model shows a\ngood image restoration effect in real world applications.\n","authors":["Zhengdong Li"],"pdf_url":"https://arxiv.org/pdf/2412.19479v1.pdf","comment":null}]},"2024-12-22T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2408.10178v2","updated":"2024-12-22T07:24:09Z","published":"2024-08-19T17:36:35Z","title":"NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface\n  Reconstruction","summary":"  Signed Distance Function (SDF)-based volume rendering has demonstrated\nsignificant capabilities in surface reconstruction. Although promising,\nSDF-based methods often fail to capture detailed geometric structures,\nresulting in visible defects. By comparing SDF-based volume rendering to\ndensity-based volume rendering, we identify two main factors within the\nSDF-based approach that degrade surface quality: SDF-to-density representation\nand geometric regularization. These factors introduce challenges that hinder\nthe optimization of the SDF field. To address these issues, we introduce\nNeuRodin, a novel two-stage neural surface reconstruction framework that not\nonly achieves high-fidelity surface reconstruction but also retains the\nflexible optimization characteristics of density-based methods. NeuRodin\nincorporates innovative strategies that facilitate transformation of arbitrary\ntopologies and reduce artifacts associated with density bias. Extensive\nevaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the\nsuperiority of NeuRodin, showing strong reconstruction capabilities for both\nindoor and outdoor environments using solely posed RGB captures. Project\nwebsite: https://open3dvlab.github.io/NeuRodin/\n","authors":["Yifan Wang","Di Huang","Weicai Ye","Guofeng Zhang","Wanli Ouyang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2408.10178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01391v2","updated":"2024-12-22T02:17:41Z","published":"2024-01-02T10:51:52Z","title":"On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional\n  Encoding","summary":"  Neural implicit fields, such as the neural signed distance field (SDF) of a\nshape, have emerged as a powerful representation for many applications, e.g.,\nencoding a 3D shape and performing collision detection. Typically, implicit\nfields are encoded by Multi-layer Perceptrons (MLP) with positional encoding\n(PE) to capture high-frequency geometric details. However, a notable side\neffect of such PE-equipped MLPs is the noisy artifacts present in the learned\nimplicit fields. While increasing the sampling rate could in general mitigate\nthese artifacts, in this paper we aim to explain this adverse phenomenon\nthrough the lens of Fourier analysis. We devise a tool to determine the\nappropriate sampling rate for learning an accurate neural implicit field\nwithout undesirable side effects. Specifically, we propose a simple yet\neffective method to estimate the intrinsic frequency of a given network with\nrandomized weights based on the Fourier analysis of the network's responses. It\nis observed that a PE-equipped MLP has an intrinsic frequency much higher than\nthe highest frequency component in the PE layer. Sampling against this\nintrinsic frequency following the Nyquist-Sannon sampling theorem allows us to\ndetermine an appropriate training sampling rate. We empirically show in the\nsetting of SDF fitting that this recommended sampling rate is sufficient to\nsecure accurate fitting results, while further increasing the sampling rate\nwould not further noticeably reduce the fitting error. Training PE-equipped\nMLPs simply with our sampling strategy leads to performances superior to the\nexisting methods.\n","authors":["Guying Lin","Lei Yang","Yuan Liu","Congyi Zhang","Junhui Hou","Xiaogang Jin","Taku Komura","John Keyser","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01391v2.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2412.16939v1","updated":"2024-12-22T09:17:57Z","published":"2024-12-22T09:17:57Z","title":"Image Quality Assessment: Investigating Causal Perceptual Effects with\n  Abductive Counterfactual Inference","summary":"  Existing full-reference image quality assessment (FR-IQA) methods often fail\nto capture the complex causal mechanisms that underlie human perceptual\nresponses to image distortions, limiting their ability to generalize across\ndiverse scenarios. In this paper, we propose an FR-IQA method based on\nabductive counterfactual inference to investigate the causal relationships\nbetween deep network features and perceptual distortions. First, we explore the\ncausal effects of deep features on perception and integrate causal reasoning\nwith feature comparison, constructing a model that effectively handles complex\ndistortion types across different IQA scenarios. Second, the analysis of the\nperceptual causal correlations of our proposed method is independent of the\nbackbone architecture and thus can be applied to a variety of deep networks.\nThrough abductive counterfactual experiments, we validate the proposed causal\nrelationships, confirming the model's superior perceptual relevance and\ninterpretability of quality scores. The experimental results demonstrate the\nrobustness and effectiveness of the method, providing competitive quality\npredictions across multiple benchmarks. The source code is available at\nhttps://anonymous.4open.science/r/DeepCausalQuality-25BC.\n","authors":["Wenhao Shen","Mingliang Zhou","Yu Chen","Xuekai Wei","Jun Luo","Huayan Pu","Weijia Jia"],"pdf_url":"https://arxiv.org/pdf/2412.16939v1.pdf","comment":null}]},"2024-12-21T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2412.16737v1","updated":"2024-12-21T18:58:16Z","published":"2024-12-21T18:58:16Z","title":"LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source\n  Photometric Stereo","summary":"  The biggest improvements in Photometric Stereo (PS) field has recently come\nfrom adoption of differentiable volumetric rendering techniques such as NeRF or\nNeural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV\nbenchmark. However, while there are sizeable datasets for environment lit\nobjects such as Digital Twin Catalogue (DTS), there are only several small\nPhotometric Stereo datasets which often lack challenging objects (simple,\nsmooth, untextured) and practical, small form factor (near-field) light setup.\n  To address this, we propose LUCES-MV, the first real-world, multi-view\ndataset designed for near-field point light source photometric stereo. Our\ndataset includes 15 objects with diverse materials, each imaged under varying\nlight conditions from an array of 15 LEDs positioned 30 to 40 centimeters from\nthe camera center. To facilitate transparent end-to-end evaluation, our dataset\nprovides not only ground truth normals and ground truth object meshes and poses\nbut also light and camera calibration images.\n  We evaluate state-of-the-art near-field photometric stereo algorithms,\nhighlighting their strengths and limitations across different material and\nshape complexities. LUCES-MV dataset offers an important benchmark for\ndeveloping more robust, accurate and scalable real-world Photometric Stereo\nbased 3D reconstruction methods.\n","authors":["Fotios Logothetis","Ignas Budvytis","Stephan Liwicki","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2412.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16467v1","updated":"2024-12-21T03:49:29Z","published":"2024-12-21T03:49:29Z","title":"Sensing Surface Patches in Volume Rendering for Inferring Signed\n  Distance Functions","summary":"  It is vital to recover 3D geometry from multi-view RGB images in many 3D\ncomputer vision tasks. The latest methods infer the geometry represented as a\nsigned distance field by minimizing the rendering error on the field through\nvolume rendering. However, it is still challenging to explicitly impose\nconstraints on surfaces for inferring more geometry details due to the limited\nability of sensing surfaces in volume rendering. To resolve this problem, we\nintroduce a method to infer signed distance functions (SDFs) with a better\nsense of surfaces through volume rendering. Using the gradients and signed\ndistances, we establish a small surface patch centered at the estimated\nintersection along a ray by pulling points randomly sampled nearby. Hence, we\nare able to explicitly impose surface constraints on the sensed surface patch,\nsuch as multi-view photo consistency and supervision from depth or normal\npriors, through volume rendering. We evaluate our method by numerical and\nvisual comparisons on scene benchmarks. Our superiority over the latest methods\njustifies our effectiveness.\n","authors":["Sijia Jiang","Tong Wu","Jing Hua","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2412.16467v1.pdf","comment":"To be appeared at AAAI25"}],"Mesh":[{"id":"http://arxiv.org/abs/2412.16776v1","updated":"2024-12-21T21:16:03Z","published":"2024-12-21T21:16:03Z","title":"DMesh++: An Efficient Differentiable Mesh for Complex Shapes","summary":"  Recent probabilistic methods for 3D triangular meshes capture diverse shapes\nby differentiable mesh connectivity, but face high computational costs with\nincreased shape details. We introduce a new differentiable mesh processing\nmethod in 2D and 3D that addresses this challenge and efficiently handles\nmeshes with intricate structures. Additionally, we present an algorithm that\nadapts the mesh resolution to local geometry in 2D for efficient\nrepresentation. We demonstrate the effectiveness of our approach on 2D point\ncloud and 3D multi-view reconstruction tasks. Visit our project page\n(https://sonsang.github.io/dmesh2-project) for source code and supplementary\nmaterial.\n","authors":["Sanghyun Son","Matheus Gadelha","Yang Zhou","Matthew Fisher","Zexiang Xu","Yi-Ling Qiao","Ming C. Lin","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.16776v1.pdf","comment":"26 pages, 27 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.16737v1","updated":"2024-12-21T18:58:16Z","published":"2024-12-21T18:58:16Z","title":"LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source\n  Photometric Stereo","summary":"  The biggest improvements in Photometric Stereo (PS) field has recently come\nfrom adoption of differentiable volumetric rendering techniques such as NeRF or\nNeural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV\nbenchmark. However, while there are sizeable datasets for environment lit\nobjects such as Digital Twin Catalogue (DTS), there are only several small\nPhotometric Stereo datasets which often lack challenging objects (simple,\nsmooth, untextured) and practical, small form factor (near-field) light setup.\n  To address this, we propose LUCES-MV, the first real-world, multi-view\ndataset designed for near-field point light source photometric stereo. Our\ndataset includes 15 objects with diverse materials, each imaged under varying\nlight conditions from an array of 15 LEDs positioned 30 to 40 centimeters from\nthe camera center. To facilitate transparent end-to-end evaluation, our dataset\nprovides not only ground truth normals and ground truth object meshes and poses\nbut also light and camera calibration images.\n  We evaluate state-of-the-art near-field photometric stereo algorithms,\nhighlighting their strengths and limitations across different material and\nshape complexities. LUCES-MV dataset offers an important benchmark for\ndeveloping more robust, accurate and scalable real-world Photometric Stereo\nbased 3D reconstruction methods.\n","authors":["Fotios Logothetis","Ignas Budvytis","Stephan Liwicki","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2412.16737v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2412.16737v1","updated":"2024-12-21T18:58:16Z","published":"2024-12-21T18:58:16Z","title":"LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source\n  Photometric Stereo","summary":"  The biggest improvements in Photometric Stereo (PS) field has recently come\nfrom adoption of differentiable volumetric rendering techniques such as NeRF or\nNeural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV\nbenchmark. However, while there are sizeable datasets for environment lit\nobjects such as Digital Twin Catalogue (DTS), there are only several small\nPhotometric Stereo datasets which often lack challenging objects (simple,\nsmooth, untextured) and practical, small form factor (near-field) light setup.\n  To address this, we propose LUCES-MV, the first real-world, multi-view\ndataset designed for near-field point light source photometric stereo. Our\ndataset includes 15 objects with diverse materials, each imaged under varying\nlight conditions from an array of 15 LEDs positioned 30 to 40 centimeters from\nthe camera center. To facilitate transparent end-to-end evaluation, our dataset\nprovides not only ground truth normals and ground truth object meshes and poses\nbut also light and camera calibration images.\n  We evaluate state-of-the-art near-field photometric stereo algorithms,\nhighlighting their strengths and limitations across different material and\nshape complexities. LUCES-MV dataset offers an important benchmark for\ndeveloping more robust, accurate and scalable real-world Photometric Stereo\nbased 3D reconstruction methods.\n","authors":["Fotios Logothetis","Ignas Budvytis","Stephan Liwicki","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2412.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v3","updated":"2024-12-21T10:41:08Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in 3D Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v3.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"}]},"2024-12-20T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2412.16361v1","updated":"2024-12-20T21:49:02Z","published":"2024-12-20T21:49:02Z","title":"Toward Robust Neural Reconstruction from Sparse Point Sets","summary":"  We consider the challenging problem of learning Signed Distance Functions\n(SDF) from sparse and noisy 3D point clouds. In contrast to recent methods that\ndepend on smoothness priors, our method, rooted in a distributionally robust\noptimization (DRO) framework, incorporates a regularization term that leverages\nsamples from the uncertainty regions of the model to improve the learned SDFs.\nThanks to tractable dual formulations, we show that this framework enables a\nstable and efficient optimization of SDFs in the absence of ground truth\nsupervision. Using a variety of synthetic and real data evaluations from\ndifferent modalities, we show that our DRO based learning framework can improve\nSDF learning with respect to baselines and the state-of-the-art methods.\n","authors":["Amine Ouasfi","Shubhendu Jena","Eric Marchand","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2412.16361v1.pdf","comment":"Project page : https://ouasfi.github.io/sdro/"},{"id":"http://arxiv.org/abs/2411.15723v3","updated":"2024-12-20T09:36:36Z","published":"2024-11-24T05:55:19Z","title":"GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian\n  Supervision","summary":"  Surface reconstruction from multi-view images is a core challenge in 3D\nvision. Recent studies have explored signed distance fields (SDF) within Neural\nRadiance Fields (NeRF) to achieve high-fidelity surface reconstructions.\nHowever, these approaches often suffer from slow training and rendering speeds\ncompared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques\nattempt to fuse depth information to extract geometry from 3DGS, but frequently\nresult in incomplete reconstructions and fragmented surfaces. In this paper, we\nintroduce GSurf, a novel end-to-end method for learning a signed distance field\ndirectly from Gaussian primitives. The continuous and smooth nature of SDF\naddresses common issues in the 3DGS family, such as holes resulting from noisy\nor missing depth data. By using Gaussian splatting for rendering, GSurf avoids\nthe redundant volume rendering typically required in other GS and SDF\nintegrations. Consequently, GSurf achieves faster training and rendering speeds\nwhile delivering 3D reconstruction quality comparable to neural implicit\nsurface methods, such as VolSDF and NeuS. Experimental results across various\nbenchmark datasets demonstrate the effectiveness of our method in producing\nhigh-fidelity 3D reconstructions.\n","authors":["Baixin Xu","Jiangbei Hu","Jiaze Li","Ying He"],"pdf_url":"https://arxiv.org/pdf/2411.15723v3.pdf","comment":"see https://github.com/xubaixinxbx/Gsurf"}],"Mesh":[{"id":"http://arxiv.org/abs/2412.14939v2","updated":"2024-12-20T10:02:01Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/GURecon/"}],"NeRF":[{"id":"http://arxiv.org/abs/2411.15723v3","updated":"2024-12-20T09:36:36Z","published":"2024-11-24T05:55:19Z","title":"GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian\n  Supervision","summary":"  Surface reconstruction from multi-view images is a core challenge in 3D\nvision. Recent studies have explored signed distance fields (SDF) within Neural\nRadiance Fields (NeRF) to achieve high-fidelity surface reconstructions.\nHowever, these approaches often suffer from slow training and rendering speeds\ncompared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques\nattempt to fuse depth information to extract geometry from 3DGS, but frequently\nresult in incomplete reconstructions and fragmented surfaces. In this paper, we\nintroduce GSurf, a novel end-to-end method for learning a signed distance field\ndirectly from Gaussian primitives. The continuous and smooth nature of SDF\naddresses common issues in the 3DGS family, such as holes resulting from noisy\nor missing depth data. By using Gaussian splatting for rendering, GSurf avoids\nthe redundant volume rendering typically required in other GS and SDF\nintegrations. Consequently, GSurf achieves faster training and rendering speeds\nwhile delivering 3D reconstruction quality comparable to neural implicit\nsurface methods, such as VolSDF and NeuS. Experimental results across various\nbenchmark datasets demonstrate the effectiveness of our method in producing\nhigh-fidelity 3D reconstructions.\n","authors":["Baixin Xu","Jiangbei Hu","Jiaze Li","Ying He"],"pdf_url":"https://arxiv.org/pdf/2411.15723v3.pdf","comment":"see https://github.com/xubaixinxbx/Gsurf"}],"IQA":[{"id":"http://arxiv.org/abs/2405.19224v4","updated":"2024-12-20T16:04:16Z","published":"2024-05-29T16:04:03Z","title":"A study on the adequacy of common IQA measures for medical images","summary":"  Image quality assessment (IQA) is standard practice in the development stage\nof novel machine learning algorithms that operate on images. The most commonly\nused IQA measures have been developed and tested for natural images, but not in\nthe medical setting. Reported inconsistencies arising in medical images are not\nsurprising, as they have different properties than natural images. In this\nstudy, we test the applicability of common IQA measures for medical image data\nby comparing their assessment to manually rated chest X-ray (5 experts) and\nphotoacoustic image data (2 experts). Moreover, we include supplementary\nstudies on grayscale natural images and accelerated brain MRI data. The results\nof all experiments show a similar outcome in line with previous findings for\nmedical images: PSNR and SSIM in the default setting are in the lower range of\nthe result list and HaarPSI outperforms the other tested measures in the\noverall performance. Also among the top performers in our experiments are the\nfull reference measures FSIM, LPIPS and MS-SSIM. Generally, the results on\nnatural images yield considerably higher correlations, suggesting that\nadditional employment of tailored IQA measures for medical imaging algorithms\nis needed.\n","authors":["Anna Breger","Clemens Karner","Ian Selby","Janek Gröhl","Sören Dittmer","Edward Lilley","Judith Babar","Jake Beckford","Thomas R Else","Timothy J Sadler","Shahab Shahipasand","Arthikkaa Thavakumar","Michael Roberts","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2405.19224v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15847v1","updated":"2024-12-20T12:39:49Z","published":"2024-12-20T12:39:49Z","title":"Image Quality Assessment: Enhancing Perceptual Exploration and\n  Interpretation with Collaborative Feature Refinement and Hausdorff distance","summary":"  Current full-reference image quality assessment (FR-IQA) methods often fuse\nfeatures from reference and distorted images, overlooking that color and\nluminance distortions occur mainly at low frequencies, whereas edge and texture\ndistortions occur at high frequencies. This work introduces a pioneering\ntraining-free FR-IQA method that accurately predicts image quality in alignment\nwith the human visual system (HVS) by leveraging a novel perceptual degradation\nmodelling approach to address this limitation. First, a collaborative feature\nrefinement module employs a carefully designed wavelet transform to extract\nperceptually relevant features, capturing multiscale perceptual information and\nmimicking how the HVS analyses visual information at various scales and\norientations in the spatial and frequency domains. Second, a Hausdorff\ndistance-based distribution similarity measurement module robustly assesses the\ndiscrepancy between the feature distributions of the reference and distorted\nimages, effectively handling outliers and variations while mimicking the\nability of HVS to perceive and tolerate certain levels of distortion. The\nproposed method accurately captures perceptual quality differences without\nrequiring training data or subjective quality scores. Extensive experiments on\nmultiple benchmark datasets demonstrate superior performance compared with\nexisting state-of-the-art approaches, highlighting its ability to correlate\nstrongly with the HVS.\\footnote{The code is available at\n\\url{https://anonymous.4open.science/r/CVPR2025-F339}.}\n","authors":["Xuekai Wei","Junyu Zhang","Qinlin Hu","Mingliang Zhou\\\\Yong Feng","Weizhi Xian","Huayan Pu","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2412.15847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15677v1","updated":"2024-12-20T08:47:07Z","published":"2024-12-20T08:47:07Z","title":"AI-generated Image Quality Assessment in Visual Communication","summary":"  Assessing the quality of artificial intelligence-generated images (AIGIs)\nplays a crucial role in their application in real-world scenarios. However,\ntraditional image quality assessment (IQA) algorithms primarily focus on\nlow-level visual perception, while existing IQA works on AIGIs overemphasize\nthe generated content itself, neglecting its effectiveness in real-world\napplications. To bridge this gap, we propose AIGI-VC, a quality assessment\ndatabase for AI-Generated Images in Visual Communication, which studies the\ncommunicability of AIGIs in the advertising field from the perspectives of\ninformation clarity and emotional interaction. The dataset consists of 2,500\nimages spanning 14 advertisement topics and 8 emotion types. It provides\ncoarse-grained human preference annotations and fine-grained preference\ndescriptions, benchmarking the abilities of IQA methods in preference\nprediction, interpretation, and reasoning. We conduct an empirical study of\nexisting representative IQA methods and large multi-modal models on the AIGI-VC\ndataset, uncovering their strengths and weaknesses.\n","authors":["Yu Tian","Yixuan Li","Baoliang Chen","Hanwei Zhu","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2412.15677v1.pdf","comment":"AAAI-2025; Project page: https://github.com/ytian73/AIGI-VC"}]},"2025-01-16T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.09600v1","updated":"2025-01-16T15:22:06Z","published":"2025-01-16T15:22:06Z","title":"Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid\n  Prototyping in Virtual Reality Applications","summary":"  SLAM is a foundational technique with broad applications in robotics and\nAR/VR. SLAM simulations evaluate new concepts, but testing on\nresource-constrained devices, such as VR HMDs, faces challenges: high\ncomputational cost and restricted sensor data access. This work proposes a\nsparse framework using mesh geometry projections as features, which improves\nefficiency and circumvents direct sensor data access, advancing SLAM research\nas we demonstrate in VR and through numerical evaluation.\n","authors":["Carlos Augusto Pinheiro de Sousa","Heiko Hamann","Oliver Deussen"],"pdf_url":"https://arxiv.org/pdf/2501.09600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20986v2","updated":"2025-01-16T08:58:44Z","published":"2024-10-28T13:04:44Z","title":"Skinned Motion Retargeting with Dense Geometric Interaction Perception","summary":"  Capturing and maintaining geometric interactions among different body parts\nis crucial for successful motion retargeting in skinned characters. Existing\napproaches often overlook body geometries or add a geometry correction stage\nafter skeletal motion retargeting. This results in conflicts between skeleton\ninteraction and geometry correction, leading to issues such as jittery,\ninterpenetration, and contact mismatches. To address these challenges, we\nintroduce a new retargeting framework, MeshRet, which directly models the dense\ngeometric interactions in motion retargeting. Initially, we establish dense\nmesh correspondences between characters using semantically consistent sensors\n(SCS), effective across diverse mesh topologies. Subsequently, we develop a\nnovel spatio-temporal representation called the dense mesh interaction (DMI)\nfield. This field, a collection of interacting SCS feature vectors, skillfully\ncaptures both contact and non-contact interactions between body geometries. By\naligning the DMI field during retargeting, MeshRet not only preserves motion\nsemantics but also prevents self-interpenetration and ensures contact\npreservation. Extensive experiments on the public Mixamo dataset and our\nnewly-collected ScanRet dataset demonstrate that MeshRet achieves\nstate-of-the-art performance. Code available at\nhttps://github.com/abcyzj/MeshRet.\n","authors":["Zijie Ye","Jia-Wei Liu","Jia Jia","Shikun Sun","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2410.20986v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2303.13397v6","updated":"2025-01-16T02:48:38Z","published":"2023-03-23T16:15:18Z","title":"DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery\n  from Videos","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.\n","authors":["Ce Zheng","Xianpeng Liu","Qucheng Peng","Tianfu Wu","Pu Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v6.pdf","comment":"WACV 2025"}],"NeRF":[{"id":"http://arxiv.org/abs/2501.09460v1","updated":"2025-01-16T10:42:29Z","published":"2025-01-16T10:42:29Z","title":"Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective\n  Scenes","summary":"  Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets.\n","authors":["Ji Shi","Xianghua Ying","Ruohao Guo","Bowei Xing","Wenzhen Yue"],"pdf_url":"https://arxiv.org/pdf/2501.09460v1.pdf","comment":"AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF"},{"id":"http://arxiv.org/abs/2501.03659v3","updated":"2025-01-16T08:20:15Z","published":"2025-01-07T09:47:46Z","title":"DehazeGS: Seeing Through Fog with 3D Gaussian Splatting","summary":"  Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/\n","authors":["Jinze Yu","Yiqun Wang","Zhengda Lu","Jianwei Guo","Yong Li","Hongxing Qin","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.03659v3.pdf","comment":"9 pages,4 figures"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.09396v1","updated":"2025-01-16T09:07:01Z","published":"2025-01-16T09:07:01Z","title":"Joint Transmission and Deblurring: A Semantic Communication Approach\n  Using Events","summary":"  Deep learning-based joint source-channel coding (JSCC) is emerging as a\npromising technology for effective image transmission. However, most existing\napproaches focus on transmitting clear images, overlooking real-world\nchallenges such as motion blur caused by camera shaking or fast-moving objects.\nMotion blur often degrades image quality, making transmission and\nreconstruction more challenging. Event cameras, which asynchronously record\npixel intensity changes with extremely low latency, have shown great potential\nfor motion deblurring tasks. However, the efficient transmission of the\nabundant data generated by event cameras remains a significant challenge. In\nthis work, we propose a novel JSCC framework for the joint transmission of\nblurry images and events, aimed at achieving high-quality reconstructions under\nlimited channel bandwidth. This approach is designed as a deblurring\ntask-oriented JSCC system. Since RGB cameras and event cameras capture the same\nscene through different modalities, their outputs contain both shared and\ndomain-specific information. To avoid repeatedly transmitting the shared\ninformation, we extract and transmit their shared information and\ndomain-specific information, respectively. At the receiver, the received\nsignals are processed by a deblurring decoder to generate clear images.\nAdditionally, we introduce a multi-stage training strategy to train the\nproposed model. Simulation results demonstrate that our method significantly\noutperforms existing JSCC-based image transmission schemes, addressing motion\nblur effectively.\n","authors":["Pujing Yang","Guangyi Zhang","Yunlong Cai","Lei Yu","Guanding Yu"],"pdf_url":"https://arxiv.org/pdf/2501.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09321v1","updated":"2025-01-16T06:25:56Z","published":"2025-01-16T06:25:56Z","title":"Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention\n  for Image Restoration Models Compression","summary":"  Transformer-based encoder-decoder models have achieved remarkable success in\nimage-to-image transfer tasks, particularly in image restoration. However,\ntheir high computational complexity-manifested in elevated FLOPs and parameter\ncounts-limits their application in real-world scenarios. Existing knowledge\ndistillation methods in image restoration typically employ lightweight student\nmodels that directly mimic the intermediate features and reconstruction results\nof the teacher, overlooking the implicit attention relationships between them.\nTo address this, we propose a Soft Knowledge Distillation (SKD) strategy that\nincorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for\ncompressing image restoration models. This mechanism facilitates interaction\nbetween the student and teacher across both channel and spatial dimensions,\nenabling the student to implicitly learn the attention matrices. Additionally,\nwe employ a Gaussian kernel function to measure the distance between student\nand teacher features in kernel space, ensuring stable and efficient feature\nlearning. To further enhance the quality of reconstructed images, we replace\nthe commonly used L1 or KL divergence loss with a contrastive learning loss at\nthe image level. Experiments on three tasks-image deraining, deblurring, and\ndenoising-demonstrate that our SKD strategy significantly reduces computational\ncomplexity while maintaining strong image restoration capabilities.\n","authors":["Yongheng Zhang","Danfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2501.09321v1.pdf","comment":"Accepted by ICASSP2025"}]},"2025-01-14T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.08333v1","updated":"2025-01-14T18:59:59Z","published":"2025-01-14T18:59:59Z","title":"DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video\n  Diffusion Models","summary":"  Understanding the ability of humans to use objects is crucial for AI to\nimprove daily life. Existing studies for learning such ability focus on\nhuman-object patterns (e.g., contact, spatial relation, orientation) in static\nsituations, and learning Human-Object Interaction (HOI) patterns over time\n(i.e., movement of human and object) is relatively less explored. In this\npaper, we introduce a novel type of affordance named Dynamic Affordance. For a\ngiven input 3D object mesh, we learn dynamic affordance which models the\ndistribution of both (1) human motion and (2) human-guided object pose during\ninteractions. As a core idea, we present a method to learn the 3D dynamic\naffordance from synthetically generated 2D videos, leveraging a pre-trained\nvideo diffusion model. Specifically, we propose a pipeline that first generates\n2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI\nsamples. Once we generate diverse 4D HOI samples on various target objects, we\ntrain our DAViD, where we present a method based on the Low-Rank Adaptation\n(LoRA) module for pre-trained human motion diffusion model (MDM) and an object\npose diffusion model with human pose guidance. Our motion diffusion model is\nextended for multi-object interactions, demonstrating the advantage of our\npipeline with LoRA for combining the concepts of object usage. Through\nextensive experiments, we demonstrate our DAViD outperforms the baselines in\ngenerating human motion with HOIs.\n","authors":["Hyeonwoo Kim","Sangwon Beak","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2501.08333v1.pdf","comment":"Project Page: https://snuvclab.github.io/david/"},{"id":"http://arxiv.org/abs/2501.08370v1","updated":"2025-01-14T18:40:33Z","published":"2025-01-14T18:40:33Z","title":"3D Gaussian Splatting with Normal Information for Mesh Extraction and\n  Improved Rendering","summary":"  Differentiable 3D Gaussian splatting has emerged as an efficient and flexible\nrendering technique for representing complex scenes from a collection of 2D\nviews and enabling high-quality real-time novel-view synthesis. However, its\nreliance on photometric losses can lead to imprecisely reconstructed geometry\nand extracted meshes, especially in regions with high curvature or fine detail.\nWe propose a novel regularization method using the gradients of a signed\ndistance function estimated from the Gaussians, to improve the quality of\nrendering while also extracting a surface mesh. The regularizing normal\nsupervision facilitates better rendering and mesh reconstruction, which is\ncrucial for downstream applications in video generation, animation, AR-VR and\ngaming. We demonstrate the effectiveness of our approach on datasets such as\nMip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on\nphotorealism metrics compared to other mesh extracting rendering methods\nwithout compromising mesh quality.\n","authors":["Meenakshi Krishnan","Liam Fowl","Ramani Duraiswami"],"pdf_url":"https://arxiv.org/pdf/2501.08370v1.pdf","comment":"ICASSP 2025: Workshop on Generative Data Augmentation for Real-World\n  Signal Processing Applications"},{"id":"http://arxiv.org/abs/2407.04545v2","updated":"2025-01-14T18:20:45Z","published":"2024-07-05T14:30:24Z","title":"Gaussian Eigen Models for Human Heads","summary":"  Current personalized neural head avatars face a trade-off: lightweight models\nlack detail and realism, while high-quality, animatable avatars require\nsignificant computational resources, making them unsuitable for commodity\ndevices. To address this gap, we introduce Gaussian Eigen Models (GEM), which\nprovide high-quality, lightweight, and easily controllable head avatars. GEM\nutilizes 3D Gaussian primitives for representing the appearance combined with\nGaussian splatting for rendering. Building on the success of mesh-based 3D\nmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbases\nfor representing the head appearance of a specific subject. In particular, we\nconstruct linear bases to represent the position, scale, rotation, and opacity\nof the 3D Gaussians. This allows us to efficiently generate Gaussian primitives\nof a specific head shape by a linear combination of the basis vectors, only\nrequiring a low-dimensional parameter vector that contains the respective\ncoefficients. We propose to construct these linear bases (GEM) by distilling\nhigh-quality compute-intense CNN-based Gaussian avatar models that can generate\nexpression-dependent appearance changes like wrinkles. These high-quality\nmodels are trained on multi-view videos of a subject and are distilled using a\nseries of principal component analyses. Once we have obtained the bases that\nrepresent the animatable appearance space of a specific human, we learn a\nregressor that takes a single RGB image as input and predicts the\nlow-dimensional parameter vector that corresponds to the shown facial\nexpression. In a series of experiments, we compare GEM's self-reenactment and\ncross-person reenactment results to state-of-the-art 3D avatar methods,\ndemonstrating GEM's higher visual quality and better generalization to new\nexpressions.\n","authors":["Wojciech Zielonka","Timo Bolkart","Thabo Beeler","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2407.04545v2.pdf","comment":"https://zielon.github.io/gem/"},{"id":"http://arxiv.org/abs/2501.08174v1","updated":"2025-01-14T14:56:31Z","published":"2025-01-14T14:56:31Z","title":"Object-Centric 2D Gaussian Splatting: Background Removal and\n  Occlusion-Aware Pruning for Compact Object Models","summary":"  Current Gaussian Splatting approaches are effective for reconstructing entire\nscenes but lack the option to target specific objects, making them\ncomputationally expensive and unsuitable for object-specific applications. We\npropose a novel approach that leverages object masks to enable targeted\nreconstruction, resulting in object-centric models. Additionally, we introduce\nan occlusion-aware pruning strategy to minimize the number of Gaussians without\ncompromising quality. Our method reconstructs compact object models, yielding\nobject-centric Gaussian and mesh representations that are up to 96\\% smaller\nand up to 71\\% faster to train compared to the baseline while retaining\ncompetitive quality. These representations are immediately usable for\ndownstream applications such as appearance editing and physics simulation\nwithout additional processing.\n","authors":["Marcel Rogge","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2501.08174v1.pdf","comment":"Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)"},{"id":"http://arxiv.org/abs/2501.07994v1","updated":"2025-01-14T10:38:18Z","published":"2025-01-14T10:38:18Z","title":"Combining imaging and shape features for prediction tasks of Alzheimer's\n  disease classification and brain age regression","summary":"  We investigate combining imaging and shape features extracted from MRI for\nthe clinically relevant tasks of brain age prediction and Alzheimer's disease\nclassification. Our proposed model fuses ResNet-extracted image embeddings with\nshape embeddings from a bespoke graph neural network. The shape embeddings are\nderived from surface meshes of 15 brain structures, capturing detailed\ngeometric information. Combined with the appearance features from T1-weighted\nimages, we observe improvements in the prediction performance on both tasks,\nwith substantial gains for classification. We evaluate the model using public\ndatasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of\nfusing imaging and shape features for brain analysis.\n","authors":["Nairouz Shehata","Carolina Piçarra","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2501.07994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07800v1","updated":"2025-01-14T02:56:19Z","published":"2025-01-14T02:56:19Z","title":"BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular\n  Videos","summary":"  Recent advancements in 3D human pose estimation from single-camera images and\nvideos have relied on parametric models, like SMPL. However, these models\noversimplify anatomical structures, limiting their accuracy in capturing true\njoint locations and movements, which reduces their applicability in\nbiomechanics, healthcare, and robotics. Biomechanically accurate pose\nestimation, on the other hand, typically requires costly marker-based motion\ncapture systems and optimization techniques in specialized labs. To bridge this\ngap, we propose BioPose, a novel learning-based framework for predicting\nbiomechanically accurate 3D human pose directly from monocular videos. BioPose\nincludes three key components: a Multi-Query Human Mesh Recovery model\n(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose\nrefinement technique. MQ-HMR leverages a multi-query deformable transformer to\nextract multi-scale fine-grained image features, enabling precise human mesh\nrecovery. NeurIK treats the mesh vertices as virtual markers, applying a\nspatial-temporal network to regress biomechanically accurate 3D poses under\nanatomical constraints. To further improve 3D pose estimations, a 2D-informed\nrefinement step optimizes the query tokens during inference by aligning the 3D\nstructure with 2D pose observations. Experiments on benchmark datasets\ndemonstrate that BioPose significantly outperforms state-of-the-art methods.\nProject website:\n\\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.\n","authors":["Farnoosh Koleini","Muhammad Usama Saleem","Pu Wang","Hongfei Xue","Ahmed Helmy","Abbey Fenwick"],"pdf_url":"https://arxiv.org/pdf/2501.07800v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2306.09349v4","updated":"2025-01-14T22:05:06Z","published":"2023-06-15T17:59:59Z","title":"UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video","summary":"  We present UrbanIR (Urban Scene Inverse Rendering), a new inverse graphics\nmodel that enables realistic, free-viewpoint renderings of scenes under various\nlighting conditions with a single video. It accurately infers shape, albedo,\nvisibility, and sun and sky illumination from wide-baseline videos, such as\nthose from car-mounted cameras, differing from NeRF's dense view settings. In\nthis context, standard methods often yield subpar geometry and material\nestimates, such as inaccurate roof representations and numerous 'floaters'.\nUrbanIR addresses these issues with novel losses that reduce errors in inverse\ngraphics inference and rendering artifacts. Its techniques allow for precise\nshadow volume estimation in the original scene. The model's outputs support\ncontrollable editing, enabling photorealistic free-viewpoint renderings of\nnight simulations, relit scenes, and inserted objects, marking a significant\nimprovement over existing state-of-the-art methods.\n","authors":["Chih-Hao Lin","Bohan Liu","Yi-Ting Chen","Kuan-Sheng Chen","David Forsyth","Jia-Bin Huang","Anand Bhattad","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2306.09349v4.pdf","comment":"https://urbaninverserendering.github.io/"},{"id":"http://arxiv.org/abs/2501.07015v2","updated":"2025-01-14T21:02:31Z","published":"2025-01-13T02:28:13Z","title":"SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting","summary":"  Achieving high-fidelity 3D reconstruction from monocular video remains\nchallenging due to the inherent limitations of traditional methods like\nStructure-from-Motion (SfM) and monocular SLAM in accurately capturing scene\ndetails. While differentiable rendering techniques such as Neural Radiance\nFields (NeRF) address some of these challenges, their high computational costs\nmake them unsuitable for real-time applications. Additionally, existing 3D\nGaussian Splatting (3DGS) methods often focus on photometric consistency,\nneglecting geometric accuracy and failing to exploit SLAM's dynamic depth and\npose updates for scene refinement. We propose a framework integrating dense\nSLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach\nintroduces SLAM-Informed Adaptive Densification, which dynamically updates and\ndensifies the Gaussian model by leveraging dense point clouds from SLAM.\nAdditionally, we incorporate Geometry-Guided Optimization, which combines\nedge-aware geometric constraints and photometric consistency to jointly\noptimize the appearance and geometry of the 3DGS scene representation, enabling\ndetailed and accurate SLAM mapping reconstruction. Experiments on the Replica\nand TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving\nstate-of-the-art results among monocular systems. Specifically, our method\nachieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,\nrepresenting improvements of 10.7%, 6.4%, and 49.4%, respectively, over the\nprevious SOTA. On TUM-RGBD, our method outperforms the closest baseline by\n10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the\npotential of our framework in bridging the gap between photometric and\ngeometric dense 3D scene representations, paving the way for practical and\nefficient monocular dense reconstruction.\n","authors":["Yue Hu","Rong Liu","Meida Chen","Peter Beerel","Andrew Feng"],"pdf_url":"https://arxiv.org/pdf/2501.07015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08286v1","updated":"2025-01-14T18:01:15Z","published":"2025-01-14T18:01:15Z","title":"VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes","summary":"  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.\n","authors":["Ke Wu","Zicheng Zhang","Muer Tie","Ziqing Ai","Zhongxue Gan","Wenchao Ding"],"pdf_url":"https://arxiv.org/pdf/2501.08286v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2501.08415v1","updated":"2025-01-14T20:12:09Z","published":"2025-01-14T20:12:09Z","title":"Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics","summary":"  Recent studies have revealed that modern image and video quality assessment\n(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can\nmanipulate a video through preprocessing to artificially increase its quality\nscore according to a certain metric, despite no actual improvement in visual\nquality. Most of the attacks studied in the literature are white-box attacks,\nwhile black-box attacks in the context of VQA have received less attention.\nMoreover, some research indicates a lack of transferability of adversarial\nexamples generated for one model to another when applied to VQA. In this paper,\nwe propose a cross-modal attack method, IC2VQA, aimed at exploring the\nvulnerabilities of modern VQA models. This approach is motivated by the\nobservation that the low-level feature spaces of images and videos are similar.\nWe investigate the transferability of adversarial perturbations across\ndifferent modalities; specifically, we analyze how adversarial perturbations\ngenerated on a white-box IQA model with an additional CLIP module can\neffectively target a VQA model. The addition of the CLIP module serves as a\nvaluable aid in increasing transferability, as the CLIP model is known for its\neffective capture of low-level semantics. Extensive experiments demonstrate\nthat IC2VQA achieves a high success rate in attacking three black-box VQA\nmodels. We compare our method with existing black-box attack strategies,\nhighlighting its superiority in terms of attack success within the same number\nof iterations and levels of attack strength. We believe that the proposed\nmethod will contribute to the deeper analysis of robust VQA metrics.\n","authors":["Georgii Gotin","Ekaterina Shumitskaya","Anastasia Antsiferova","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2501.08415v1.pdf","comment":"Accepted for VISAPP 2025"}]},"2025-01-13T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.05379v2","updated":"2025-01-13T17:22:30Z","published":"2025-01-09T17:04:33Z","title":"Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance","summary":"  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https://arc2avatar.github.io for more\nresources.\n","authors":["Dimitrios Gerogiannis","Foivos Paraperas Papantoniou","Rolandos Alexandros Potamias","Alexandros Lattas","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2501.05379v2.pdf","comment":"Project Page https://arc2avatar.github.io"},{"id":"http://arxiv.org/abs/2501.07478v1","updated":"2025-01-13T16:52:28Z","published":"2025-01-13T16:52:28Z","title":"3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point\n  Cloud or Mesh","summary":"  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D\nreconstructions, but these scenes often require specialised renderers for\neffective visualisation. In contrast, point clouds are a widely used 3D\nrepresentation and are compatible with most popular 3D processing software, yet\nconverting 3DGS scenes into point clouds is a complex challenge. In this work\nwe introduce 3DGS-to-PC, a flexible and highly customisable framework that is\ncapable of transforming 3DGS scenes into dense, high-accuracy point clouds. We\nsample points probabilistically from each Gaussian as a 3D density function. We\nadditionally threshold new points using the Mahalanobis distance to the\nGaussian centre, preventing extreme outliers. The result is a point cloud that\nclosely represents the shape encoded into the 3D Gaussian scene. Individual\nGaussians use spherical harmonics to adapt colours depending on view, and each\npoint may contribute only subtle colour hints to the resulting rendered scene.\nTo avoid spurious or incorrect colours that do not fit with the final point\ncloud, we recalculate Gaussian colours via a customised image rendering\napproach, assigning each Gaussian the colour of the pixel to which it\ncontributes most across all views. 3DGS-to-PC also supports mesh generation\nthrough Poisson Surface Reconstruction, applied to points sampled from\npredicted surface Gaussians. This allows coloured meshes to be generated from\n3DGS scenes without the need for re-training. This package is highly\ncustomisable and capability of simple integration into existing 3DGS pipelines.\n3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud\nand surface-based formats.\n","authors":["Lewis A G Stuart","Michael P Pound"],"pdf_url":"https://arxiv.org/pdf/2501.07478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07104v1","updated":"2025-01-13T07:32:44Z","published":"2025-01-13T07:32:44Z","title":"RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular\n  Video Based on Rectified Mesh-embedded Gaussians","summary":"  We introduce RMAvatar, a novel human avatar representation with Gaussian\nsplatting embedded on mesh to learn clothed avatar from a monocular video. We\nutilize the explicit mesh geometry to represent motion and shape of a virtual\nhuman and implicit appearance rendering with Gaussian Splatting. Our method\nconsists of two main modules: Gaussian initialization module and Gaussian\nrectification module. We embed Gaussians into triangular faces and control\ntheir motion through the mesh, which ensures low-frequency motion and surface\ndeformation of the avatar. Due to the limitations of LBS formula, the human\nskeleton is hard to control complex non-rigid transformations. We then design a\npose-related Gaussian rectification module to learn fine-detailed non-rigid\ndeformations, further improving the realism and expressiveness of the avatar.\nWe conduct extensive experiments on public datasets, RMAvatar shows\nstate-of-the-art performance on both rendering quality and quantitative\nevaluations. Please see our project page at https://rm-avatar.github.io.\n","authors":["Sen Peng","Weixing Xie","Zilong Wang","Xiaohu Guo","Zhonggui Chen","Baorong Yang","Xiao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.07104v1.pdf","comment":"CVM2025"}],"NeRF":[{"id":"http://arxiv.org/abs/2501.05226v2","updated":"2025-01-13T15:30:39Z","published":"2025-01-09T13:29:54Z","title":"Light Transport-aware Diffusion Posterior Sampling for Single-View\n  Reconstruction of 3D Volumes","summary":"  We introduce a single-view reconstruction technique of volumetric fields in\nwhich multiple light scattering effects are omnipresent, such as in clouds. We\nmodel the unknown distribution of volumetric fields using an unconditional\ndiffusion model trained on a novel benchmark dataset comprising 1,000\nsynthetically simulated volumetric density fields. The neural diffusion model\nis trained on the latent codes of a novel, diffusion-friendly, monoplanar\nrepresentation. The generative model is used to incorporate a tailored\nparametric diffusion posterior sampling technique into different reconstruction\ntasks. A physically-based differentiable volume renderer is employed to provide\ngradients with respect to light transport in the latent space. This stands in\ncontrast to classic NeRF approaches and makes the reconstructions better\naligned with observed data. Through various experiments, we demonstrate\nsingle-view reconstruction of volumetric clouds at a previously unattainable\nquality.\n","authors":["Ludwic Leonard","Nils Thuerey","Ruediger Westermann"],"pdf_url":"https://arxiv.org/pdf/2501.05226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08072v1","updated":"2025-01-13T10:01:27Z","published":"2025-01-13T10:01:27Z","title":"Evaluating Human Perception of Novel View Synthesis: Subjective Quality\n  Assessment of Gaussian Splatting and NeRF in Dynamic Scenes","summary":"  Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two\ngroundbreaking technologies that have revolutionized the field of Novel View\nSynthesis (NVS), enabling immersive photorealistic rendering and user\nexperiences by synthesizing multiple viewpoints from a set of images of sparse\nviews. The potential applications of NVS, such as high-quality virtual and\naugmented reality, detailed 3D modeling, and realistic medical organ imaging,\nunderscore the importance of quality assessment of NVS methods from the\nperspective of human perception. Although some previous studies have explored\nsubjective quality assessments for NVS technology, they still face several\nchallenges, especially in NVS methods selection, scenario coverage, and\nevaluation methodology. To address these challenges, we conducted two\nsubjective experiments for the quality assessment of NVS technologies\ncontaining both GS-based and NeRF-based methods, focusing on dynamic and\nreal-world scenes. This study covers 360{\\deg}, front-facing, and\nsingle-viewpoint videos while providing a richer and greater number of real\nscenes. Meanwhile, it's the first time to explore the impact of NVS methods in\ndynamic scenes with moving objects. The two types of subjective experiments\nhelp to fully comprehend the influences of different viewing paths from a human\nperception perspective and pave the way for future development of\nfull-reference and no-reference quality metrics. In addition, we established a\ncomprehensive benchmark of various state-of-the-art objective metrics on the\nproposed database, highlighting that existing methods still struggle to\naccurately capture subjective quality. The results give us some insights into\nthe limitations of existing NVS methods and may promote the development of new\nNVS methods.\n","authors":["Yuhang Zhang","Joshua Maraval","Zhengyu Zhang","Nicolas Ramin","Shishun Tian","Lu Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08072v1.pdf","comment":null}]},"2025-01-12T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.03397v3","updated":"2025-01-12T23:38:16Z","published":"2025-01-06T21:34:52Z","title":"DoubleDiffusion: Combining Heat Diffusion with Denoising Diffusion for\n  Generative Learning on 3D Meshes","summary":"  This paper proposes DoubleDiffusion, a novel framework that combines heat\ndissipation diffusion and denoising diffusion for direct generative learning on\n3D mesh surfaces. Our approach addresses the challenges of generating\ncontinuous signal distributions residing on a curve manifold surface. Unlike\nprevious methods that rely on unrolling 3D meshes into 2D or adopting field\nrepresentations, DoubleDiffusion leverages the Laplacian-Beltrami operator to\nprocess features respecting the mesh structure. This combination enables\neffective geometry-aware signal diffusion across the underlying geometry. As\nshown in Fig.1, we demonstrate that DoubleDiffusion has the ability to generate\nRGB signal distributions on complex 3D mesh surfaces and achieves per-category\nshape-conditioned texture generation across different shape geometry. Our work\ncontributes a new direction in diffusion-based generative modeling on 3D\nsurfaces, with potential applications in the field of 3D asset generation.\n","authors":["Xuyang Wang","Ziang Cheng","Zhenyu Li","Jiayu Yang","Haorui Ji","Pan Ji","Mehrtash Harandi","Richard Hartley","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2501.03397v3.pdf","comment":"Codes: https://github.com/Wxyxixixi/DoubleDiffusion_3D_Mesh"},{"id":"http://arxiv.org/abs/2501.06903v1","updated":"2025-01-12T19:01:05Z","published":"2025-01-12T19:01:05Z","title":"Synthetic Prior for Few-Shot Drivable Head Avatar Inversion","summary":"  We present SynShot, a novel method for the few-shot inversion of a drivable\nhead avatar based on a synthetic prior. We tackle two major challenges. First,\ntraining a controllable 3D generative network requires a large number of\ndiverse sequences, for which pairs of images and high-quality tracked meshes\nare not always available. Second, state-of-the-art monocular avatar models\nstruggle to generalize to new views and expressions, lacking a strong prior and\noften overfitting to a specific viewpoint distribution. Inspired by machine\nlearning models trained solely on synthetic data, we propose a method that\nlearns a prior model from a large dataset of synthetic heads with diverse\nidentities, expressions, and viewpoints. With few input images, SynShot\nfine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a\nphotorealistic head avatar that generalizes to novel expressions and\nviewpoints. We model the head avatar using 3D Gaussian splatting and a\nconvolutional encoder-decoder that outputs Gaussian parameters in UV texture\nspace. To account for the different modeling complexities over parts of the\nhead (e.g., skin vs hair), we embed the prior with explicit control for\nupsampling the number of per-part primitives. Compared to state-of-the-art\nmonocular methods that require thousands of real training images, SynShot\nsignificantly improves novel view and expression synthesis.\n","authors":["Wojciech Zielonka","Stephan J. Garbin","Alexandros Lattas","George Kopanas","Paulo Gotardo","Thabo Beeler","Justus Thies","Timo Bolkart"],"pdf_url":"https://arxiv.org/pdf/2501.06903v1.pdf","comment":"Website https://zielon.github.io/synshot/"}],"NeRF":[{"id":"http://arxiv.org/abs/2501.06927v1","updated":"2025-01-12T20:36:39Z","published":"2025-01-12T20:36:39Z","title":"CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications","summary":"  In this paper, we present a large-scale fine-grained dataset using\nhigh-resolution images captured from locations worldwide. Compared to existing\ndatasets, our dataset offers a significantly larger size and includes a higher\nlevel of detail, making it uniquely suited for fine-grained 3D applications.\nNotably, our dataset is built using drone-captured aerial imagery, which\nprovides a more accurate perspective for capturing real-world site layouts and\narchitectural structures. By reconstructing environments with these detailed\nimages, our dataset supports applications such as the COLMAP format for\nGaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible\nwith widely-used techniques including SLAM, Multi-View Stereo, and Neural\nRadiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.\nThis makes it a benchmark for reconstruction and segmentation tasks. The\ndataset enables seamless integration with multi-modal data, supporting a range\nof 3D applications, from architectural reconstruction to virtual tourism. Its\nflexibility promotes innovation, facilitating breakthroughs in 3D modeling and\nanalysis.\n","authors":["Xinyi Zheng","Steve Zhang","Weizhe Lin","Aaron Zhang","Walterio W. Mayol-Cuevas","Junxiao Shen"],"pdf_url":"https://arxiv.org/pdf/2501.06927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06897v1","updated":"2025-01-12T18:38:51Z","published":"2025-01-12T18:38:51Z","title":"ActiveGAMER: Active GAussian Mapping through Efficient Rendering","summary":"  We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian\nSplatting (3DGS) to achieve high-quality, real-time scene mapping and\nexploration. Unlike traditional NeRF-based methods, which are computationally\ndemanding and restrict active mapping performance, our approach leverages the\nefficient rendering capabilities of 3DGS, allowing effective and efficient\nexploration in complex environments. The core of our system is a\nrendering-based information gain module that dynamically identifies the most\ninformative viewpoints for next-best-view planning, enhancing both geometric\nand photometric reconstruction accuracy. ActiveGAMER also integrates a\ncarefully balanced framework, combining coarse-to-fine exploration,\npost-refinement, and a global-local keyframe selection strategy to maximize\nreconstruction completeness and fidelity. Our system autonomously explores and\nreconstructs environments with state-of-the-art geometric and photometric\naccuracy and completeness, significantly surpassing existing approaches in both\naspects. Extensive evaluations on benchmark datasets such as Replica and MP3D\nhighlight ActiveGAMER's effectiveness in active mapping tasks.\n","authors":["Liyan Chen","Huangying Zhan","Kevin Chen","Xiangyu Xu","Qingan Yan","Changjiang Cai","Yi Xu"],"pdf_url":"https://arxiv.org/pdf/2501.06897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06770v1","updated":"2025-01-12T10:31:33Z","published":"2025-01-12T10:31:33Z","title":"SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for\n  Efficient and Enhanced 3D-Aware Image Synthesis","summary":"  Neural volume rendering techniques, such as NeRF, have revolutionized\n3D-aware image synthesis by enabling the generation of images of a single scene\nor object from various camera poses. However, the high computational cost of\nNeRF presents challenges for synthesizing high-resolution (HR) images. Most\nexisting methods address this issue by leveraging 2D super-resolution, which\ncompromise 3D-consistency. Other methods propose radiance manifolds or\ntwo-stage generation to achieve 3D-consistent HR synthesis, yet they are\nlimited to specific synthesis tasks, reducing their universality. To tackle\nthese challenges, we propose SuperNeRF-GAN, a universal framework for\n3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its\nseamless integration with NeRF-based 3D-aware image synthesis methods and it\ncan simultaneously enhance the resolution of generated images while preserving\n3D-consistency and reducing computational cost. Specifically, given a\npre-trained generator capable of producing a NeRF representation such as\ntri-plane, we first perform volume rendering to obtain a low-resolution image\nwith corresponding depth and normal map. Then, we employ a NeRF\nSuper-Resolution module which learns a network to obtain a high-resolution\nNeRF. Next, we propose a novel Depth-Guided Rendering process which contains\nthree simple yet effective steps, including the construction of a\nboundary-correct multi-depth map through depth aggregation, a normal-guided\ndepth super-resolution and a depth-guided NeRF rendering. Experimental results\ndemonstrate the superior efficiency, 3D-consistency, and quality of our\napproach. Additionally, ablation studies confirm the effectiveness of our\nproposed components.\n","authors":["Peng Zheng","Linzhi Huang","Yizhou Yu","Yi Chang","Yilin Wang","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2501.06770v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2409.07650v2","updated":"2025-01-12T05:21:45Z","published":"2024-09-11T22:32:12Z","title":"Foundation Models Boost Low-Level Perceptual Similarity Metrics","summary":"  For full-reference image quality assessment (FR-IQA) using deep-learning\napproaches, the perceptual similarity score between a distorted image and a\nreference image is typically computed as a distance measure between features\nextracted from a pretrained CNN or more recently, a Transformer network. Often,\nthese intermediate features require further fine-tuning or processing with\nadditional neural network layers to align the final similarity scores with\nhuman judgments. So far, most IQA models based on foundation models have\nprimarily relied on the final layer or the embedding for the quality score\nestimation. In contrast, this work explores the potential of utilizing the\nintermediate features of these foundation models, which have largely been\nunexplored so far in the design of low-level perceptual similarity metrics. We\ndemonstrate that the intermediate features are comparatively more effective.\nMoreover, without requiring any training, these metrics can outperform both\ntraditional and state-of-the-art learned metrics by utilizing distance measures\nbetween the features.\n","authors":["Abhijay Ghildyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2409.07650v2.pdf","comment":"ICASSP 2025, Code: https://github.com/abhijay9/ZS-IQA"}]},"2025-01-10T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2409.11315v2","updated":"2025-01-10T19:36:30Z","published":"2024-09-17T16:13:59Z","title":"MinD-3D++: Advancing fMRI-Based 3D Reconstruction with High-Quality\n  Textured Mesh Generation and a Comprehensive Dataset","summary":"  Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)\ndata, introduced as Recon3DMind, is of significant interest to both cognitive\nneuroscience and computer vision. To advance this task, we present the fMRI-3D\ndataset, which includes data from 15 participants and showcases a total of\n4,768 3D objects. The dataset consists of two components: fMRI-Shape,\npreviously introduced and available at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,\nproposed in this paper and available at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse\nincludes data from 5 subjects, 4 of whom are also part of the core set in\nfMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all\naccompanied by text captions. This significantly enhances the diversity and\npotential applications of the dataset. Moreover, we propose MinD-3D++, a novel\nframework for decoding textured 3D visual information from fMRI signals. The\nframework evaluates the feasibility of not only reconstructing 3D objects from\nthe human mind but also generating, for the first time, 3D textured meshes with\ndetailed textures from fMRI data. We establish new benchmarks by designing\nmetrics at the semantic, structural, and textured levels to evaluate model\nperformance. Furthermore, we assess the model's effectiveness in\nout-of-distribution settings and analyze the attribution of the proposed 3D\npari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our\nexperiments demonstrate that MinD-3D++ not only reconstructs 3D objects with\nhigh semantic and spatial accuracy but also provides deeper insights into how\nthe human brain processes 3D visual information. Project page:\nhttps://jianxgao.github.io/MinD-3D.\n","authors":["Jianxiong Gao","Yanwei Fu","Yuqian Fu","Yun Wang","Xuelin Qian","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2409.11315v2.pdf","comment":"Extended version of \"MinD-3D: Reconstruct High-quality 3D objects in\n  Human Brain\", ECCV 2024 (arXiv: 2312.07485)"},{"id":"http://arxiv.org/abs/2412.05580v2","updated":"2025-01-10T17:06:36Z","published":"2024-12-07T08:08:24Z","title":"Self-Supervised Masked Mesh Learning for Unsupervised Anomaly Detection\n  on 3D Cortical Surfaces","summary":"  Unsupervised anomaly detection in brain imaging is challenging. In this\npaper, we propose a self-supervised masked mesh learning for unsupervised\nanomaly detection in 3D cortical surfaces. Our framework leverages the\nintrinsic geometry of the cortical surface to learn a self-supervised\nrepresentation that captures the underlying structure of the brain. We\nintroduce a masked mesh convolutional neural network (MMN) that learns to\npredict masked regions of the cortical surface. By training the MMN on a large\ndataset of healthy subjects, we learn a representation that captures the normal\nvariation in the cortical surface. We then use this representation to detect\nanomalies in unseen individuals by calculating anomaly scores based on the\nreconstruction error of the MMN. We evaluate our framework by training on\npopulation-scale dataset UKB and HCP-Aging and testing on two datasets of\nAlzheimer's disease patients ADNI and OASIS3. Our results show that our\nframework can detect anomalies in cortical thickness, cortical volume, and\ncortical sulcus features, which are known to be sensitive biomarkers for\nAlzheimer's disease. Our proposed framework provides a promising approach for\nunsupervised anomaly detection based on normative variation of cortical\nfeatures.\n","authors":["Hao-Chun Yang","Sicheng Dai","Saige Rutherford","Christian Gaser","Andre F Marquand","Christian F Beckmann","Thomas Wolfers"],"pdf_url":"https://arxiv.org/pdf/2412.05580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06521v2","updated":"2025-01-10T12:05:16Z","published":"2024-06-10T17:59:01Z","title":"PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity\n  Surface Reconstruction","summary":"  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.\n","authors":["Danpeng Chen","Hai Li","Weicai Ye","Yifan Wang","Weijian Xie","Shangjin Zhai","Nan Wang","Haomin Liu","Hujun Bao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06521v2.pdf","comment":"project page: https://zju3dv.github.io/pgsr/"}],"NeRF":[{"id":"http://arxiv.org/abs/2406.06521v2","updated":"2025-01-10T12:05:16Z","published":"2024-06-10T17:59:01Z","title":"PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity\n  Surface Reconstruction","summary":"  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.\n","authors":["Danpeng Chen","Hai Li","Weicai Ye","Yifan Wang","Weijian Xie","Shangjin Zhai","Nan Wang","Haomin Liu","Hujun Bao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06521v2.pdf","comment":"project page: https://zju3dv.github.io/pgsr/"},{"id":"http://arxiv.org/abs/2501.05783v1","updated":"2025-01-10T08:33:31Z","published":"2025-01-10T08:33:31Z","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via\n  Dynamic-NeRF-based UV Mapping","summary":"  In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.75%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification.\n","authors":["Yanjie Li","Wenxuan Zhang","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.05783v1.pdf","comment":"23 pages, 22 figures, submitted to ICLR2025"}],"IQA":[{"id":"http://arxiv.org/abs/2412.07277v2","updated":"2025-01-10T12:17:00Z","published":"2024-12-10T08:07:19Z","title":"Backdoor Attacks against No-Reference Image Quality Assessment Models\n  via a Scalable Trigger","summary":"  No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the\nquality of a single input image without using any reference, plays a critical\nrole in evaluating and optimizing computer vision systems, e.g., low-light\nenhancement. Recent research indicates that NR-IQA models are susceptible to\nadversarial attacks, which can significantly alter predicted scores with\nvisually imperceptible perturbations. Despite revealing vulnerabilities, these\nattack methods have limitations, including high computational demands,\nuntargeted manipulation, limited practical utility in white-box scenarios, and\nreduced effectiveness in black-box scenarios. To address these challenges, we\nshift our focus to another significant threat and present a novel\npoisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker\nto manipulate the IQA model's output to any desired target value by simply\nadjusting a scaling coefficient $\\alpha$ for the trigger. We propose to inject\nthe trigger in the discrete cosine transform (DCT) domain to improve the local\ninvariance of the trigger for countering trigger diminishment in NR-IQA models\ndue to widely adopted data augmentations. Furthermore, the universal\nadversarial perturbations (UAP) in the DCT space are designed as the trigger,\nto increase IQA model susceptibility to manipulation and improve attack\neffectiveness. In addition to the heuristic method for poison-label BAIQA\n(P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on\n$\\alpha$ sampling and image data refinement, driven by theoretical insights we\nreveal. Extensive experiments on diverse datasets and various NR-IQA models\ndemonstrate the effectiveness of our attacks. Code can be found at\nhttps://github.com/yuyi-sd/BAIQA.\n","authors":["Yi Yu","Song Xia","Xun Lin","Wenhan Yang","Shijian Lu","Yap-peng Tan","Alex Kot"],"pdf_url":"https://arxiv.org/pdf/2412.07277v2.pdf","comment":"Accept by AAAI 2025"}],"Deblur":[{"id":"http://arxiv.org/abs/2412.05488v2","updated":"2025-01-10T00:58:28Z","published":"2024-12-07T01:19:14Z","title":"Enhancing Sample Generation of Diffusion Models using Noise Level\n  Correction","summary":"  The denoising process of diffusion models can be interpreted as an\napproximate projection of noisy samples onto the data manifold. Moreover, the\nnoise level in these samples approximates their distance to the underlying\nmanifold. Building on this insight, we propose a novel method to enhance sample\ngeneration by aligning the estimated noise level with the true distance of\nnoisy samples to the manifold. Specifically, we introduce a noise level\ncorrection network, leveraging a pre-trained denoising network, to refine noise\nlevel estimates during the denoising process. Additionally, we extend this\napproach to various image restoration tasks by integrating task-specific\nconstraints, including inpainting, deblurring, super-resolution, colorization,\nand compressed sensing. Experimental results demonstrate that our method\nsignificantly improves sample quality in both unconstrained and constrained\ngeneration scenarios. Notably, the proposed noise level correction framework is\ncompatible with existing denoising schedulers (e.g., DDIM), offering additional\nperformance improvements.\n","authors":["Abulikemu Abuduweili","Chenyang Yuan","Changliu Liu","Frank Permenter"],"pdf_url":"https://arxiv.org/pdf/2412.05488v2.pdf","comment":null}]},"2025-01-08T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.04689v1","updated":"2025-01-08T18:52:03Z","published":"2025-01-08T18:52:03Z","title":"SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single\n  Images","summary":"  We study the problem of single-image 3D object reconstruction. Recent works\nhave diverged into two directions: regression-based modeling and generative\nmodeling. Regression methods efficiently infer visible surfaces, but struggle\nwith occluded regions. Generative methods handle uncertain regions better by\nmodeling distributions, but are computationally expensive and the generation is\noften misaligned with visible surfaces. In this paper, we present SPAR3D, a\nnovel two-stage approach aiming to take the best of both directions. The first\nstage of SPAR3D generates sparse 3D point clouds using a lightweight point\ndiffusion model, which has a fast sampling speed. The second stage uses both\nthe sampled point cloud and the input image to create highly detailed meshes.\nOur two-stage design enables probabilistic modeling of the ill-posed\nsingle-image 3D task while maintaining high computational efficiency and great\noutput fidelity. Using point clouds as an intermediate representation further\nallows for interactive user edits. Evaluated on diverse datasets, SPAR3D\ndemonstrates superior performance over previous state-of-the-art methods, at an\ninference speed of 0.7 seconds. Project page with code and model:\nhttps://spar3d.github.io\n","authors":["Zixuan Huang","Mark Boss","Aaryaman Vasishta","James M. Rehg","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2501.04689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04628v1","updated":"2025-01-08T17:19:35Z","published":"2025-01-08T17:19:35Z","title":"FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using\n  Gaussian Splatting with Depth-Feature Consistency","summary":"  Recently, Gaussian Splatting has sparked a new trend in the field of computer\nvision. Apart from novel view synthesis, it has also been extended to the area\nof multi-view reconstruction. The latest methods facilitate complete, detailed\nsurface reconstruction while ensuring fast training speed. However, these\nmethods still require dense input views, and their output quality significantly\ndegrades with sparse views. We observed that the Gaussian primitives tend to\noverfit the few training views, leading to noisy floaters and incomplete\nreconstruction surfaces. In this paper, we present an innovative sparse-view\nreconstruction framework that leverages intra-view depth and multi-view feature\nconsistency to achieve remarkably accurate surface reconstruction.\nSpecifically, we utilize monocular depth ranking information to supervise the\nconsistency of depth distribution within patches and employ a smoothness loss\nto enhance the continuity of the distribution. To achieve finer surface\nreconstruction, we optimize the absolute position of depth through multi-view\nprojection features. Extensive experiments on DTU and BlendedMVS demonstrate\nthat our method outperforms state-of-the-art methods with a speedup of 60x to\n200x, achieving swift and fine-grained mesh reconstruction without the need for\ncostly pre-training.\n","authors":["Han Huang","Yulun Wu","Chao Deng","Ge Gao","Ming Gu","Yu-Shen Liu"],"pdf_url":"https://arxiv.org/pdf/2501.04628v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://alvin528.github.io/FatesGS/"},{"id":"http://arxiv.org/abs/2406.15811v2","updated":"2025-01-08T15:32:35Z","published":"2024-06-22T10:33:14Z","title":"PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored\n  Point Cloud","summary":"  Reconstructing textured meshes from colored point clouds is an important but\nchallenging task. Most existing methods yield blurry-looking textures or rely\non 3D training data that are hard to acquire. Regarding this, we propose\nPointDreamer, a novel framework for textured mesh reconstruction from colored\npoint cloud via diffusion-based 2D inpainting. Specifically, we first\nreconstruct an untextured mesh. Next, we project the input point cloud into 2D\nspace to generate sparse multi-view images, and then inpaint empty pixels\nutilizing a pre-trained 2D diffusion model. After that, we unproject the colors\nof the inpainted dense images onto the untextured mesh, thus obtaining the\nfinal textured mesh. This project-inpaint-unproject pipeline bridges the gap\nbetween 3D point clouds and 2D diffusion models for the first time. Thanks to\nthe powerful 2D diffusion model pre-trained on extensive 2D data, PointDreamer\nreconstructs clear, high-quality textures with high robustness to sparse or\nnoisy input. Also, it's zero-shot requiring no extra training. In addition, we\ndesign Non-Border-First unprojection strategy to address the border-area\ninconsistency issue, which is less explored but commonly-occurred in methods\nthat generate 3D textures from multiview images. Extensive qualitative and\nquantitative experiments on various synthetic and real-scanned datasets show\nthe SoTA performance of PointDreamer, by significantly outperforming baseline\nmethods with 30% improvement in LPIPS score (from 0.118 to 0.068). Code at:\nhttps://github.com/YuQiao0303/PointDreamer.\n","authors":["Qiao Yu","Xianzhi Li","Yuan Tang","Xu Han","Jinfeng Xu","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2406.15811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04374v1","updated":"2025-01-08T09:28:25Z","published":"2025-01-08T09:28:25Z","title":"Instructive3D: Editing Large Reconstruction Models with Text\n  Instructions","summary":"  Transformer based methods have enabled users to create, modify, and\ncomprehend text and image data. Recently proposed Large Reconstruction Models\n(LRMs) further extend this by providing the ability to generate high-quality 3D\nmodels with the help of a single object image. These models, however, lack the\nability to manipulate or edit the finer details, such as adding standard design\npatterns or changing the color and reflectance of the generated objects, thus\nlacking fine-grained control that may be very helpful in domains such as\naugmented reality, animation and gaming. Naively training LRMs for this purpose\nwould require generating precisely edited images and 3D object pairs, which is\ncomputationally expensive. In this paper, we propose Instructive3D, a novel LRM\nbased model that integrates generation and fine-grained editing, through user\ntext prompts, of 3D objects into a single model. We accomplish this by adding\nan adapter that performs a diffusion process conditioned on a text prompt\nspecifying edits in the triplane latent space representation of 3D object\nmodels. Our method does not require the generation of edited 3D objects.\nAdditionally, Instructive3D allows us to perform geometrically consistent\nmodifications, as the edits done through user-defined text prompts are applied\nto the triplane latent representation thus enhancing the versatility and\nprecision of 3D objects generated. We compare the objects generated by\nInstructive3D and a baseline that first generates the 3D object meshes using a\nstandard LRM model and then edits these 3D objects using text prompts when\nimages are provided from the Objaverse LVIS dataset. We find that Instructive3D\nproduces qualitatively superior 3D objects with the properties specified by the\nedit prompts.\n","authors":["Kunal Kathare","Ankit Dhiman","K Vikas Gowda","Siddharth Aravindan","Shubham Monga","Basavaraja Shanthappa Vandrotti","Lokesh R Boregowda"],"pdf_url":"https://arxiv.org/pdf/2501.04374v1.pdf","comment":"Accepted at WACV 2025. First two authors contributed equally"}],"Deblur":[{"id":"http://arxiv.org/abs/2501.04486v1","updated":"2025-01-08T13:13:52Z","published":"2025-01-08T13:13:52Z","title":"MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration","summary":"  Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2.\n","authors":["Zhi Jin","Yuwei Qiu","Kaihao Zhang","Hongdong Li","Wenhan Luo"],"pdf_url":"https://arxiv.org/pdf/2501.04486v1.pdf","comment":null}]},"2025-01-07T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.00625v2","updated":"2025-01-07T16:49:29Z","published":"2024-12-31T19:53:27Z","title":"Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google\n  Earth and Gaussian Splatting","summary":"  Recently released open-source pre-trained foundational image segmentation and\nobject detection models (SAM2+GroundingDINO) allow for geometrically consistent\nsegmentation of objects of interest in multi-view 2D images. Users can use\ntext-based or click-based prompts to segment objects of interest without\nrequiring labeled training datasets. Gaussian Splatting allows for the learning\nof the 3D representation of a scene's geometry and radiance based on 2D images.\nCombining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and\nour improvements in mask refinement based on morphological operations and\ncontour simplification, we created a pipeline to extract the 3D mesh of any\nbuilding based on its name, address, or geographic coordinates.\n","authors":["Kyle Gao","Liangzhi Li","Hongjie He","Dening Lu","Linlin Xu","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2501.00625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03830v1","updated":"2025-01-07T14:41:26Z","published":"2025-01-07T14:41:26Z","title":"MeshConv3D: Efficient convolution and pooling operators for triangular\n  3D meshes","summary":"  Convolutional neural networks (CNNs) have been pivotal in various 2D image\nanalysis tasks, including computer vision, image indexing and retrieval or\nsemantic classification. Extending CNNs to 3D data such as point clouds and 3D\nmeshes raises significant challenges since the very basic convolution and\npooling operators need to be completely re-visited and re-defined in an\nappropriate manner to tackle irregular connectivity issues. In this paper, we\nintroduce MeshConv3D, a 3D mesh-dedicated methodology integrating specialized\nconvolution and face collapse-based pooling operators. MeshConv3D operates\ndirectly on meshes of arbitrary topology, without any need of prior\nre-meshing/conversion techniques. In order to validate our approach, we have\nconsidered a semantic classification task. The experimental results obtained on\nthree distinct benchmark datasets show that the proposed approach makes it\npossible to achieve equivalent or superior classification results, while\nminimizing the related memory footprint and computational load.\n","authors":["Germain Bregeon","Marius Preda","Radu Ispas","Titus Zaharia"],"pdf_url":"https://arxiv.org/pdf/2501.03830v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2501.04074v1","updated":"2025-01-07T18:59:53Z","published":"2025-01-07T18:59:53Z","title":"NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View\n  Mirror Scene Reconstruction with 3D Surface Primitives","summary":"  While neural radiance fields (NeRF) led to a breakthrough in photorealistic\nnovel view synthesis, handling mirroring surfaces still denotes a particular\nchallenge as they introduce severe inconsistencies in the scene representation.\nPrevious attempts either focus on reconstructing single reflective objects or\nrely on strong supervision guidance in terms of additional user-provided\nannotations of visible image regions of the mirrors, thereby limiting the\npractical usability. In contrast, in this paper, we present NeRF-MD, a method\nwhich shows that NeRFs can be considered as mirror detectors and which is\ncapable of reconstructing neural radiance fields of scenes containing mirroring\nsurfaces without the need for prior annotations. To this end, we first compute\nan initial estimate of the scene geometry by training a standard NeRF using a\ndepth reprojection loss. Our key insight lies in the fact that parts of the\nscene corresponding to a mirroring surface will still exhibit a significant\nphotometric inconsistency, whereas the remaining parts are already\nreconstructed in a plausible manner. This allows us to detect mirror surfaces\nby fitting geometric primitives to such inconsistent regions in this initial\nstage of the training. Using this information, we then jointly optimize the\nradiance field and mirror geometry in a second training stage to refine their\nquality. We demonstrate the capability of our method to allow the faithful\ndetection of mirrors in the scene as well as the reconstruction of a single\nconsistent scene representation, and demonstrate its potential in comparison to\nbaseline and mirror-aware approaches.\n","authors":["Leif Van Holland","Michael Weinmann","Jan U. Müller","Patrick Stotko","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2501.04074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03992v1","updated":"2025-01-07T18:50:06Z","published":"2025-01-07T18:50:06Z","title":"NeuralSVG: An Implicit Representation for Text-to-Vector Generation","summary":"  Vector graphics are essential in design, providing artists with a versatile\nmedium for creating resolution-independent and highly editable visual content.\nRecent advancements in vision-language and diffusion models have fueled\ninterest in text-to-vector graphics generation. However, existing approaches\noften suffer from over-parameterized outputs or treat the layered structure - a\ncore feature of vector graphics - as a secondary goal, diminishing their\npractical use. Recognizing the importance of layered SVG representations, we\npropose NeuralSVG, an implicit neural representation for generating vector\ngraphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),\nNeuralSVG encodes the entire scene into the weights of a small MLP network,\noptimized using Score Distillation Sampling (SDS). To encourage a layered\nstructure in the generated SVG, we introduce a dropout-based regularization\ntechnique that strengthens the standalone meaning of each shape. We\nadditionally demonstrate that utilizing a neural representation provides an\nadded benefit of inference-time control, enabling users to dynamically adapt\nthe generated SVG based on user-provided inputs, all with a single learned\nrepresentation. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that NeuralSVG outperforms existing methods in generating\nstructured and flexible SVG.\n","authors":["Sagi Polaczek","Yuval Alaluf","Elad Richardson","Yael Vinker","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2501.03992v1.pdf","comment":"Project Page: https://sagipolaczek.github.io/NeuralSVG/"},{"id":"http://arxiv.org/abs/2501.03605v1","updated":"2025-01-07T08:06:35Z","published":"2025-01-07T08:06:35Z","title":"ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian\n  Splatting","summary":"  With the rapid development of 3D reconstruction technology, the widespread\ndistribution of 3D data has become a future trend. While traditional visual\ndata (such as images and videos) and NeRF-based formats already have mature\ntechniques for copyright protection, steganographic techniques for the emerging\n3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address\nthis, we propose ConcealGS, an innovative method for embedding implicit\ninformation into 3D-GS. By introducing the knowledge distillation and gradient\noptimization strategy based on 3D-GS, ConcealGS overcomes the limitations of\nNeRF-based models and enhances the robustness of implicit information and the\nquality of 3D reconstruction. We evaluate ConcealGS in various potential\napplication scenarios, and experimental results have demonstrated that\nConcealGS not only successfully recovers implicit information but also has\nalmost no impact on rendering quality, providing a new approach for embedding\ninvisible and recoverable information into 3D models in the future.\n","authors":["Yifeng Yang","Hengyu Liu","Chenxin Li","Yining Sun","Wuyang Li","Yifan Liu","Yiyang Lin","Yixuan Yuan","Nanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2501.03605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02807v2","updated":"2025-01-07T07:47:22Z","published":"2025-01-06T07:00:22Z","title":"AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal\n  Conditions and Larger Scene","summary":"  Compared to frame-based methods, computational neuromorphic imaging using\nevent cameras offers significant advantages, such as minimal motion blur,\nenhanced temporal resolution, and high dynamic range. The multi-view\nconsistency of Neural Radiance Fields combined with the unique benefits of\nevent cameras, has spurred recent research into reconstructing NeRF from data\ncaptured by moving event cameras. While showing impressive performance,\nexisting methods rely on ideal conditions with the availability of uniform and\nhigh-quality event sequences and accurate camera poses, and mainly focus on the\nobject level reconstruction, thus limiting their practical applications. In\nthis work, we propose AE-NeRF to address the challenges of learning event-based\nNeRF from non-ideal conditions, including non-uniform event sequences, noisy\nposes, and various scales of scenes. Our method exploits the density of event\nstreams and jointly learn a pose correction module with an event-based NeRF\n(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.\nTo generalize to larger scenes, we propose hierarchical event distillation with\na proposal e-NeRF network and a vanilla e-NeRF network to resample and refine\nthe reconstruction process. We further propose an event reconstruction loss and\na temporal loss to improve the view consistency of the reconstructed scene. We\nestablished a comprehensive benchmark that includes large-scale scenes to\nsimulate practical non-ideal conditions, incorporating both synthetic and\nchallenging real-world event datasets. The experimental results show that our\nmethod achieves a new state-of-the-art in event-based 3D reconstruction.\n","authors":["Chaoran Feng","Wangbo Yu","Xinhua Cheng","Zhenyu Tang","Junwu Zhang","Li Yuan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2501.02807v2.pdf","comment":null}]},"2025-01-06T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2410.22715v2","updated":"2025-01-06T16:43:05Z","published":"2024-10-30T05:53:07Z","title":"SCRREAM : SCan, Register, REnder And Map:A Framework for Annotating\n  Accurate and Dense 3D Indoor Scenes with a Benchmark","summary":"  Traditionally, 3d indoor datasets have generally prioritized scale over\nground-truth accuracy in order to obtain improved generalization. However,\nusing these datasets to evaluate dense geometry tasks, such as depth rendering,\ncan be problematic as the meshes of the dataset are often incomplete and may\nproduce wrong ground truth to evaluate the details. In this paper, we propose\nSCRREAM, a dataset annotation framework that allows annotation of fully dense\nmeshes of objects in the scene and registers camera poses on the real image\nsequence, which can produce accurate ground truth for both sparse 3D as well as\ndense 3D tasks. We show the details of the dataset annotation pipeline and\nshowcase four possible variants of datasets that can be obtained from our\nframework with example scenes, such as indoor reconstruction and SLAM, scene\nediting & object removal, human reconstruction and 6d pose estimation. Recent\npipelines for indoor reconstruction and SLAM serve as new benchmarks. In\ncontrast to previous indoor dataset, our design allows to evaluate dense\ngeometry tasks on eleven sample scenes against accurately rendered ground truth\ndepth maps.\n","authors":["HyunJun Jung","Weihang Li","Shun-Cheng Wu","William Bittner","Nikolas Brasch","Jifei Song","Eduardo Pérez-Pellitero","Zhensong Zhang","Arthur Moreau","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2410.22715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02845v1","updated":"2025-01-06T08:48:17Z","published":"2025-01-06T08:48:17Z","title":"HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian\n  Splatting Based Data Augmentation","summary":"  Understanding of bimanual hand-object interaction plays an important role in\nrobotics and virtual reality. However, due to significant occlusions between\nhands and object as well as the high degree-of-freedom motions, it is\nchallenging to collect and annotate a high-quality, large-scale dataset, which\nprevents further improvement of bimanual hand-object interaction-related\nbaselines. In this work, we propose a new 3D Gaussian Splatting based data\naugmentation framework for bimanual hand-object interaction, which is capable\nof augmenting existing dataset to large-scale photorealistic data with various\nhand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects\nand hands, and to deal with the rendering blur problem due to multi-resolution\ninput images used, we design a super-resolution module. Second, we extend the\nsingle hand grasping pose optimization module for the bimanual hand object to\ngenerate various poses of bimanual hand-object interaction, which can\nsignificantly expand the pose distribution of the dataset. Third, we conduct an\nanalysis for the impact of different aspects of the proposed data augmentation\non the understanding of the bimanual hand-object interaction. We perform our\ndata augmentation on two benchmarks, H2O and Arctic, and verify that our method\ncan improve the performance of the baselines.\n","authors":["Wentian Qu","Jiahe Li","Jian Cheng","Jian Shi","Chenyu Meng","Cuixia Ma","Hongan Wang","Xiaoming Deng","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.02845v1.pdf","comment":"Accepted by AAAI2025"}],"HDR":[{"id":"http://arxiv.org/abs/2412.14456v2","updated":"2025-01-06T12:41:59Z","published":"2024-12-19T02:15:55Z","title":"LEDiff: Latent Exposure Diffusion for HDR Generation","summary":"  While consumer displays increasingly support more than 10 stops of dynamic\nrange, most image assets such as internet photographs and generative AI content\nremain limited to 8-bit low dynamic range (LDR), constraining their utility\nacross high dynamic range (HDR) applications. Currently, no generative model\ncan produce high-bit, high-dynamic range content in a generalizable way.\nExisting LDR-to-HDR conversion methods often struggle to produce photorealistic\ndetails and physically-plausible dynamic range in the clipped areas. We\nintroduce LEDiff, a method that enables a generative model with HDR content\ngeneration through latent space fusion inspired by image-space exposure fusion\ntechniques. It also functions as an LDR-to-HDR converter, expanding the dynamic\nrange of existing low-dynamic range images. Our approach uses a small HDR\ndataset to enable a pretrained diffusion model to recover detail and dynamic\nrange in clipped highlights and shadows. LEDiff brings HDR capabilities to\nexisting generative models and converts any LDR image to HDR, creating\nphotorealistic HDR outputs for image generation, image-based lighting (HDR\nenvironment map generation), and photographic effects such as depth of field\nsimulation, where linear HDR data is essential for realistic quality.\n","authors":["Chao Wang","Zhihao Xia","Thomas Leimkuehler","Karol Myszkowski","Xuaner Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14456v2.pdf","comment":null}]},"2025-01-04T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2501.02158v1","updated":"2025-01-04T01:53:51Z","published":"2025-01-04T01:53:51Z","title":"Joint Optimization for 4D Human-Scene Reconstruction in the Wild","summary":"  Reconstructing human motion and its surrounding environment is crucial for\nunderstanding human-scene interaction and predicting human movements in the\nscene. While much progress has been made in capturing human-scene interaction\nin constrained environments, those prior methods can hardly reconstruct the\nnatural and diverse human motion and scene context from web videos. In this\nwork, we propose JOSH, a novel optimization-based method for 4D human-scene\nreconstruction in the wild from monocular videos. JOSH uses techniques in both\ndense scene reconstruction and human mesh recovery as initialization, and then\nit leverages the human-scene contact constraints to jointly optimize the scene,\nthe camera poses, and the human motion. Experiment results show JOSH achieves\nbetter results on both global human motion estimation and dense scene\nreconstruction by joint optimization of scene geometry and human motion. We\nfurther design a more efficient model, JOSH3R, and directly train it with\npseudo-labels from web videos. JOSH3R outperforms other optimization-free\nmethods by only training with labels predicted from JOSH, further demonstrating\nits accuracy and generalization ability.\n","authors":["Zhizheng Liu","Joe Lin","Wayne Wu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.02158v1.pdf","comment":"Project Page: https://genforce.github.io/JOSH/"}],"NeRF":[{"id":"http://arxiv.org/abs/2304.11842v4","updated":"2025-01-04T03:21:45Z","published":"2023-04-24T06:22:06Z","title":"Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via\n  Algorithm-Hardware Co-Design","summary":"  Novel view synthesis is an essential functionality for enabling immersive\nexperiences in various Augmented- and Virtual-Reality (AR/VR) applications, for\nwhich generalizable Neural Radiance Fields (NeRFs) have gained increasing\npopularity thanks to their cross-scene generalization capability. Despite their\npromise, the real-device deployment of generalizable NeRFs is bottlenecked by\ntheir prohibitive complexity due to the required massive memory accesses to\nacquire scene features, causing their ray marching process to be\nmemory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware\nco-design framework dedicated to generalizable NeRF acceleration, which for the\nfirst time enables real-time generalizable NeRFs. On the algorithm side,\nGen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact\nthat different regions of a 3D scene contribute differently to the rendered\npixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF\nhighlights an accelerator micro-architecture to maximize the data reuse\nopportunities among different rays by making use of their epipolar geometric\nrelationship. Furthermore, our Gen-NeRF accelerator features a customized\ndataflow to enhance data locality during point-to-hardware mapping and an\noptimized scene feature storage strategy to minimize memory bank conflicts.\nExtensive experiments validate the effectiveness of our proposed Gen-NeRF\nframework in enabling real-time and generalizable novel view synthesis.\n","authors":["Yonggan Fu","Zhifan Ye","Jiayi Yuan","Shunyao Zhang","Sixu Li","Haoran You","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2304.11842v4.pdf","comment":"Accepted by ISCA 2023"}]},"2024-12-29T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2412.18608v2","updated":"2024-12-29T16:01:58Z","published":"2024-12-24T18:59:43Z","title":"PartGen: Part-level 3D Generation and Reconstruction with Multi-View\n  Diffusion Models","summary":"  Text- or image-to-3D generators and 3D scanners can now produce 3D assets\nwith high-quality shapes and textures. These assets typically consist of a\nsingle, fused representation, like an implicit neural field, a Gaussian\nmixture, or a mesh, without any useful structure. However, most applications\nand creative workflows require assets to be made of several meaningful parts\nthat can be manipulated independently. To address this gap, we introduce\nPartGen, a novel approach that generates 3D objects composed of meaningful\nparts starting from text, an image, or an unstructured 3D object. First, given\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\nmodel extracts a set of plausible and view-consistent part segmentations,\ndividing the object into parts. Then, a second multi-view diffusion model takes\neach part separately, fills in the occlusions, and uses those completed views\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\ncompletion process considers the context of the entire object to ensure that\nthe parts integrate cohesively. The generative completion model can make up for\nthe information missing due to occlusions; in extreme cases, it can hallucinate\nentirely invisible parts based on the input 3D asset. We evaluate our method on\ngenerated and real 3D assets and show that it outperforms segmentation and\npart-extraction baselines by a large margin. We also showcase downstream\napplications such as 3D part editing.\n","authors":["Minghao Chen","Roman Shapovalov","Iro Laina","Tom Monnier","Jianyuan Wang","David Novotny","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2412.18608v2.pdf","comment":"Project Page: https://silent-chen.github.io/PartGen/"},{"id":"http://arxiv.org/abs/2412.20422v1","updated":"2024-12-29T10:12:01Z","published":"2024-12-29T10:12:01Z","title":"Bringing Objects to Life: 4D generation from 3D objects","summary":"  Recent advancements in generative modeling now enable the creation of 4D\ncontent (moving 3D objects) controlled with text prompts. 4D generation has\nlarge potential in applications like virtual worlds, media, and gaming, but\nexisting methods provide limited control over the appearance and geometry of\ngenerated content. In this work, we introduce a method for animating\nuser-provided 3D objects by conditioning on textual prompts to guide 4D\ngeneration, enabling custom animations while maintaining the identity of the\noriginal object. We first convert a 3D mesh into a ``static\" 4D Neural Radiance\nField (NeRF) that preserves the visual attributes of the input object. Then, we\nanimate the object using an Image-to-Video diffusion model driven by text. To\nimprove motion realism, we introduce an incremental viewpoint selection\nprotocol for sampling perspectives to promote lifelike movement and a masked\nScore Distillation Sampling (SDS) loss, which leverages attention maps to focus\noptimization on relevant regions. We evaluate our model in terms of temporal\ncoherence, prompt adherence, and visual fidelity and find that our method\noutperforms baselines that are based on other approaches, achieving up to\nthreefold improvements in identity preservation measured using LPIPS scores,\nand effectively balancing visual quality with dynamic content.\n","authors":["Ohad Rahamim","Ori Malca","Dvir Samuel","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2412.20422v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2412.20422v1","updated":"2024-12-29T10:12:01Z","published":"2024-12-29T10:12:01Z","title":"Bringing Objects to Life: 4D generation from 3D objects","summary":"  Recent advancements in generative modeling now enable the creation of 4D\ncontent (moving 3D objects) controlled with text prompts. 4D generation has\nlarge potential in applications like virtual worlds, media, and gaming, but\nexisting methods provide limited control over the appearance and geometry of\ngenerated content. In this work, we introduce a method for animating\nuser-provided 3D objects by conditioning on textual prompts to guide 4D\ngeneration, enabling custom animations while maintaining the identity of the\noriginal object. We first convert a 3D mesh into a ``static\" 4D Neural Radiance\nField (NeRF) that preserves the visual attributes of the input object. Then, we\nanimate the object using an Image-to-Video diffusion model driven by text. To\nimprove motion realism, we introduce an incremental viewpoint selection\nprotocol for sampling perspectives to promote lifelike movement and a masked\nScore Distillation Sampling (SDS) loss, which leverages attention maps to focus\noptimization on relevant regions. We evaluate our model in terms of temporal\ncoherence, prompt adherence, and visual fidelity and find that our method\noutperforms baselines that are based on other approaches, achieving up to\nthreefold improvements in identity preservation measured using LPIPS scores,\nand effectively balancing visual quality with dynamic content.\n","authors":["Ohad Rahamim","Ori Malca","Dvir Samuel","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2412.20422v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2412.20596v1","updated":"2024-12-29T22:00:42Z","published":"2024-12-29T22:00:42Z","title":"Zero-Shot Image Restoration Using Few-Step Guidance of Consistency\n  Models (and Beyond)","summary":"  In recent years, it has become popular to tackle image restoration tasks with\na single pretrained diffusion model (DM) and data-fidelity guidance, instead of\ntraining a dedicated deep neural network per task. However, such \"zero-shot\"\nrestoration schemes currently require many Neural Function Evaluations (NFEs)\nfor performing well, which may be attributed to the many NFEs needed in the\noriginal generative functionality of the DMs. Recently, faster variants of DMs\nhave been explored for image generation. These include Consistency Models\n(CMs), which can generate samples via a couple of NFEs. However, existing works\nthat use guided CMs for restoration still require tens of NFEs or fine-tuning\nof the model per task that leads to performance drop if the assumptions during\nthe fine-tuning are not accurate. In this paper, we propose a zero-shot\nrestoration scheme that uses CMs and operates well with as little as 4 NFEs. It\nis based on a wise combination of several ingredients: better initialization,\nback-projection guidance, and above all a novel noise injection mechanism. We\ndemonstrate the advantages of our approach for image super-resolution,\ndeblurring and inpainting. Interestingly, we show that the usefulness of our\nnoise injection technique goes beyond CMs: it can also mitigate the performance\ndegradation of existing guided DM methods when reducing their NFE count.\n","authors":["Tomer Garber","Tom Tirer"],"pdf_url":"https://arxiv.org/pdf/2412.20596v1.pdf","comment":"Code can be found at: https://github.com/tirer-lab/CM4IR"}]},"2024-12-25T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2412.18785v1","updated":"2024-12-25T05:35:30Z","published":"2024-12-25T05:35:30Z","title":"Simultaneously Recovering Multi-Person Meshes and Multi-View Cameras\n  with Human Semantics","summary":"  Dynamic multi-person mesh recovery has broad applications in sports\nbroadcasting, virtual reality, and video games. However, current multi-view\nframeworks rely on a time-consuming camera calibration procedure. In this work,\nwe focus on multi-person motion capture with uncalibrated cameras, which mainly\nfaces two challenges: one is that inter-person interactions and occlusions\nintroduce inherent ambiguities for both camera calibration and motion capture;\nthe other is that a lack of dense correspondences can be used to constrain\nsparse camera geometries in a dynamic multi-person scene. Our key idea is to\nincorporate motion prior knowledge to simultaneously estimate camera parameters\nand human meshes from noisy human semantics. We first utilize human information\nfrom 2D images to initialize intrinsic and extrinsic parameters. Thus, the\napproach does not rely on any other calibration tools or background features.\nThen, a pose-geometry consistency is introduced to associate the detected\nhumans from different views. Finally, a latent motion prior is proposed to\nrefine the camera parameters and human motions. Experimental results show that\naccurate camera parameters and human motions can be obtained through a one-step\nreconstruction. The code are publicly available\nat~\\url{https://github.com/boycehbz/DMMR}.\n","authors":["Buzhen Huang","Jingyi Ju","Yuan Shu","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.18785v1.pdf","comment":"TCSVT. arXiv admin note: text overlap with arXiv:2110.10355"}],"HDR":[{"id":"http://arxiv.org/abs/2412.18981v1","updated":"2024-12-25T20:36:29Z","published":"2024-12-25T20:36:29Z","title":"HAND: Hierarchical Attention Network for Multi-Scale Handwritten\n  Document Recognition and Layout Analysis","summary":"  Handwritten document recognition (HDR) is one of the most challenging tasks\nin the field of computer vision, due to the various writing styles and complex\nlayouts inherent in handwritten texts. Traditionally, this problem has been\napproached as two separate tasks, handwritten text recognition and layout\nanalysis, and struggled to integrate the two processes effectively. This paper\nintroduces HAND (Hierarchical Attention Network for Multi-Scale Document), a\nnovel end-to-end and segmentation-free architecture for simultaneous text\nrecognition and layout analysis tasks. Our model's key components include an\nadvanced convolutional encoder integrating Gated Depth-wise Separable and\nOctave Convolutions for robust feature extraction, a Multi-Scale Adaptive\nProcessing (MSAP) framework that dynamically adjusts to document complexity and\na hierarchical attention decoder with memory-augmented and sparse attention\nmechanisms. These components enable our model to scale effectively from\nsingle-line to triple-column pages while maintaining computational efficiency.\nAdditionally, HAND adopts curriculum learning across five complexity levels. To\nimprove the recognition accuracy of complex ancient manuscripts, we fine-tune\nand integrate a Domain-Adaptive Pre-trained mT5 model for post-processing\nrefinement. Extensive evaluations on the READ 2016 dataset demonstrate the\nsuperior performance of HAND, achieving up to 59.8% reduction in CER for\nline-level recognition and 31.2% for page-level recognition compared to\nstate-of-the-art methods. The model also maintains a compact size of 5.60M\nparameters while establishing new benchmarks in both text recognition and\nlayout analysis. Source code and pre-trained models are available at :\nhttps://github.com/MHHamdan/HAND.\n","authors":["Mohammed Hamdan","Abderrahmane Rahiche","Mohamed Cheriet"],"pdf_url":"https://arxiv.org/pdf/2412.18981v1.pdf","comment":null}]},"2024-12-24T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2412.18194v1","updated":"2024-12-24T06:03:42Z","published":"2024-12-24T06:03:42Z","title":"VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics\n  Manipulation with Long-Horizon Reasoning Tasks","summary":"  General-purposed embodied agents are designed to understand the users'\nnatural instructions or intentions and act precisely to complete universal\ntasks. Recently, methods based on foundation models especially\nVision-Language-Action models (VLAs) have shown a substantial potential to\nsolve language-conditioned manipulation (LCM) tasks well. However, existing\nbenchmarks do not adequately meet the needs of VLAs and relative algorithms. To\nbetter define such general-purpose tasks in the context of LLMs and advance the\nresearch in VLAs, we present VLABench, an open-source benchmark for evaluating\nuniversal LCM task learning. VLABench provides 100 carefully designed\ncategories of tasks, with strong randomization in each category of task and a\ntotal of 2000+ objects. VLABench stands out from previous benchmarks in four\nkey aspects: 1) tasks requiring world knowledge and common sense transfer, 2)\nnatural language instructions with implicit human intentions rather than\ntemplates, 3) long-horizon tasks demanding multi-step reasoning, and 4)\nevaluation of both action policies and language model capabilities. The\nbenchmark assesses multiple competencies including understanding of\nmesh\\&texture, spatial relationship, semantic instruction, physical laws,\nknowledge transfer and reasoning, etc. To support the downstream finetuning, we\nprovide high-quality training data collected via an automated framework\nincorporating heuristic skills and prior information. The experimental results\nindicate that both the current state-of-the-art pretrained VLAs and the\nworkflow based on VLMs face challenges in our tasks.\n","authors":["Shiduo Zhang","Zhe Xu","Peiju Liu","Xiaopeng Yu","Yuan Li","Qinghui Gao","Zhaoye Fei","Zhangyue Yin","Zuxuan Wu","Yu-Gang Jiang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.18194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07132v2","updated":"2024-12-24T01:01:19Z","published":"2024-12-10T02:41:21Z","title":"Revisiting Lesion Tracking in 3D Total Body Photography","summary":"  Melanoma is the most deadly form of skin cancer. Tracking the evolution of\nnevi and detecting new lesions across the body is essential for the early\ndetection of melanoma. Despite prior work on longitudinal tracking of skin\nlesions in 3D total body photography, there are still several challenges,\nincluding 1) low accuracy for finding correct lesion pairs across scans, 2)\nsensitivity to noisy lesion detection, and 3) lack of large-scale datasets with\nnumerous annotated lesion pairs. We propose a framework that takes in a pair of\n3D textured meshes, matches lesions in the context of total body photography,\nand identifies unmatchable lesions. We start by computing correspondence maps\nbringing the source and target meshes to a template mesh. Using these maps to\ndefine source/target signals over the template domain, we construct a flow\nfield aligning the mapped signals. The initial correspondence maps are then\nrefined by advecting forward/backward along the vector field. Finally, lesion\nassignment is performed using the refined correspondence maps. We propose the\nfirst large-scale dataset for skin lesion tracking with 25K lesion pairs across\n198 subjects. The proposed method achieves a success rate of 89.9% (at 10 mm\ncriterion) for all pairs of annotated lesions and a matching accuracy of 98.2%\nfor subjects with more than 200 lesions.\n","authors":["Wei-Lun Huang","Minghao Xue","Zhiyou Liu","Davood Tashayyod","Jun Kang","Amir Gandjbakhche","Misha Kazhdan","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2412.07132v2.pdf","comment":"v2"}],"Deblur":[{"id":"http://arxiv.org/abs/2407.03771v4","updated":"2024-12-24T08:15:48Z","published":"2024-07-04T09:32:12Z","title":"SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors","summary":"  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance\nin 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.\nFulfilling this requirement can be challenging in real-world scenarios\nespecially when the camera moves fast, which severely limits the application of\n3DGS. To address these challenges, we proposed Spike Gausian Splatting\n(SpikeGS), the first framework that integrates the spike streams into 3DGS\npipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With\naccumulation rasterization, interval supervision, and a specially designed\npipeline, SpikeGS extracts detailed geometry and texture from high temporal\nresolution but texture lacking spike stream, reconstructs 3D scenes captured in\n1 second. Extensive experiments on multiple synthetic and real-world datasets\ndemonstrate the superiority of SpikeGS compared with existing spike-based and\ndeblur 3D scene reconstruction methods. Codes and data will be released soon.\n","authors":["Yijia Guo","Liwen Hu","Yuanxi Bai","Jiawei Yao","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03771v4.pdf","comment":"Accepted by AAAI2025"}]},"2024-12-23T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2407.07755v2","updated":"2024-12-23T23:56:06Z","published":"2024-07-10T15:28:02Z","title":"Neural Geometry Processing via Spherical Neural Surfaces","summary":"  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. In the absence of an analogous toolbox, neural\nrepresentations are typically discretized and converted into a mesh, before\napplying any geometry processing algorithm. This is unsatisfactory and, as we\ndemonstrate, unnecessary. In this work, we propose a spherical neural surface\nrepresentation for genus-0 surfaces and demonstrate how to compute core\ngeometric operators directly on this representation. Namely, we estimate\nsurface normals and first and second fundamental forms of the surface, as well\nas compute surface gradient, surface divergence and Laplace-Beltrami operator\non scalar/vector fields defined on the surface. Our representation is fully\nseamless, overcoming a key limitation of similar explicit representations such\nas Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable\ngeometry processing directly on the neural representations without any\nunnecessary meshing. We demonstrate illustrative applications in (neural)\nspectral analysis, heat flow and mean curvature flow, and evaluate robustness\nto isometric shape variations. We propose theoretical formulations and validate\ntheir numerical estimates, against analytical estimates, mesh-based baselines,\nand neural alternatives, where available. By systematically linking neural\nsurface representations with classical geometry processing algorithms, we\nbelieve that this work can become a key ingredient in enabling neural geometry\nprocessing. Code will be released upon acceptance, accessible from the project\nwebpage.\n","authors":["Romy Williamson","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2407.07755v2.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.17806v1","updated":"2024-12-23T18:58:34Z","published":"2024-12-23T18:58:34Z","title":"Reconstructing People, Places, and Cameras","summary":"  We present \"Humans and Structure from Motion\" (HSfM), a method for jointly\nreconstructing multiple human meshes, scene point clouds, and camera parameters\nin a metric world coordinate system from a sparse set of uncalibrated\nmulti-view images featuring people. Our approach combines data-driven scene\nreconstruction with the traditional Structure-from-Motion (SfM) framework to\nachieve more accurate scene reconstruction and camera estimation, while\nsimultaneously recovering human meshes. In contrast to existing scene\nreconstruction and SfM methods that lack metric scale information, our method\nestimates approximate metric scale by leveraging a human statistical model.\nFurthermore, it reconstructs multiple human meshes within the same world\ncoordinate system alongside the scene point cloud, effectively capturing\nspatial relationships among individuals and their positions in the environment.\nWe initialize the reconstruction of humans, scenes, and cameras using robust\nfoundational models and jointly optimize these elements. This joint\noptimization synergistically improves the accuracy of each component. We\ncompare our method to existing approaches on two challenging benchmarks,\nEgoHumans and EgoExo4D, demonstrating significant improvements in human\nlocalization accuracy within the world coordinate frame (reducing error from\n3.51m to 1.04m in EgoHumans and from 2.9m to 0.56m in EgoExo4D). Notably, our\nresults show that incorporating human data into the SfM pipeline improves\ncamera pose estimation (e.g., increasing RRA@15 by 20.3% on EgoHumans).\nAdditionally, qualitative results show that our approach improves overall scene\nreconstruction quality. Our code is available at: muelea.github.io/hsfm.\n","authors":["Lea Müller","Hongsuk Choi","Anthony Zhang","Brent Yi","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2412.17806v1.pdf","comment":"Project website: muelea.github.io/hsfm"},{"id":"http://arxiv.org/abs/2412.14535v2","updated":"2024-12-23T09:14:13Z","published":"2024-12-19T05:23:49Z","title":"DAMPER: A Dual-Stage Medical Report Generation Framework with\n  Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching","summary":"  Medical report generation is crucial for clinical diagnosis and patient\nmanagement, summarizing diagnoses and recommendations based on medical imaging.\nHowever, existing work often overlook the clinical pipeline involved in report\nwriting, where physicians typically conduct an initial quick review followed by\na detailed examination. Moreover, current alignment methods may lead to\nmisaligned relationships. To address these issues, we propose DAMPER, a\ndual-stage framework for medical report generation that mimics the clinical\npipeline of report writing in two stages. In the first stage, a MeSH-Guided\nCoarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image\nfeatures with medical subject headings (MeSH) features to generate a rough\nkeyphrase representation of the overall impression. In the second stage, a\nHypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs\nhypergraphs for image patches and report annotations, modeling high-order\nrelationships within each modality and performing hypergraph matching to\ncapture semantic correlations between image regions and textual phrases.\nFinally,the coarse-grained visual features, generated MeSH representations, and\nvisual hypergraph features are fed into a report decoder to produce the final\nmedical report. Extensive experiments on public datasets demonstrate the\neffectiveness of DAMPER in generating comprehensive and accurate medical\nreports, outperforming state-of-the-art methods across various evaluation\nmetrics.\n","authors":["Xiaofei Huang","Wenting Chen","Jie Liu","Qisheng Lu","Xiaoling Luo","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14535v2.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2412.17628v1","updated":"2024-12-23T14:59:46Z","published":"2024-12-23T14:59:46Z","title":"Editing Implicit and Explicit Representations of Radiance Fields: A\n  Survey","summary":"  Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent\nyears by offering a new volumetric representation, which is compact and\nprovides high-quality image rendering. However, the methods to edit those\nradiance fields developed slower than the many improvements to other aspects of\nNeRF. With the recent development of alternative radiance field-based\nrepresentations inspired by NeRF as well as the worldwide rise in popularity of\ntext-to-image models, many new opportunities and strategies have emerged to\nprovide radiance field editing. In this paper, we deliver a comprehensive\nsurvey of the different editing methods present in the literature for NeRF and\nother similar radiance field representations. We propose a new taxonomy for\nclassifying existing works based on their editing methodologies, review\npioneering models, reflect on current and potential new applications of\nradiance field editing, and compare state-of-the-art approaches in terms of\nediting options and performance.\n","authors":["Arthur Hubert","Gamal Elghazaly","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2412.17628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19525v2","updated":"2024-12-23T14:15:50Z","published":"2024-11-29T07:49:44Z","title":"LokiTalk: Learning Fine-Grained and Generalizable Correspondences to\n  Enhance NeRF-based Talking Head Synthesis","summary":"  Despite significant progress in talking head synthesis since the introduction\nof Neural Radiance Fields (NeRF), visual artifacts and high training costs\npersist as major obstacles to large-scale commercial adoption. We propose that\nidentifying and establishing fine-grained and generalizable correspondences\nbetween driving signals and generated results can simultaneously resolve both\nproblems. Here we present LokiTalk, a novel framework designed to enhance\nNeRF-based talking heads with lifelike facial dynamics and improved training\nefficiency. To achieve fine-grained correspondences, we introduce\nRegion-Specific Deformation Fields, which decompose the overall portrait motion\ninto lip movements, eye blinking, head pose, and torso movements. By\nhierarchically modeling the driving signals and their associated regions\nthrough two cascaded deformation fields, we significantly improve dynamic\naccuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware\nKnowledge Transfer, a plug-and-play module that learns generalizable dynamic\nand static correspondences from multi-identity videos, while simultaneously\nextracting ID-specific dynamic and static features to refine the depiction of\nindividual characters. Comprehensive evaluations demonstrate that LokiTalk\ndelivers superior high-fidelity results and training efficiency compared to\nprevious methods. The code will be released upon acceptance.\n","authors":["Tianqi Li","Ruobing Zheng","Bonan Li","Zicheng Zhang","Meng Wang","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2411.19525v2.pdf","comment":"Project Page: https://digital-avatar.github.io/ai/LokiTalk/"},{"id":"http://arxiv.org/abs/2412.17532v1","updated":"2024-12-23T12:57:34Z","published":"2024-12-23T12:57:34Z","title":"Exploring Dynamic Novel View Synthesis Technologies for Cinematography","summary":"  Novel view synthesis (NVS) has shown significant promise for applications in\ncinematographic production, particularly through the exploitation of Neural\nRadiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D\nscenes, enabling the creation of new shots that are challenging to capture in\nthe real world due to set topology or expensive equipment requirement. This\ninnovation also offers cinematographic advantages such as smooth camera\nmovements, virtual re-shoots, slow-motion effects, etc. This paper explores\ndynamic NVS with the aim of facilitating the model selection process. We\nshowcase its potential through a short montage filmed using various NVS models.\n","authors":["Adrian Azzarelli","Nantheera Anantrasirichai","David R Bull"],"pdf_url":"https://arxiv.org/pdf/2412.17532v1.pdf","comment":null}]},"2025-01-11T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2501.06488v1","updated":"2025-01-11T09:12:43Z","published":"2025-01-11T09:12:43Z","title":"NVS-SQA: Exploring Self-Supervised Quality Representation Learning for\n  Neurally Synthesized Scenes without References","summary":"  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).\n","authors":["Qiang Qu","Yiran Shen","Xiaoming Chen","Yuk Ying Chung","Weidong Cai","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15890v2","updated":"2025-01-11T08:09:56Z","published":"2024-12-20T13:40:28Z","title":"NeuroPump: Simultaneous Geometric and Color Rectification for Underwater\n  Images","summary":"  Underwater image restoration aims to remove geometric and color distortions\ndue to water refraction, absorption and scattering. Previous studies focus on\nrestoring either color or the geometry, but to our best knowledge, not both.\nHowever, in practice it may be cumbersome to address the two rectifications\none-by-one. In this paper, we propose NeuroPump, a self-supervised method to\nsimultaneously optimize and rectify underwater geometry and color as if water\nwere pumped out. The key idea is to explicitly model refraction, absorption and\nscattering in Neural Radiance Field (NeRF) pipeline, such that it not only\nperforms simultaneous geometric and color rectification, but also enables to\nsynthesize novel views and optical effects by controlling the decoupled\nparameters. In addition, to address issue of lack of real paired ground truth\nimages, we propose an underwater 360 benchmark dataset that has real paired\n(i.e., with and without water) images. Our method clearly outperforms other\nbaselines both quantitatively and qualitatively. Our project page is available\nat: https://ygswu.github.io/NeuroPump.github.io/.\n","authors":["Yue Guo","Haoxiang Liao","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.15890v2.pdf","comment":null}]},"2025-01-05T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2412.19370v2","updated":"2025-01-05T21:19:48Z","published":"2024-12-26T22:35:29Z","title":"BeSplat: Gaussian Splatting from a Single Blurry Image and Event Stream","summary":"  Novel view synthesis has been greatly enhanced by the development of radiance\nfield methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively\naddressed key challenges, such as long training times and slow rendering\nspeeds, typically associated with Neural Radiance Fields (NeRF), while\nmaintaining high-quality reconstructions. In this work (BeSplat), we\ndemonstrate the recovery of sharp radiance field (Gaussian splats) from a\nsingle motion-blurred image and its corresponding event stream. Our method\njointly learns the scene representation via Gaussian Splatting and recovers the\ncamera motion through Bezier SE(3) formulation effectively, minimizing\ndiscrepancies between synthesized and real-world measurements of both blurry\nimage and corresponding event stream. We evaluate our approach on both\nsynthetic and real datasets, showcasing its ability to render view-consistent,\nsharp images from the learned radiance field and the estimated camera\ntrajectory. To the best of our knowledge, ours is the first work to address\nthis highly challenging ill-posed problem in a Gaussian Splatting framework\nwith the effective incorporation of temporal information captured using the\nevent stream.\n","authors":["Gopi Raju Matta","Reddypalli Trisha","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.19370v2.pdf","comment":"Accepted for publication at EVGEN2025, WACV-25 Workshop"},{"id":"http://arxiv.org/abs/2410.18137v2","updated":"2025-01-05T07:36:42Z","published":"2024-10-22T00:02:26Z","title":"Advancing Super-Resolution in Neural Radiance Fields via Variational\n  Diffusion Strategies","summary":"  We present a novel method for diffusion-guided frameworks for view-consistent\nsuper-resolution (SR) in neural rendering. Our approach leverages existing 2D\nSR models in conjunction with advanced techniques such as Variational Score\nDistilling (VSD) and a LoRA fine-tuning helper, with spatial training to\nsignificantly boost the quality and consistency of upscaled 2D images compared\nto the previous methods in the literature, such as Renoised Score Distillation\n(RSD) proposed in DiSR-NeRF (1), or SDS proposed in DreamFusion. The VSD score\nfacilitates precise fine-tuning of SR models, resulting in high-quality,\nview-consistent images. To address the common challenge of inconsistencies\namong independent SR 2D images, we integrate Iterative 3D Synchronization\n(I3DS) from the DiSR-NeRF framework. Our quantitative benchmarks and\nqualitative results on the LLFF dataset demonstrate the superior performance of\nour system compared to existing methods such as DiSR-NeRF.\n","authors":["Shrey Vishen","Jatin Sarabu","Saurav Kumar","Chinmay Bharathulwar","Rithwick Lakshmanan","Vishnu Srinivas"],"pdf_url":"https://arxiv.org/pdf/2410.18137v2.pdf","comment":"All our code is available at\n  https://github.com/shreyvish5678/Advancing-Super-Resolution-in-Neural-Radiance-Fields-via-Variational-Diffusion-Strategies"}]},"2025-01-01T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2404.06727v2","updated":"2025-01-01T04:29:58Z","published":"2024-04-10T04:24:42Z","title":"Bayesian NeRF: Quantifying Uncertainty with Volume Density for Neural\n  Implicit Fields","summary":"  We present a Bayesian Neural Radiance Field (NeRF), which explicitly\nquantifies uncertainty in the volume density by modeling uncertainty in the\noccupancy, without the need for additional networks, making it particularly\nsuited for challenging observations and uncontrolled image environments. NeRF\ndiverges from traditional geometric methods by providing an enriched scene\nrepresentation, rendering color and density in 3D space from various\nviewpoints. However, NeRF encounters limitations in addressing uncertainties\nsolely through geometric structure information, leading to inaccuracies when\ninterpreting scenes with insufficient real-world observations. While previous\nefforts have relied on auxiliary networks, we propose a series of formulation\nextensions to NeRF that manage uncertainties in density, both color and\ndensity, and occupancy, all without the need for additional networks. In\nexperiments, we show that our method significantly enhances performance on RGB\nand depth images in the comprehensive dataset. Given that uncertainty modeling\naligns well with the inherently uncertain environments of Simultaneous\nLocalization and Mapping (SLAM), we applied our approach to SLAM systems and\nobserved notable improvements in mapping and tracking performance. These\nresults confirm the effectiveness of our Bayesian NeRF approach in quantifying\nuncertainty based on geometric structure, making it a robust solution for\nchallenging real-world scenarios.\n","authors":["Sibeak Lee","Kyeongsu Kang","Seongbo Ha","Hyeonwoo Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06727v2.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2401.12977v2","updated":"2025-01-01T05:24:46Z","published":"2024-01-23T18:59:56Z","title":"IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images","summary":"  Inverse rendering seeks to recover 3D geometry, surface material, and\nlighting from captured images, enabling advanced applications such as\nnovel-view synthesis, relighting, and virtual object insertion. However, most\nexisting techniques rely on high dynamic range (HDR) images as input, limiting\naccessibility for general users. In response, we introduce IRIS, an inverse\nrendering framework that recovers the physically based material,\nspatially-varying HDR lighting, and camera response functions from multi-view,\nlow-dynamic-range (LDR) images. By eliminating the dependence on HDR input, we\nmake inverse rendering technology more accessible. We evaluate our approach on\nreal-world and synthetic scenes and compare it with state-of-the-art methods.\nOur results show that IRIS effectively recovers HDR lighting, accurate\nmaterial, and plausible camera response functions, supporting photorealistic\nrelighting and object insertion.\n","authors":["Chih-Hao Lin","Jia-Bin Huang","Zhengqin Li","Zhao Dong","Christian Richardt","Tuotuo Li","Michael Zollhöfer","Johannes Kopf","Shenlong Wang","Changil Kim"],"pdf_url":"https://arxiv.org/pdf/2401.12977v2.pdf","comment":"Project Website: https://irisldr.github.io/"}]},"2024-12-30T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2404.13921v2","updated":"2024-12-30T13:26:37Z","published":"2024-04-22T06:59:03Z","title":"NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion\n  Strategies for NeRF-based Indoor Multi-view 3D Object Detection","summary":"  In indoor scenes, the diverse distribution of object locations and scales\nmakes the visual 3D perception task a big challenge.\n  Previous works (e.g, NeRF-Det) have demonstrated that implicit representation\nhas the capacity to benefit the visual 3D perception task in indoor scenes with\nhigh amount of overlap between input images.\n  However, previous works cannot fully utilize the advancement of implicit\nrepresentation because of fixed sampling and simple multi-view feature fusion.\n  In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a\nsimple yet effective method, NeRF-DetS, to address above issues. NeRF-DetS\nincludes two modules: Progressive Adaptive Sampling Strategy (PASS) and\nDepth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).\n  Specifically,\n  (1)PASS can automatically sample features of each layer within a dense 3D\ndetector, using offsets predicted by the previous layer.\n  (2)DS-MHA can not only efficiently fuse multi-view features with strong\nocclusion awareness but also reduce computational cost.\n  Extensive experiments on ScanNetV2 dataset demonstrate our NeRF-DetS\noutperforms NeRF-Det, by achieving +5.02% and +5.92% improvement in mAP under\nIoU25 and IoU50, respectively. Also, NeRF-DetS shows consistent improvements on\nARKITScenes.\n","authors":["Chi Huang","Xinyang Li","Yansong Qu","Changli Wu","Xiaofan Li","Shengchuan Zhang","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2404.13921v2.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2412.18774v2","updated":"2024-12-30T14:54:57Z","published":"2024-12-25T04:29:22Z","title":"Embodied Image Quality Assessment for Robotic Intelligence","summary":"  Image quality assessment (IQA) of user-generated content (UGC) is a critical\ntechnique for human quality of experience (QoE). However, for robot-generated\ncontent (RGC), will its image quality be consistent with the Moravec paradox\nand counter to human common sense? Human subjective scoring is more based on\nthe attractiveness of the image. Embodied agent are required to interact and\nperceive in the environment, and finally perform specific tasks. Visual images\nas inputs directly influence downstream tasks. In this paper, we first propose\nan embodied image quality assessment (EIQA) frameworks. We establish assessment\nmetrics for input images based on the downstream tasks of robot. In addition,\nwe construct an Embodied Preference Database (EPD) containing 5,000 reference\nand distorted image annotations. The performance of mainstream IQA algorithms\non EPD dataset is finally verified. The experiments demonstrate that quality\nassessment of embodied images is different from that of humans. We sincerely\nhope that the EPD can contribute to the development of embodied AI by focusing\non image quality assessment. The benchmark is available at\nhttps://github.com/Jianbo-maker/EPD_benchmark.\n","authors":["Jianbo Zhang","Chunyi Li","Liang Yuan","Guoquan Zheng","Jie Hao","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.18774v2.pdf","comment":"6 pages, 5 figures"}]},"2024-12-26T00:00:00Z":{"NeRF":[{"id":"http://arxiv.org/abs/2412.15447v2","updated":"2024-12-26T16:37:43Z","published":"2024-12-19T22:59:55Z","title":"LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene\n  Reconstruction","summary":"  Photorealistic 3D scene reconstruction plays an important role in autonomous\ndriving, enabling the generation of novel data from existing datasets to\nsimulate safety-critical scenarios and expand training data without additional\nacquisition costs. Gaussian Splatting (GS) facilitates real-time,\nphotorealistic rendering with an explicit 3D Gaussian representation of the\nscene, providing faster processing and more intuitive scene editing than the\nimplicit Neural Radiance Fields (NeRFs). While extensive GS research has\nyielded promising advancements in autonomous driving applications, they\noverlook two critical aspects: First, existing methods mainly focus on\nlow-speed and feature-rich urban scenes and ignore the fact that highway\nscenarios play a significant role in autonomous driving. Second, while LiDARs\nare commonplace in autonomous driving platforms, existing methods learn\nprimarily from images and use LiDAR only for initial estimates or without\nprecise sensor modeling, thus missing out on leveraging the rich depth\ninformation LiDAR offers and limiting the ability to synthesize LiDAR data. In\nthis paper, we propose a novel GS method for dynamic scene synthesis and\nediting with improved scene reconstruction through LiDAR supervision and\nsupport for LiDAR rendering. Unlike prior works that are tested mostly on urban\ndatasets, to the best of our knowledge, we are the first to focus on the more\nchallenging and highly relevant highway scenes for autonomous driving, with\nsparse sensor views and monotone backgrounds. Visit our project page at:\nhttps://umautobots.github.io/lihi_gs\n","authors":["Pou-Chun Kung","Xianling Zhang","Katherine A. Skinner","Nikita Jaipuria"],"pdf_url":"https://arxiv.org/pdf/2412.15447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19149v1","updated":"2024-12-26T10:10:03Z","published":"2024-12-26T10:10:03Z","title":"Generating Editable Head Avatars with 3D Gaussian GANs","summary":"  Generating animatable and editable 3D head avatars is essential for various\napplications in computer vision and graphics. Traditional 3D-aware generative\nadversarial networks (GANs), often using implicit fields like Neural Radiance\nFields (NeRF), achieve photorealistic and view-consistent 3D head synthesis.\nHowever, these methods face limitations in deformation flexibility and\neditability, hindering the creation of lifelike and easily modifiable 3D heads.\nWe propose a novel approach that enhances the editability and animation control\nof 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit\n3D representation. This method enables easier illumination control and improved\neditability. Central to our approach is the Editable Gaussian Head (EG-Head)\nmodel, which combines a 3D Morphable Model (3DMM) with texture maps, allowing\nprecise expression control and flexible texture editing for accurate animation\nwhile preserving identity. To capture complex non-facial geometries like hair,\nwe use an auxiliary set of 3DGS and tri-plane features. Extensive experiments\ndemonstrate that our approach delivers high-quality 3D-aware synthesis with\nstate-of-the-art controllability. Our code and models are available at\nhttps://github.com/liguohao96/EGG3D.\n","authors":["Guohao Li","Hongyu Yang","Yifang Men","Di Huang","Weixin Li","Ruijie Yang","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.19149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19130v1","updated":"2024-12-26T09:20:04Z","published":"2024-12-26T09:20:04Z","title":"MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View\n  Stereo","summary":"  This study addresses the challenge of online 3D model generation for neural\nrendering using an RGB image stream. Previous research has tackled this issue\nby incorporating Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS)\nas scene representations within dense SLAM methods. However, most studies focus\nprimarily on estimating coarse 3D scenes rather than achieving detailed\nreconstructions. Moreover, depth estimation based solely on images is often\nambiguous, resulting in low-quality 3D models that lead to inaccurate\nrenderings. To overcome these limitations, we propose a novel framework for\nhigh-quality 3DGS modeling that leverages an online multi-view stereo (MVS)\napproach. Our method estimates MVS depth using sequential frames from a local\ntime window and applies comprehensive depth refinement techniques to filter out\noutliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore,\nwe introduce a parallelized backend module that optimizes the 3DGS model\nefficiently, ensuring timely updates with each new keyframe. Experimental\nresults demonstrate that our method outperforms state-of-the-art dense SLAM\nmethods, particularly excelling in challenging outdoor environments.\n","authors":["Byeonggwon Lee","Junkyu Park","Khang Truong Giang","Sungho Jo","Soohwan Song"],"pdf_url":"https://arxiv.org/pdf/2412.19130v1.pdf","comment":"7 pages, 6 figures, submitted to IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2412.19089v1","updated":"2024-12-26T07:04:20Z","published":"2024-12-26T07:04:20Z","title":"Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from\n  Unsynchronized and Uncalibrated Videos","summary":"  Recent works on dynamic neural field reconstruction assume input from\nsynchronized multi-view videos with known poses. These input constraints are\noften unmet in real-world setups, making the approach impractical. We\ndemonstrate that unsynchronized videos with unknown poses can generate dynamic\nneural fields if the videos capture human motion. Humans are one of the most\ncommon dynamic subjects whose poses can be estimated using state-of-the-art\nmethods. While noisy, the estimated human shape and pose parameters provide a\ndecent initialization for the highly non-convex and under-constrained problem\nof training a consistent dynamic neural representation. Given the sequences of\npose and shape of humans, we estimate the time offsets between videos, followed\nby camera pose estimations by analyzing 3D joint locations. Then, we train\ndynamic NeRF employing multiresolution rids while simultaneously refining both\ntime offsets and camera poses. The setup still involves optimizing many\nparameters, therefore, we introduce a robust progressive learning strategy to\nstabilize the process. Experiments show that our approach achieves accurate\nspatiotemporal calibration and high-quality scene reconstruction in challenging\nconditions.\n","authors":["Changwoon Choi","Jeongjun Kim","Geonho Cha","Minkwan Kim","Dongyoon Wee","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2412.19089v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2402.14401v2","updated":"2024-12-26T08:29:23Z","published":"2024-02-22T09:39:46Z","title":"Diffusion Model Based Visual Compensation Guidance and Visual Difference\n  Analysis for No-Reference Image Quality Assessment","summary":"  Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA)\nmethods still suffer from finding a balance between learning feature\ninformation at the pixel level of the image and capturing high-level feature\ninformation and the efficient utilization of the obtained high-level feature\ninformation remains a challenge. As a novel class of state-of-the-art (SOTA)\ngenerative model, the diffusion model exhibits the capability to model\nintricate relationships, enabling a comprehensive understanding of images and\npossessing a better learning of both high-level and low-level visual features.\nIn view of these, we pioneer the exploration of the diffusion model into the\ndomain of NR-IQA. Firstly, we devise a new diffusion restoration network that\nleverages the produced enhanced image and noise-containing images,\nincorporating nonlinear features obtained during the denoising process of the\ndiffusion model, as high-level visual information. Secondly, two visual\nevaluation branches are designed to comprehensively analyze the obtained\nhigh-level feature information. These include the visual compensation guidance\nbranch, grounded in the transformer architecture and noise embedding strategy,\nand the visual difference analysis branch, built on the ResNet architecture and\nthe residual transposed attention block. Extensive experiments are conducted on\nseven public NR-IQA datasets, and the results demonstrate that the proposed\nmodel outperforms SOTA methods for NR-IQA.\n","authors":["Zhaoyang Wang","Bo Hu","Mingyang Zhang","Jie Li","Leida Li","Maoguo Gong","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2402.14401v2.pdf","comment":"Accepted by TIP"}]},"2025-01-02T00:00:00Z":{"IQA":[{"id":"http://arxiv.org/abs/2501.01116v1","updated":"2025-01-02T07:30:17Z","published":"2025-01-02T07:30:17Z","title":"HarmonyIQA: Pioneering Benchmark and Model for Image Harmonization\n  Quality Assessment","summary":"  Image composition involves extracting a foreground object from one image and\npasting it into another image through Image harmonization algorithms (IHAs),\nwhich aim to adjust the appearance of the foreground object to better match the\nbackground. Existing image quality assessment (IQA) methods may fail to align\nwith human visual preference on image harmonization due to the insensitivity to\nminor color or light inconsistency. To address the issue and facilitate the\nadvancement of IHAs, we introduce the first Image Quality Assessment Database\nfor image Harmony evaluation (HarmonyIQAD), which consists of 1,350 harmonized\nimages generated by 9 different IHAs, and the corresponding human visual\npreference scores. Based on this database, we propose a Harmony Image Quality\nAssessment (HarmonyIQA), to predict human visual preference for harmonized\nimages. Extensive experiments show that HarmonyIQA achieves state-of-the-art\nperformance on human visual preference evaluation for harmonized images, and\nalso achieves competing results on traditional IQA tasks. Furthermore,\ncross-dataset evaluation also shows that HarmonyIQA exhibits better\ngeneralization ability than self-supervised learning-based IQA methods. Both\nHarmonyIQAD and HarmonyIQA will be made publicly available upon paper\npublication.\n","authors":["Zitong Xu","Huiyu Duan","Guangji Ma","Liu Yang","Jiarui Wang","Qingbo Wu","Xiongkuo Min","Guangtao Zhai","Patrick Le Callet"],"pdf_url":"https://arxiv.org/pdf/2501.01116v1.pdf","comment":null}]},"2024-12-28T00:00:00Z":{"Deblur":[{"id":"http://arxiv.org/abs/2412.20066v1","updated":"2024-12-28T07:40:39Z","published":"2024-12-28T07:40:39Z","title":"MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration","summary":"  Recent advancements in Mamba have shown promising results in image\nrestoration. These methods typically flatten 2D images into multiple distinct\n1D sequences along rows and columns, process each sequence independently using\nselective scan operation, and recombine them to form the outputs. However, such\na paradigm overlooks two vital aspects: i) the local relationships and spatial\ncontinuity inherent in natural images, and ii) the discrepancies among\nsequences unfolded through totally different ways. To overcome the drawbacks,\nwe explore two problems in Mamba-based restoration methods: i) how to design a\nscanning strategy preserving both locality and continuity while facilitating\nrestoration, and ii) how to aggregate the distinct sequences unfolded in\ntotally different ways. To address these problems, we propose a novel\nMamba-based Image Restoration model (MaIR), which consists of Nested S-shaped\nScanning strategy (NSS) and Sequence Shuffle Attention block (SSA).\nSpecifically, NSS preserves locality and continuity of the input images through\nthe stripe-based scanning region and the S-shaped scanning path, respectively.\nSSA aggregates sequences through calculating attention weights within the\ncorresponding channels of different sequences. Thanks to NSS and SSA, MaIR\nsurpasses 40 baselines across 14 challenging datasets, achieving\nstate-of-the-art performance on the tasks of image super-resolution, denoising,\ndeblurring and dehazing. Our codes will be available after acceptance.\n","authors":["Boyun Li","Haiyu Zhao","Wenxin Wang","Peng Hu","Yuanbiao Gou","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2412.20066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20045v1","updated":"2024-12-28T06:17:44Z","published":"2024-12-28T06:17:44Z","title":"Enhancing Diffusion Models for Inverse Problems with Covariance-Aware\n  Posterior Sampling","summary":"  Inverse problems exist in many disciplines of science and engineering. In\ncomputer vision, for example, tasks such as inpainting, deblurring, and super\nresolution can be effectively modeled as inverse problems. Recently, denoising\ndiffusion probabilistic models (DDPMs) are shown to provide a promising\nsolution to noisy linear inverse problems without the need for additional task\nspecific training. Specifically, with the prior provided by DDPMs, one can\nsample from the posterior by approximating the likelihood. In the literature,\napproximations of the likelihood are often based on the mean of conditional\ndensities of the reverse process, which can be obtained using Tweedie formula.\nTo obtain a better approximation to the likelihood, in this paper we first\nderive a closed form formula for the covariance of the reverse process. Then,\nwe propose a method based on finite difference method to approximate this\ncovariance such that it can be readily obtained from the existing pretrained\nDDPMs, thereby not increasing the complexity compared to existing approaches.\nFinally, based on the mean and approximated covariance of the reverse process,\nwe present a new approximation to the likelihood. We refer to this method as\ncovariance-aware diffusion posterior sampling (CA-DPS). Experimental results\nshow that CA-DPS significantly improves reconstruction performance without\nrequiring hyperparameter tuning. The code for the paper is put in the\nsupplementary materials.\n","authors":["Shayan Mohajer Hamidi","En-Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.20045v1.pdf","comment":null}]}}